{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use('Agg')\n",
    "from finrl.config_tickers import DOW_30_TICKER,NAS_100_TICKER,HSI_50_TICKER,DAX_30_TICKER,SP_500_TICKER\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "from finrl.config_tickers import DOW_30_TICKER\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import data_split\n",
    "from stock_env import StockTradingEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent,DRLEnsembleAgent\n",
    "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline,trx_plot\n",
    "from feature_engineer import FeatureEngineer\n",
    "from models import DRLAgent\n",
    "from portfolio_optimization_env import PortfolioOptimizationEnv\n",
    "from pprint import pprint\n",
    "import scienceplots\n",
    "import sys\n",
    "sys.path.append(\"../FinRL-Library\")\n",
    "import quantstats as qs\n",
    "plt.style.use('science')\n",
    "import os\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.config import (\n",
    "    DATA_SAVE_DIR,\n",
    "    TRAINED_MODEL_DIR,\n",
    "    TENSORBOARD_LOG_DIR,\n",
    "    RESULTS_DIR,\n",
    "    INDICATORS,\n",
    "    TRAIN_START_DATE,\n",
    "    TRAIN_END_DATE,\n",
    "    TEST_START_DATE,\n",
    "    TEST_END_DATE,\n",
    "    TRADE_START_DATE,\n",
    "    TRADE_END_DATE,\n",
    ")\n",
    "\n",
    "check_and_make_directories([DATA_SAVE_DIR, TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR])\n",
    "\n",
    "def DRL_prediction(model, environment,time_window, deterministic=True):\n",
    "    \"\"\"make a prediction and get results\"\"\"\n",
    "    test_env, test_obs = environment.get_sb_env()\n",
    "    account_memory = None  # This help avoid unnecessary list creation\n",
    "    actions_memory = None  # optimize memory consumption\n",
    "    # state_memory=[] #add memory pool to store states\n",
    "\n",
    "    test_env.reset()\n",
    "    max_steps = len(environment._df.index.unique()) - time_window - 1\n",
    "\n",
    "    for i in range(len(environment._df.index.unique())):\n",
    "        action, _states = model.predict(test_obs, deterministic=deterministic)\n",
    "        # account_memory = test_env.env_method(method_name=\"save_asset_memory\")\n",
    "        # actions_memory = test_env.env_method(method_name=\"save_action_memory\")\n",
    "        test_obs, rewards, dones, info = test_env.step(action)\n",
    "        if i == max_steps:  # more descriptive condition for early termination to clarify the logic\n",
    "            date_list = environment._date_memory\n",
    "            portfolio_return = environment._portfolio_return_memory\n",
    "            # print(len(date_list))\n",
    "            # print(len(asset_list))\n",
    "            df_account_value = pd.DataFrame(\n",
    "                {\"date\": date_list, \"daily_return\": portfolio_return,'account' :  environment._asset_memory[\"final\"],'weights':environment._final_weights}\n",
    "            )\n",
    "            df_date = pd.DataFrame(date_list)\n",
    "            df_date.columns = [\"date\"]\n",
    "\n",
    "            action_list = environment._actions_memory\n",
    "            df_actions = pd.DataFrame(action_list)\n",
    "            tiks = environment._tic_list\n",
    "            df_actions.columns = np.insert(tiks,0,'POS')\n",
    "            df_actions.index = df_date.date\n",
    "            # df_actions = pd.DataFrame({'date':date_list,'actions':action_list})\n",
    "            account_memory = df_account_value\n",
    "            actions_memory = df_actions\n",
    "        # add current state to state memory\n",
    "        # state_memory=test_env.env_method(method_name=\"save_state_memory\")\n",
    "\n",
    "        if dones[0]:\n",
    "            print(\"hit end!\")\n",
    "            break\n",
    "    return account_memory, actions_memory,test_obs\n",
    "TRAIN_START_DATE = '2000-01-01'\n",
    "TRAIN_END_DATE = '2022-12-30'\n",
    "TEST_START_DATE = '2023-01-01'\n",
    "TEST_END_DATE = '2024-10-01'\n",
    "from pandas import read_csv\n",
    "\n",
    "\n",
    "df_dow =read_csv('./data/dow.csv')\n",
    "df_nasdaq =read_csv('./data/nasdaq.csv')\n",
    "df_hsi = read_csv('./data/hsi.csv')\n",
    "df_dax = read_csv('./data/dax.csv')\n",
    "df_sp500 = read_csv('./data/sp500.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added turbulence index\n",
      "Successfully added user defined features\n"
     ]
    }
   ],
   "source": [
    "\n",
    "INDICATORS = [\n",
    "    \"close_5_ema\",\n",
    "]\n",
    "fe = FeatureEngineer(use_technical_indicator=False,\n",
    "                     tech_indicator_list = INDICATORS,\n",
    "                     use_turbulence=True,\n",
    "                     user_defined_feature =True)\n",
    "\n",
    "processed = fe.preprocess_data(df_sp500)\n",
    "processed = processed.fillna(0)\n",
    "processed= processed.replace(np.inf,0)\n",
    "train_data= data_split(processed, TRAIN_START_DATE, TRAIN_END_DATE)\n",
    "test_data = data_split(processed, TEST_START_DATE, TEST_END_DATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sample = train_data.copy()\n",
    "test_data_sample = test_data.copy()\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sample = train_data.copy()\n",
    "test_data_sample = test_data.copy()\n",
    "columns = np.random.choice(train_data.tic.unique(),10,replace=True)\n",
    "train_data_sample = train_data_sample[train_data_sample['tic'].isin(columns)]\n",
    "test_data_sample = test_data_sample[train_data_sample['tic'].isin(columns)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.0003, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "--------------------------------------\n",
      "| time/              |               |\n",
      "|    fps             | 5             |\n",
      "|    iterations      | 1             |\n",
      "|    time_elapsed    | 377           |\n",
      "|    total_timesteps | 2048          |\n",
      "| train/             |               |\n",
      "|    reward          | -0.0029867147 |\n",
      "--------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 749          |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.6570768    |\n",
      "|    clip_fraction        | 0.863        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -488         |\n",
      "|    explained_variance   | 0.866        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -4.98        |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0696      |\n",
      "|    reward               | 0.0006393535 |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.0056       |\n",
      "------------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 1761636.25\n",
      "Final accumulative portfolio value: 1.76163625\n",
      "Maximum DrawDown: -0.5875748637642813\n",
      "Sharpe ratio: 0.22287467523856194\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.79e+03    |\n",
      "|    ep_rew_mean          | 0.582       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 5           |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 1125        |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.16879562  |\n",
      "|    clip_fraction        | 0.729       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -491        |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -4.91       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | 0.000294    |\n",
      "|    reward               | 0.012410558 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.00253     |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 5.79e+03      |\n",
      "|    ep_rew_mean          | 0.582         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 5             |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 1500          |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.61348146    |\n",
      "|    clip_fraction        | 0.755         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -492          |\n",
      "|    explained_variance   | 0.801         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -4.98         |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.0573       |\n",
      "|    reward               | -0.0074381805 |\n",
      "|    std                  | 1.02          |\n",
      "|    value_loss           | 0.00533       |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5.79e+03     |\n",
      "|    ep_rew_mean          | 0.582        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 1876         |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.10879004   |\n",
      "|    clip_fraction        | 0.63         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -495         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -4.95        |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0218      |\n",
      "|    reward               | 0.0012517481 |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 0.00221      |\n",
      "------------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 1708854.5\n",
      "Final accumulative portfolio value: 1.7088545\n",
      "Maximum DrawDown: -0.6017468878157919\n",
      "Sharpe ratio: 0.21647886967321475\n",
      "=================================\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5.79e+03     |\n",
      "|    ep_rew_mean          | 0.566        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 2254         |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.10053135   |\n",
      "|    clip_fraction        | 0.599        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -496         |\n",
      "|    explained_variance   | -3.58e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -4.99        |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0257      |\n",
      "|    reward               | -0.012348405 |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 0.000918     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5.79e+03     |\n",
      "|    ep_rew_mean          | 0.566        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 2632         |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.1814439    |\n",
      "|    clip_fraction        | 0.649        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -498         |\n",
      "|    explained_variance   | 0.676        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -5.06        |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.0621      |\n",
      "|    reward               | 0.0005920564 |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 0.0028       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5.79e+03     |\n",
      "|    ep_rew_mean          | 0.566        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 3009         |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.09603719   |\n",
      "|    clip_fraction        | 0.555        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -500         |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -5.05        |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.0373      |\n",
      "|    reward               | 0.0023476915 |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 0.00174      |\n",
      "------------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 1709603.5\n",
      "Final accumulative portfolio value: 1.7096035\n",
      "Maximum DrawDown: -0.5953365620778068\n",
      "Sharpe ratio: 0.2165763968590003\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 5.79e+03      |\n",
      "|    ep_rew_mean          | 0.562         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 5             |\n",
      "|    iterations           | 9             |\n",
      "|    time_elapsed         | 3391          |\n",
      "|    total_timesteps      | 18432         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.09269366    |\n",
      "|    clip_fraction        | 0.562         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -502          |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -5.07         |\n",
      "|    n_updates            | 80            |\n",
      "|    policy_gradient_loss | -0.043        |\n",
      "|    reward               | -0.0035983017 |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 0.00068       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 5.79e+03      |\n",
      "|    ep_rew_mean          | 0.562         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 5             |\n",
      "|    iterations           | 10            |\n",
      "|    time_elapsed         | 3763          |\n",
      "|    total_timesteps      | 20480         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.1075414     |\n",
      "|    clip_fraction        | 0.574         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -503          |\n",
      "|    explained_variance   | 0.368         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -5.1          |\n",
      "|    n_updates            | 90            |\n",
      "|    policy_gradient_loss | -0.0616       |\n",
      "|    reward               | -0.0026739193 |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 0.0019        |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5.79e+03     |\n",
      "|    ep_rew_mean          | 0.562        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 4135         |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.077822074  |\n",
      "|    clip_fraction        | 0.527        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -505         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -5.06        |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.0414      |\n",
      "|    reward               | 0.0058556674 |\n",
      "|    std                  | 1.06         |\n",
      "|    value_loss           | 0.00153      |\n",
      "------------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 1650990.5\n",
      "Final accumulative portfolio value: 1.6509905\n",
      "Maximum DrawDown: -0.5945258025261111\n",
      "Sharpe ratio: 0.209243647506765\n",
      "=================================\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5.79e+03     |\n",
      "|    ep_rew_mean          | 0.551        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 4511         |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.07845086   |\n",
      "|    clip_fraction        | 0.543        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -507         |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -5.13        |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.0393      |\n",
      "|    reward               | -0.005537435 |\n",
      "|    std                  | 1.06         |\n",
      "|    value_loss           | 0.00097      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5.79e+03     |\n",
      "|    ep_rew_mean          | 0.551        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 4885         |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.10342414   |\n",
      "|    clip_fraction        | 0.562        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -508         |\n",
      "|    explained_variance   | -0.000424    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -5.17        |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.067       |\n",
      "|    reward               | 0.0023652932 |\n",
      "|    std                  | 1.07         |\n",
      "|    value_loss           | 0.00114      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.79e+03    |\n",
      "|    ep_rew_mean          | 0.551       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 5           |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 5257        |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.079406485 |\n",
      "|    clip_fraction        | 0.526       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -510        |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -5.13       |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.041      |\n",
      "|    reward               | 0.016136521 |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 0.00154     |\n",
      "-----------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 1714675.75\n",
      "Final accumulative portfolio value: 1.71467575\n",
      "Maximum DrawDown: -0.5890442775995737\n",
      "Sharpe ratio: 0.21719093973533368\n",
      "=================================\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5.79e+03     |\n",
      "|    ep_rew_mean          | 0.551        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 5635         |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.07771512   |\n",
      "|    clip_fraction        | 0.509        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -511         |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -5.18        |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.0384      |\n",
      "|    reward               | -0.000764364 |\n",
      "|    std                  | 1.08         |\n",
      "|    value_loss           | 0.000995     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5.79e+03     |\n",
      "|    ep_rew_mean          | 0.551        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 6010         |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.08229598   |\n",
      "|    clip_fraction        | 0.524        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -513         |\n",
      "|    explained_variance   | 0.00522      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -5.16        |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.0597      |\n",
      "|    reward               | 0.0036341592 |\n",
      "|    std                  | 1.08         |\n",
      "|    value_loss           | 0.00101      |\n",
      "------------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 1777601.5\n",
      "Final accumulative portfolio value: 1.7776015\n",
      "Maximum DrawDown: -0.5848693196336427\n",
      "Sharpe ratio: 0.22476822900679666\n",
      "=================================\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5.79e+03     |\n",
      "|    ep_rew_mean          | 0.558        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 6386         |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.06901413   |\n",
      "|    clip_fraction        | 0.511        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -514         |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -5.21        |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.0443      |\n",
      "|    reward               | -0.015328828 |\n",
      "|    std                  | 1.09         |\n",
      "|    value_loss           | 0.0015       |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 5.79e+03      |\n",
      "|    ep_rew_mean          | 0.558         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 5             |\n",
      "|    iterations           | 18            |\n",
      "|    time_elapsed         | 6759          |\n",
      "|    total_timesteps      | 36864         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.07694813    |\n",
      "|    clip_fraction        | 0.525         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -516          |\n",
      "|    explained_variance   | 0.0567        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -5.21         |\n",
      "|    n_updates            | 170           |\n",
      "|    policy_gradient_loss | -0.0506       |\n",
      "|    reward               | -0.0013174148 |\n",
      "|    std                  | 1.09          |\n",
      "|    value_loss           | 0.00116       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 5.79e+03      |\n",
      "|    ep_rew_mean          | 0.558         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 5             |\n",
      "|    iterations           | 19            |\n",
      "|    time_elapsed         | 7131          |\n",
      "|    total_timesteps      | 38912         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.066840775   |\n",
      "|    clip_fraction        | 0.485         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -517          |\n",
      "|    explained_variance   | 0.0915        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -5.22         |\n",
      "|    n_updates            | 180           |\n",
      "|    policy_gradient_loss | -0.0535       |\n",
      "|    reward               | 0.00047505536 |\n",
      "|    std                  | 1.1           |\n",
      "|    value_loss           | 0.00093       |\n",
      "-------------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 1686896.625\n",
      "Final accumulative portfolio value: 1.686896625\n",
      "Maximum DrawDown: -0.5891050200090366\n",
      "Sharpe ratio: 0.21378343635878866\n",
      "=================================\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5.79e+03     |\n",
      "|    ep_rew_mean          | 0.555        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 7506         |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.071984     |\n",
      "|    clip_fraction        | 0.507        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -519         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -5.26        |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.0407      |\n",
      "|    reward               | 0.0042353463 |\n",
      "|    std                  | 1.1          |\n",
      "|    value_loss           | 0.00146      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.79e+03    |\n",
      "|    ep_rew_mean          | 0.555       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 5           |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 7879        |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.078843445 |\n",
      "|    clip_fraction        | 0.517       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -520        |\n",
      "|    explained_variance   | 0.166       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -5.27       |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0584     |\n",
      "|    reward               | 0.005696373 |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 0.00127     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.79e+03    |\n",
      "|    ep_rew_mean          | 0.555       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 5           |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 8252        |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.070082374 |\n",
      "|    clip_fraction        | 0.488       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -522        |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -5.27       |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0488     |\n",
      "|    reward               | -0.03383098 |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 0.00166     |\n",
      "-----------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 1765882.375\n",
      "Final accumulative portfolio value: 1.765882375\n",
      "Maximum DrawDown: -0.5888525742078827\n",
      "Sharpe ratio: 0.22339819628146365\n",
      "=================================\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5.79e+03     |\n",
      "|    ep_rew_mean          | 0.559        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 8630         |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.06576021   |\n",
      "|    clip_fraction        | 0.474        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -523         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -5.28        |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.05        |\n",
      "|    reward               | 0.0009919252 |\n",
      "|    std                  | 1.12         |\n",
      "|    value_loss           | 0.000591     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5.79e+03     |\n",
      "|    ep_rew_mean          | 0.559        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 9004         |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.07289829   |\n",
      "|    clip_fraction        | 0.487        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -525         |\n",
      "|    explained_variance   | 0.195        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -5.32        |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.0558      |\n",
      "|    reward               | -0.014232891 |\n",
      "|    std                  | 1.12         |\n",
      "|    value_loss           | 0.00151      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 5.79e+03      |\n",
      "|    ep_rew_mean          | 0.559         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 5             |\n",
      "|    iterations           | 25            |\n",
      "|    time_elapsed         | 9376          |\n",
      "|    total_timesteps      | 51200         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.060488157   |\n",
      "|    clip_fraction        | 0.448         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -526          |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -5.31         |\n",
      "|    n_updates            | 240           |\n",
      "|    policy_gradient_loss | -0.0436       |\n",
      "|    reward               | -0.0011830173 |\n",
      "|    std                  | 1.12          |\n",
      "|    value_loss           | 0.00144       |\n",
      "-------------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 1550085.0\n",
      "Final accumulative portfolio value: 1.550085\n",
      "Maximum DrawDown: -0.5967873691627934\n",
      "Sharpe ratio: 0.19594830814974365\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.79e+03    |\n",
      "|    ep_rew_mean          | 0.547       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 5           |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 9753        |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.0631122   |\n",
      "|    clip_fraction        | 0.471       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -527        |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -5.35       |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0528     |\n",
      "|    reward               | 0.008130194 |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 0.000592    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5.79e+03     |\n",
      "|    ep_rew_mean          | 0.547        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 10126        |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0696782    |\n",
      "|    clip_fraction        | 0.487        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -529         |\n",
      "|    explained_variance   | 0.219        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -5.35        |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.0588      |\n",
      "|    reward               | -0.002312471 |\n",
      "|    std                  | 1.13         |\n",
      "|    value_loss           | 0.00159      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 5.79e+03      |\n",
      "|    ep_rew_mean          | 0.547         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 5             |\n",
      "|    iterations           | 28            |\n",
      "|    time_elapsed         | 10499         |\n",
      "|    total_timesteps      | 57344         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.061904393   |\n",
      "|    clip_fraction        | 0.466         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -530          |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -5.38         |\n",
      "|    n_updates            | 270           |\n",
      "|    policy_gradient_loss | -0.0489       |\n",
      "|    reward               | -0.0045953915 |\n",
      "|    std                  | 1.14          |\n",
      "|    value_loss           | 0.00153       |\n",
      "-------------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 1587614.875\n",
      "Final accumulative portfolio value: 1.587614875\n",
      "Maximum DrawDown: -0.5988753477103697\n",
      "Sharpe ratio: 0.2010044260065012\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 5.79e+03      |\n",
      "|    ep_rew_mean          | 0.54          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 5             |\n",
      "|    iterations           | 29            |\n",
      "|    time_elapsed         | 10877         |\n",
      "|    total_timesteps      | 59392         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0653715     |\n",
      "|    clip_fraction        | 0.489         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -532          |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -5.39         |\n",
      "|    n_updates            | 280           |\n",
      "|    policy_gradient_loss | -0.0443       |\n",
      "|    reward               | -0.0070749456 |\n",
      "|    std                  | 1.14          |\n",
      "|    value_loss           | 0.00101       |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5.79e+03     |\n",
      "|    ep_rew_mean          | 0.54         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 11250        |\n",
      "|    total_timesteps      | 61440        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.061897244  |\n",
      "|    clip_fraction        | 0.456        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -533         |\n",
      "|    explained_variance   | 0.273        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -5.43        |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.0562      |\n",
      "|    reward               | 0.0052555734 |\n",
      "|    std                  | 1.15         |\n",
      "|    value_loss           | 0.000926     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 5.79e+03      |\n",
      "|    ep_rew_mean          | 0.54          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 5             |\n",
      "|    iterations           | 31            |\n",
      "|    time_elapsed         | 11621         |\n",
      "|    total_timesteps      | 63488         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.068047255   |\n",
      "|    clip_fraction        | 0.457         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -535          |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -5.4          |\n",
      "|    n_updates            | 300           |\n",
      "|    policy_gradient_loss | -0.0498       |\n",
      "|    reward               | -0.0016840562 |\n",
      "|    std                  | 1.15          |\n",
      "|    value_loss           | 0.00153       |\n",
      "-------------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 1539701.75\n",
      "Final accumulative portfolio value: 1.53970175\n",
      "Maximum DrawDown: -0.5977952633693745\n",
      "Sharpe ratio: 0.19453620459779059\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.79e+03    |\n",
      "|    ep_rew_mean          | 0.532       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 5           |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 12001       |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.06362164  |\n",
      "|    clip_fraction        | 0.461       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -536        |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -5.41       |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0476     |\n",
      "|    reward               | 0.016620155 |\n",
      "|    std                  | 1.16        |\n",
      "|    value_loss           | 0.00102     |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 5.79e+03      |\n",
      "|    ep_rew_mean          | 0.532         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 5             |\n",
      "|    iterations           | 33            |\n",
      "|    time_elapsed         | 12375         |\n",
      "|    total_timesteps      | 67584         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.07141762    |\n",
      "|    clip_fraction        | 0.485         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -537          |\n",
      "|    explained_variance   | 0.277         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -5.45         |\n",
      "|    n_updates            | 320           |\n",
      "|    policy_gradient_loss | -0.0641       |\n",
      "|    reward               | 0.00096179463 |\n",
      "|    std                  | 1.16          |\n",
      "|    value_loss           | 0.000886      |\n",
      "-------------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 1632603.5\n",
      "Final accumulative portfolio value: 1.6326035\n",
      "Maximum DrawDown: -0.5854947915187702\n",
      "Sharpe ratio: 0.2068678190603604\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.79e+03    |\n",
      "|    ep_rew_mean          | 0.529       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 5           |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 12752       |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.06297224  |\n",
      "|    clip_fraction        | 0.455       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -539        |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -5.46       |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0491     |\n",
      "|    reward               | -0.01843776 |\n",
      "|    std                  | 1.17        |\n",
      "|    value_loss           | 0.00151     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.79e+03    |\n",
      "|    ep_rew_mean          | 0.529       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 5           |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 13127       |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.067105114 |\n",
      "|    clip_fraction        | 0.491       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -540        |\n",
      "|    explained_variance   | 0.151       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -5.45       |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0553     |\n",
      "|    reward               | 0.020028263 |\n",
      "|    std                  | 1.17        |\n",
      "|    value_loss           | 0.00111     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5.79e+03     |\n",
      "|    ep_rew_mean          | 0.529        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 13500        |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.060063165  |\n",
      "|    clip_fraction        | 0.458        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -542         |\n",
      "|    explained_variance   | 0.0313       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -5.49        |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.0532      |\n",
      "|    reward               | 0.0006993947 |\n",
      "|    std                  | 1.18         |\n",
      "|    value_loss           | 0.00137      |\n",
      "------------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 1684784.25\n",
      "Final accumulative portfolio value: 1.68478425\n",
      "Maximum DrawDown: -0.5755186696775665\n",
      "Sharpe ratio: 0.21350076836926438\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.79e+03    |\n",
      "|    ep_rew_mean          | 0.53        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 5           |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 13877       |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.070737615 |\n",
      "|    clip_fraction        | 0.462       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -543        |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -5.49       |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0511     |\n",
      "|    reward               | 0.008046354 |\n",
      "|    std                  | 1.18        |\n",
      "|    value_loss           | 0.000953    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5.79e+03     |\n",
      "|    ep_rew_mean          | 0.53         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 14250        |\n",
      "|    total_timesteps      | 77824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.06282246   |\n",
      "|    clip_fraction        | 0.473        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -545         |\n",
      "|    explained_variance   | 0.102        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -5.53        |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.0572      |\n",
      "|    reward               | -0.021339593 |\n",
      "|    std                  | 1.19         |\n",
      "|    value_loss           | 0.00125      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 5.79e+03      |\n",
      "|    ep_rew_mean          | 0.53          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 5             |\n",
      "|    iterations           | 39            |\n",
      "|    time_elapsed         | 14623         |\n",
      "|    total_timesteps      | 79872         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.05958554    |\n",
      "|    clip_fraction        | 0.448         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -546          |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -5.54         |\n",
      "|    n_updates            | 380           |\n",
      "|    policy_gradient_loss | -0.0512       |\n",
      "|    reward               | -0.0014164342 |\n",
      "|    std                  | 1.19          |\n",
      "|    value_loss           | 0.00162       |\n",
      "-------------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 1655577.625\n",
      "Final accumulative portfolio value: 1.655577625\n",
      "Maximum DrawDown: -0.585914080601543\n",
      "Sharpe ratio: 0.2097839299720482\n",
      "=================================\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5.79e+03     |\n",
      "|    ep_rew_mean          | 0.529        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 15000        |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.052508272  |\n",
      "|    clip_fraction        | 0.431        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -547         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -5.55        |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.0508      |\n",
      "|    reward               | 0.0036471058 |\n",
      "|    std                  | 1.2          |\n",
      "|    value_loss           | 0.000553     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 5.79e+03      |\n",
      "|    ep_rew_mean          | 0.529         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 5             |\n",
      "|    iterations           | 41            |\n",
      "|    time_elapsed         | 15372         |\n",
      "|    total_timesteps      | 83968         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.05572766    |\n",
      "|    clip_fraction        | 0.464         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -549          |\n",
      "|    explained_variance   | 0.133         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -5.57         |\n",
      "|    n_updates            | 400           |\n",
      "|    policy_gradient_loss | -0.0547       |\n",
      "|    reward               | -0.0040074307 |\n",
      "|    std                  | 1.2           |\n",
      "|    value_loss           | 0.00146       |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.79e+03    |\n",
      "|    ep_rew_mean          | 0.529       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 5           |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 15746       |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059843376 |\n",
      "|    clip_fraction        | 0.448       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -550        |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -5.58       |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0534     |\n",
      "|    reward               | 0.007210541 |\n",
      "|    std                  | 1.21        |\n",
      "|    value_loss           | 0.00151     |\n",
      "-----------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 1676853.125\n",
      "Final accumulative portfolio value: 1.676853125\n",
      "Maximum DrawDown: -0.5995867748305952\n",
      "Sharpe ratio: 0.21248903776902484\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 5.79e+03      |\n",
      "|    ep_rew_mean          | 0.53          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 5             |\n",
      "|    iterations           | 43            |\n",
      "|    time_elapsed         | 16126         |\n",
      "|    total_timesteps      | 88064         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.052256078   |\n",
      "|    clip_fraction        | 0.446         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -552          |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -5.59         |\n",
      "|    n_updates            | 420           |\n",
      "|    policy_gradient_loss | -0.0567       |\n",
      "|    reward               | -0.0032609764 |\n",
      "|    std                  | 1.21          |\n",
      "|    value_loss           | 0.000446      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5.79e+03     |\n",
      "|    ep_rew_mean          | 0.53         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 16498        |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.06397628   |\n",
      "|    clip_fraction        | 0.485        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -553         |\n",
      "|    explained_variance   | 0.138        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -5.62        |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.0589      |\n",
      "|    reward               | 0.0077671176 |\n",
      "|    std                  | 1.22         |\n",
      "|    value_loss           | 0.00144      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5.79e+03    |\n",
      "|    ep_rew_mean          | 0.53        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 5           |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 16869       |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.06137701  |\n",
      "|    clip_fraction        | 0.45        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -554        |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -5.62       |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0522     |\n",
      "|    reward               | 0.009481637 |\n",
      "|    std                  | 1.22        |\n",
      "|    value_loss           | 0.00152     |\n",
      "-----------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 1460101.75\n",
      "Final accumulative portfolio value: 1.46010175\n",
      "Maximum DrawDown: -0.603982995707212\n",
      "Sharpe ratio: 0.18331615694006984\n",
      "=================================\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5.79e+03     |\n",
      "|    ep_rew_mean          | 0.521        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 17247        |\n",
      "|    total_timesteps      | 94208        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.05914223   |\n",
      "|    clip_fraction        | 0.448        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -556         |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -5.62        |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.0502      |\n",
      "|    reward               | 0.0023348469 |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 0.000996     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5.79e+03     |\n",
      "|    ep_rew_mean          | 0.521        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 47           |\n",
      "|    time_elapsed         | 17620        |\n",
      "|    total_timesteps      | 96256        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0696999    |\n",
      "|    clip_fraction        | 0.489        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -557         |\n",
      "|    explained_variance   | 0.225        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -5.66        |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.064       |\n",
      "|    reward               | 0.0031491946 |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 0.000898     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5.79e+03     |\n",
      "|    ep_rew_mean          | 0.521        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 17993        |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.05116687   |\n",
      "|    clip_fraction        | 0.412        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -559         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -5.64        |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.0467      |\n",
      "|    reward               | -0.023669835 |\n",
      "|    std                  | 1.24         |\n",
      "|    value_loss           | 0.00148      |\n",
      "------------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 1522339.125\n",
      "Final accumulative portfolio value: 1.522339125\n",
      "Maximum DrawDown: -0.5956321001728615\n",
      "Sharpe ratio: 0.19212779397042756\n",
      "=================================\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5.79e+03     |\n",
      "|    ep_rew_mean          | 0.516        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 5            |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 18373        |\n",
      "|    total_timesteps      | 100352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.05889614   |\n",
      "|    clip_fraction        | 0.441        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -560         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -5.68        |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.0522      |\n",
      "|    reward               | -0.004434749 |\n",
      "|    std                  | 1.24         |\n",
      "|    value_loss           | 0.00112      |\n",
      "------------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 1309134.0\n",
      "Final accumulative portfolio value: 1.309134\n",
      "Maximum DrawDown: -0.12843506233141355\n",
      "Sharpe ratio: 1.231668699840551\n",
      "=================================\n",
      "hit end!\n",
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 5             |\n",
      "|    iterations         | 100           |\n",
      "|    time_elapsed       | 91            |\n",
      "|    total_timesteps    | 500           |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -502          |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 99            |\n",
      "|    policy_loss        | 1.97          |\n",
      "|    reward             | -0.0006216908 |\n",
      "|    std                | 1.05          |\n",
      "|    value_loss         | 3.78e-05      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 5            |\n",
      "|    iterations         | 200          |\n",
      "|    time_elapsed       | 182          |\n",
      "|    total_timesteps    | 1000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -516         |\n",
      "|    explained_variance | 5.96e-08     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 199          |\n",
      "|    policy_loss        | 3.75         |\n",
      "|    reward             | 0.0024069168 |\n",
      "|    std                | 1.09         |\n",
      "|    value_loss         | 7.58e-05     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 5            |\n",
      "|    iterations         | 300          |\n",
      "|    time_elapsed       | 274          |\n",
      "|    total_timesteps    | 1500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -536         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 299          |\n",
      "|    policy_loss        | -0.595       |\n",
      "|    reward             | 0.0012698452 |\n",
      "|    std                | 1.16         |\n",
      "|    value_loss         | 1.42e-05     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 5            |\n",
      "|    iterations         | 400          |\n",
      "|    time_elapsed       | 365          |\n",
      "|    total_timesteps    | 2000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -556         |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 399          |\n",
      "|    policy_loss        | 15.3         |\n",
      "|    reward             | -0.016334781 |\n",
      "|    std                | 1.22         |\n",
      "|    value_loss         | 0.000993     |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 5           |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 457         |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -568        |\n",
      "|    explained_variance | -1.73e-05   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -3.02       |\n",
      "|    reward             | 0.004797258 |\n",
      "|    std                | 1.27        |\n",
      "|    value_loss         | 0.000102    |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 5           |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 548         |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -585        |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | 4.15        |\n",
      "|    reward             | 0.012928945 |\n",
      "|    std                | 1.33        |\n",
      "|    value_loss         | 0.00043     |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 5             |\n",
      "|    iterations         | 700           |\n",
      "|    time_elapsed       | 639           |\n",
      "|    total_timesteps    | 3500          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -605          |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 699           |\n",
      "|    policy_loss        | -0.408        |\n",
      "|    reward             | -0.0027975196 |\n",
      "|    std                | 1.41          |\n",
      "|    value_loss         | 4.98e-05      |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 5           |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 730         |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -626        |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | 9.9         |\n",
      "|    reward             | 0.000936189 |\n",
      "|    std                | 1.5         |\n",
      "|    value_loss         | 0.000312    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 5            |\n",
      "|    iterations         | 900          |\n",
      "|    time_elapsed       | 822          |\n",
      "|    total_timesteps    | 4500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -647         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 899          |\n",
      "|    policy_loss        | 0.695        |\n",
      "|    reward             | 0.0011603058 |\n",
      "|    std                | 1.6          |\n",
      "|    value_loss         | 3.39e-06     |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 5           |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 913         |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -667        |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | 0.187       |\n",
      "|    reward             | 0.006456824 |\n",
      "|    std                | 1.69        |\n",
      "|    value_loss         | 1.85e-06    |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 5             |\n",
      "|    iterations         | 1100          |\n",
      "|    time_elapsed       | 1004          |\n",
      "|    total_timesteps    | 5500          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -685          |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 1099          |\n",
      "|    policy_loss        | 10.6          |\n",
      "|    reward             | -0.0067426274 |\n",
      "|    std                | 1.78          |\n",
      "|    value_loss         | 0.000274      |\n",
      "-----------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 2342417.0\n",
      "Final accumulative portfolio value: 2.342417\n",
      "Maximum DrawDown: -0.5773926130694009\n",
      "Sharpe ratio: 0.28300049967552926\n",
      "=================================\n",
      "---------------------------------------\n",
      "| rollout/              |             |\n",
      "|    ep_len_mean        | 5.79e+03    |\n",
      "|    ep_rew_mean        | 0.867       |\n",
      "| time/                 |             |\n",
      "|    fps                | 5           |\n",
      "|    iterations         | 1200        |\n",
      "|    time_elapsed       | 1100        |\n",
      "|    total_timesteps    | 6000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -701        |\n",
      "|    explained_variance | -1.96       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1199        |\n",
      "|    policy_loss        | 24.6        |\n",
      "|    reward             | 0.002217336 |\n",
      "|    std                | 1.87        |\n",
      "|    value_loss         | 0.00133     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/              |             |\n",
      "|    ep_len_mean        | 5.79e+03    |\n",
      "|    ep_rew_mean        | 0.867       |\n",
      "| time/                 |             |\n",
      "|    fps                | 5           |\n",
      "|    iterations         | 1300        |\n",
      "|    time_elapsed       | 1193        |\n",
      "|    total_timesteps    | 6500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -715        |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1299        |\n",
      "|    policy_loss        | 8.11        |\n",
      "|    reward             | 0.016860595 |\n",
      "|    std                | 1.95        |\n",
      "|    value_loss         | 0.000154    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/              |              |\n",
      "|    ep_len_mean        | 5.79e+03     |\n",
      "|    ep_rew_mean        | 0.867        |\n",
      "| time/                 |              |\n",
      "|    fps                | 5            |\n",
      "|    iterations         | 1400         |\n",
      "|    time_elapsed       | 1286         |\n",
      "|    total_timesteps    | 7000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -731         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1399         |\n",
      "|    policy_loss        | 8.99         |\n",
      "|    reward             | -0.001224731 |\n",
      "|    std                | 2.04         |\n",
      "|    value_loss         | 0.00016      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/              |              |\n",
      "|    ep_len_mean        | 5.79e+03     |\n",
      "|    ep_rew_mean        | 0.867        |\n",
      "| time/                 |              |\n",
      "|    fps                | 5            |\n",
      "|    iterations         | 1500         |\n",
      "|    time_elapsed       | 1379         |\n",
      "|    total_timesteps    | 7500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -751         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1499         |\n",
      "|    policy_loss        | -2.64        |\n",
      "|    reward             | 0.0059827063 |\n",
      "|    std                | 2.16         |\n",
      "|    value_loss         | 2.14e-05     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/              |              |\n",
      "|    ep_len_mean        | 5.79e+03     |\n",
      "|    ep_rew_mean        | 0.867        |\n",
      "| time/                 |              |\n",
      "|    fps                | 5            |\n",
      "|    iterations         | 1600         |\n",
      "|    time_elapsed       | 1471         |\n",
      "|    total_timesteps    | 8000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -767         |\n",
      "|    explained_variance | 1.19e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1599         |\n",
      "|    policy_loss        | -37          |\n",
      "|    reward             | -0.059589963 |\n",
      "|    std                | 2.27         |\n",
      "|    value_loss         | 0.00474      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/              |              |\n",
      "|    ep_len_mean        | 5.79e+03     |\n",
      "|    ep_rew_mean        | 0.867        |\n",
      "| time/                 |              |\n",
      "|    fps                | 5            |\n",
      "|    iterations         | 1700         |\n",
      "|    time_elapsed       | 1563         |\n",
      "|    total_timesteps    | 8500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -782         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1699         |\n",
      "|    policy_loss        | 2.02         |\n",
      "|    reward             | 0.0052277055 |\n",
      "|    std                | 2.36         |\n",
      "|    value_loss         | 5.05e-05     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/              |               |\n",
      "|    ep_len_mean        | 5.79e+03      |\n",
      "|    ep_rew_mean        | 0.867         |\n",
      "| time/                 |               |\n",
      "|    fps                | 5             |\n",
      "|    iterations         | 1800          |\n",
      "|    time_elapsed       | 1655          |\n",
      "|    total_timesteps    | 9000          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -799          |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 1799          |\n",
      "|    policy_loss        | 5.11          |\n",
      "|    reward             | -0.0058257673 |\n",
      "|    std                | 2.49          |\n",
      "|    value_loss         | 5.01e-05      |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/              |             |\n",
      "|    ep_len_mean        | 5.79e+03    |\n",
      "|    ep_rew_mean        | 0.867       |\n",
      "| time/                 |             |\n",
      "|    fps                | 5           |\n",
      "|    iterations         | 1900        |\n",
      "|    time_elapsed       | 1746        |\n",
      "|    total_timesteps    | 9500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -819        |\n",
      "|    explained_variance | 2.98e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1899        |\n",
      "|    policy_loss        | -14         |\n",
      "|    reward             | 0.017255887 |\n",
      "|    std                | 2.63        |\n",
      "|    value_loss         | 0.000322    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/              |              |\n",
      "|    ep_len_mean        | 5.79e+03     |\n",
      "|    ep_rew_mean        | 0.867        |\n",
      "| time/                 |              |\n",
      "|    fps                | 5            |\n",
      "|    iterations         | 2000         |\n",
      "|    time_elapsed       | 1838         |\n",
      "|    total_timesteps    | 10000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -837         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1999         |\n",
      "|    policy_loss        | -3.81        |\n",
      "|    reward             | -0.005188311 |\n",
      "|    std                | 2.78         |\n",
      "|    value_loss         | 7.6e-05      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/              |               |\n",
      "|    ep_len_mean        | 5.79e+03      |\n",
      "|    ep_rew_mean        | 0.867         |\n",
      "| time/                 |               |\n",
      "|    fps                | 5             |\n",
      "|    iterations         | 2100          |\n",
      "|    time_elapsed       | 1931          |\n",
      "|    total_timesteps    | 10500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -857          |\n",
      "|    explained_variance | -1.19e-07     |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 2099          |\n",
      "|    policy_loss        | 3.43          |\n",
      "|    reward             | -0.0006508561 |\n",
      "|    std                | 2.95          |\n",
      "|    value_loss         | 1.78e-05      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/              |              |\n",
      "|    ep_len_mean        | 5.79e+03     |\n",
      "|    ep_rew_mean        | 0.867        |\n",
      "| time/                 |              |\n",
      "|    fps                | 5            |\n",
      "|    iterations         | 2200         |\n",
      "|    time_elapsed       | 2023         |\n",
      "|    total_timesteps    | 11000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -875         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 2199         |\n",
      "|    policy_loss        | 12.6         |\n",
      "|    reward             | -0.022630386 |\n",
      "|    std                | 3.1          |\n",
      "|    value_loss         | 0.000273     |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/              |             |\n",
      "|    ep_len_mean        | 5.79e+03    |\n",
      "|    ep_rew_mean        | 0.867       |\n",
      "| time/                 |             |\n",
      "|    fps                | 5           |\n",
      "|    iterations         | 2300        |\n",
      "|    time_elapsed       | 2114        |\n",
      "|    total_timesteps    | 11500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -892        |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2299        |\n",
      "|    policy_loss        | 28.6        |\n",
      "|    reward             | 0.009469474 |\n",
      "|    std                | 3.27        |\n",
      "|    value_loss         | 0.00128     |\n",
      "---------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 2233130.0\n",
      "Final accumulative portfolio value: 2.23313\n",
      "Maximum DrawDown: -0.5810120418399373\n",
      "Sharpe ratio: 0.2731621269341315\n",
      "=================================\n",
      "---------------------------------------\n",
      "| rollout/              |             |\n",
      "|    ep_len_mean        | 5.79e+03    |\n",
      "|    ep_rew_mean        | 0.843       |\n",
      "| time/                 |             |\n",
      "|    fps                | 5           |\n",
      "|    iterations         | 2400        |\n",
      "|    time_elapsed       | 2210        |\n",
      "|    total_timesteps    | 12000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -909        |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2399        |\n",
      "|    policy_loss        | -18.2       |\n",
      "|    reward             | -0.01846581 |\n",
      "|    std                | 3.42        |\n",
      "|    value_loss         | 0.000418    |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/              |               |\n",
      "|    ep_len_mean        | 5.79e+03      |\n",
      "|    ep_rew_mean        | 0.843         |\n",
      "| time/                 |               |\n",
      "|    fps                | 5             |\n",
      "|    iterations         | 2500          |\n",
      "|    time_elapsed       | 2303          |\n",
      "|    total_timesteps    | 12500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -924          |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 2499          |\n",
      "|    policy_loss        | 4.92          |\n",
      "|    reward             | -0.0028472508 |\n",
      "|    std                | 3.58          |\n",
      "|    value_loss         | 5.97e-05      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/              |               |\n",
      "|    ep_len_mean        | 5.79e+03      |\n",
      "|    ep_rew_mean        | 0.843         |\n",
      "| time/                 |               |\n",
      "|    fps                | 5             |\n",
      "|    iterations         | 2600          |\n",
      "|    time_elapsed       | 2395          |\n",
      "|    total_timesteps    | 13000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -941          |\n",
      "|    explained_variance | -1.19e-07     |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 2599          |\n",
      "|    policy_loss        | 8.11          |\n",
      "|    reward             | -0.0047981036 |\n",
      "|    std                | 3.76          |\n",
      "|    value_loss         | 8.46e-05      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/              |              |\n",
      "|    ep_len_mean        | 5.79e+03     |\n",
      "|    ep_rew_mean        | 0.843        |\n",
      "| time/                 |              |\n",
      "|    fps                | 5            |\n",
      "|    iterations         | 2700         |\n",
      "|    time_elapsed       | 2486         |\n",
      "|    total_timesteps    | 13500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -959         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 2699         |\n",
      "|    policy_loss        | -22.9        |\n",
      "|    reward             | -0.012461198 |\n",
      "|    std                | 3.97         |\n",
      "|    value_loss         | 0.000633     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/              |               |\n",
      "|    ep_len_mean        | 5.79e+03      |\n",
      "|    ep_rew_mean        | 0.843         |\n",
      "| time/                 |               |\n",
      "|    fps                | 5             |\n",
      "|    iterations         | 2800          |\n",
      "|    time_elapsed       | 2578          |\n",
      "|    total_timesteps    | 14000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -974          |\n",
      "|    explained_variance | 1.19e-07      |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 2799          |\n",
      "|    policy_loss        | 25            |\n",
      "|    reward             | 0.00076324417 |\n",
      "|    std                | 4.14          |\n",
      "|    value_loss         | 0.000778      |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/              |             |\n",
      "|    ep_len_mean        | 5.79e+03    |\n",
      "|    ep_rew_mean        | 0.843       |\n",
      "| time/                 |             |\n",
      "|    fps                | 5           |\n",
      "|    iterations         | 2900        |\n",
      "|    time_elapsed       | 2670        |\n",
      "|    total_timesteps    | 14500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -990        |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2899        |\n",
      "|    policy_loss        | 25.1        |\n",
      "|    reward             | 0.034353826 |\n",
      "|    std                | 4.33        |\n",
      "|    value_loss         | 0.00129     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/              |             |\n",
      "|    ep_len_mean        | 5.79e+03    |\n",
      "|    ep_rew_mean        | 0.843       |\n",
      "| time/                 |             |\n",
      "|    fps                | 5           |\n",
      "|    iterations         | 3000        |\n",
      "|    time_elapsed       | 2761        |\n",
      "|    total_timesteps    | 15000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.01e+03   |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2999        |\n",
      "|    policy_loss        | -2.39       |\n",
      "|    reward             | 0.007963928 |\n",
      "|    std                | 4.55        |\n",
      "|    value_loss         | 1.17e-05    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/              |              |\n",
      "|    ep_len_mean        | 5.79e+03     |\n",
      "|    ep_rew_mean        | 0.843        |\n",
      "| time/                 |              |\n",
      "|    fps                | 5            |\n",
      "|    iterations         | 3100         |\n",
      "|    time_elapsed       | 2853         |\n",
      "|    total_timesteps    | 15500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.02e+03    |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 3099         |\n",
      "|    policy_loss        | -4.98        |\n",
      "|    reward             | 0.0050248997 |\n",
      "|    std                | 4.8          |\n",
      "|    value_loss         | 2.91e-05     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/              |              |\n",
      "|    ep_len_mean        | 5.79e+03     |\n",
      "|    ep_rew_mean        | 0.843        |\n",
      "| time/                 |              |\n",
      "|    fps                | 5            |\n",
      "|    iterations         | 3200         |\n",
      "|    time_elapsed       | 2944         |\n",
      "|    total_timesteps    | 16000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.04e+03    |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 3199         |\n",
      "|    policy_loss        | -4.41        |\n",
      "|    reward             | -0.002360744 |\n",
      "|    std                | 5.06         |\n",
      "|    value_loss         | 2.2e-05      |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 5.79e+03   |\n",
      "|    ep_rew_mean        | 0.843      |\n",
      "| time/                 |            |\n",
      "|    fps                | 5          |\n",
      "|    iterations         | 3300       |\n",
      "|    time_elapsed       | 3035       |\n",
      "|    total_timesteps    | 16500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.06e+03  |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3299       |\n",
      "|    policy_loss        | -3.2       |\n",
      "|    reward             | 0.00984704 |\n",
      "|    std                | 5.35       |\n",
      "|    value_loss         | 1.45e-05   |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/              |              |\n",
      "|    ep_len_mean        | 5.79e+03     |\n",
      "|    ep_rew_mean        | 0.843        |\n",
      "| time/                 |              |\n",
      "|    fps                | 5            |\n",
      "|    iterations         | 3400         |\n",
      "|    time_elapsed       | 3127         |\n",
      "|    total_timesteps    | 17000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.08e+03    |\n",
      "|    explained_variance | 5.96e-08     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 3399         |\n",
      "|    policy_loss        | 12.4         |\n",
      "|    reward             | -0.004033583 |\n",
      "|    std                | 5.61         |\n",
      "|    value_loss         | 0.000217     |\n",
      "----------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 1932170.25\n",
      "Final accumulative portfolio value: 1.93217025\n",
      "Maximum DrawDown: -0.5676461435128743\n",
      "Sharpe ratio: 0.24248599855349573\n",
      "=================================\n",
      "----------------------------------------\n",
      "| rollout/              |              |\n",
      "|    ep_len_mean        | 5.79e+03     |\n",
      "|    ep_rew_mean        | 0.786        |\n",
      "| time/                 |              |\n",
      "|    fps                | 5            |\n",
      "|    iterations         | 3500         |\n",
      "|    time_elapsed       | 3223         |\n",
      "|    total_timesteps    | 17500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.09e+03    |\n",
      "|    explained_variance | -10.4        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 3499         |\n",
      "|    policy_loss        | -14.3        |\n",
      "|    reward             | -0.006162819 |\n",
      "|    std                | 5.88         |\n",
      "|    value_loss         | 0.000213     |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/              |             |\n",
      "|    ep_len_mean        | 5.79e+03    |\n",
      "|    ep_rew_mean        | 0.786       |\n",
      "| time/                 |             |\n",
      "|    fps                | 5           |\n",
      "|    iterations         | 3600        |\n",
      "|    time_elapsed       | 3314        |\n",
      "|    total_timesteps    | 18000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.11e+03   |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3599        |\n",
      "|    policy_loss        | -57.6       |\n",
      "|    reward             | 0.007422252 |\n",
      "|    std                | 6.15        |\n",
      "|    value_loss         | 0.00284     |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/              |               |\n",
      "|    ep_len_mean        | 5.79e+03      |\n",
      "|    ep_rew_mean        | 0.786         |\n",
      "| time/                 |               |\n",
      "|    fps                | 5             |\n",
      "|    iterations         | 3700          |\n",
      "|    time_elapsed       | 3406          |\n",
      "|    total_timesteps    | 18500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.12e+03     |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 3699          |\n",
      "|    policy_loss        | -3.85         |\n",
      "|    reward             | -0.0014573222 |\n",
      "|    std                | 6.42          |\n",
      "|    value_loss         | 1.5e-05       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/              |               |\n",
      "|    ep_len_mean        | 5.79e+03      |\n",
      "|    ep_rew_mean        | 0.786         |\n",
      "| time/                 |               |\n",
      "|    fps                | 5             |\n",
      "|    iterations         | 3800          |\n",
      "|    time_elapsed       | 3499          |\n",
      "|    total_timesteps    | 19000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.14e+03     |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 3799          |\n",
      "|    policy_loss        | -13.2         |\n",
      "|    reward             | -0.0023950387 |\n",
      "|    std                | 6.75          |\n",
      "|    value_loss         | 0.00018       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/              |               |\n",
      "|    ep_len_mean        | 5.79e+03      |\n",
      "|    ep_rew_mean        | 0.786         |\n",
      "| time/                 |               |\n",
      "|    fps                | 5             |\n",
      "|    iterations         | 3900          |\n",
      "|    time_elapsed       | 3591          |\n",
      "|    total_timesteps    | 19500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.16e+03     |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 3899          |\n",
      "|    policy_loss        | -20.1         |\n",
      "|    reward             | -0.0115344385 |\n",
      "|    std                | 7.1           |\n",
      "|    value_loss         | 0.000358      |\n",
      "-----------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 5.79e+03  |\n",
      "|    ep_rew_mean        | 0.786     |\n",
      "| time/                 |           |\n",
      "|    fps                | 5         |\n",
      "|    iterations         | 4000      |\n",
      "|    time_elapsed       | 3683      |\n",
      "|    total_timesteps    | 20000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.17e+03 |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3999      |\n",
      "|    policy_loss        | -43.9     |\n",
      "|    reward             | 0.033492  |\n",
      "|    std                | 7.38      |\n",
      "|    value_loss         | 0.00145   |\n",
      "-------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/              |                |\n",
      "|    ep_len_mean        | 5.79e+03       |\n",
      "|    ep_rew_mean        | 0.786          |\n",
      "| time/                 |                |\n",
      "|    fps                | 5              |\n",
      "|    iterations         | 4100           |\n",
      "|    time_elapsed       | 3776           |\n",
      "|    total_timesteps    | 20500          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -1.19e+03      |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 4099           |\n",
      "|    policy_loss        | -20.4          |\n",
      "|    reward             | -0.00071789755 |\n",
      "|    std                | 7.7            |\n",
      "|    value_loss         | 0.000365       |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/              |              |\n",
      "|    ep_len_mean        | 5.79e+03     |\n",
      "|    ep_rew_mean        | 0.786        |\n",
      "| time/                 |              |\n",
      "|    fps                | 5            |\n",
      "|    iterations         | 4200         |\n",
      "|    time_elapsed       | 3868         |\n",
      "|    total_timesteps    | 21000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.2e+03     |\n",
      "|    explained_variance | 1.19e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 4199         |\n",
      "|    policy_loss        | 9.42         |\n",
      "|    reward             | -0.001410525 |\n",
      "|    std                | 8.07         |\n",
      "|    value_loss         | 7.89e-05     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/              |              |\n",
      "|    ep_len_mean        | 5.79e+03     |\n",
      "|    ep_rew_mean        | 0.786        |\n",
      "| time/                 |              |\n",
      "|    fps                | 5            |\n",
      "|    iterations         | 4300         |\n",
      "|    time_elapsed       | 3959         |\n",
      "|    total_timesteps    | 21500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.22e+03    |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 4299         |\n",
      "|    policy_loss        | -23.1        |\n",
      "|    reward             | 0.0010669497 |\n",
      "|    std                | 8.49         |\n",
      "|    value_loss         | 0.000435     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/              |               |\n",
      "|    ep_len_mean        | 5.79e+03      |\n",
      "|    ep_rew_mean        | 0.786         |\n",
      "| time/                 |               |\n",
      "|    fps                | 5             |\n",
      "|    iterations         | 4400          |\n",
      "|    time_elapsed       | 4051          |\n",
      "|    total_timesteps    | 22000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.24e+03     |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 4399          |\n",
      "|    policy_loss        | 11.1          |\n",
      "|    reward             | -0.0002147786 |\n",
      "|    std                | 8.96          |\n",
      "|    value_loss         | 9.79e-05      |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/              |             |\n",
      "|    ep_len_mean        | 5.79e+03    |\n",
      "|    ep_rew_mean        | 0.786       |\n",
      "| time/                 |             |\n",
      "|    fps                | 5           |\n",
      "|    iterations         | 4500        |\n",
      "|    time_elapsed       | 4143        |\n",
      "|    total_timesteps    | 22500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.26e+03   |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4499        |\n",
      "|    policy_loss        | 48.8        |\n",
      "|    reward             | -0.06933676 |\n",
      "|    std                | 9.43        |\n",
      "|    value_loss         | 0.00186     |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/              |              |\n",
      "|    ep_len_mean        | 5.79e+03     |\n",
      "|    ep_rew_mean        | 0.786        |\n",
      "| time/                 |              |\n",
      "|    fps                | 5            |\n",
      "|    iterations         | 4600         |\n",
      "|    time_elapsed       | 4235         |\n",
      "|    total_timesteps    | 23000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.27e+03    |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 4599         |\n",
      "|    policy_loss        | 60.3         |\n",
      "|    reward             | 0.0024014462 |\n",
      "|    std                | 9.89         |\n",
      "|    value_loss         | 0.0025       |\n",
      "----------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 1573512.25\n",
      "Final accumulative portfolio value: 1.57351225\n",
      "Maximum DrawDown: -0.6015725288337277\n",
      "Sharpe ratio: 0.19908821773622687\n",
      "=================================\n",
      "----------------------------------------\n",
      "| rollout/              |              |\n",
      "|    ep_len_mean        | 5.79e+03     |\n",
      "|    ep_rew_mean        | 0.707        |\n",
      "| time/                 |              |\n",
      "|    fps                | 5            |\n",
      "|    iterations         | 4700         |\n",
      "|    time_elapsed       | 4331         |\n",
      "|    total_timesteps    | 23500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.29e+03    |\n",
      "|    explained_variance | 1.19e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 4699         |\n",
      "|    policy_loss        | -8.07        |\n",
      "|    reward             | 0.0031247148 |\n",
      "|    std                | 10.3         |\n",
      "|    value_loss         | 0.000103     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/              |              |\n",
      "|    ep_len_mean        | 5.79e+03     |\n",
      "|    ep_rew_mean        | 0.707        |\n",
      "| time/                 |              |\n",
      "|    fps                | 5            |\n",
      "|    iterations         | 4800         |\n",
      "|    time_elapsed       | 4423         |\n",
      "|    total_timesteps    | 24000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.3e+03     |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 4799         |\n",
      "|    policy_loss        | 40.9         |\n",
      "|    reward             | 0.0011544713 |\n",
      "|    std                | 10.8         |\n",
      "|    value_loss         | 0.00104      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/              |             |\n",
      "|    ep_len_mean        | 5.79e+03    |\n",
      "|    ep_rew_mean        | 0.707       |\n",
      "| time/                 |             |\n",
      "|    fps                | 5           |\n",
      "|    iterations         | 4900        |\n",
      "|    time_elapsed       | 4515        |\n",
      "|    total_timesteps    | 24500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.32e+03   |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4899        |\n",
      "|    policy_loss        | 30.8        |\n",
      "|    reward             | 0.007053948 |\n",
      "|    std                | 11.3        |\n",
      "|    value_loss         | 0.00069     |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/              |               |\n",
      "|    ep_len_mean        | 5.79e+03      |\n",
      "|    ep_rew_mean        | 0.707         |\n",
      "| time/                 |               |\n",
      "|    fps                | 5             |\n",
      "|    iterations         | 5000          |\n",
      "|    time_elapsed       | 4608          |\n",
      "|    total_timesteps    | 25000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.34e+03     |\n",
      "|    explained_variance | -1.19e-07     |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 4999          |\n",
      "|    policy_loss        | -0.285        |\n",
      "|    reward             | -0.0031615947 |\n",
      "|    std                | 11.9          |\n",
      "|    value_loss         | 3.04e-05      |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/              |             |\n",
      "|    ep_len_mean        | 5.79e+03    |\n",
      "|    ep_rew_mean        | 0.707       |\n",
      "| time/                 |             |\n",
      "|    fps                | 5           |\n",
      "|    iterations         | 5100        |\n",
      "|    time_elapsed       | 4701        |\n",
      "|    total_timesteps    | 25500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.35e+03   |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5099        |\n",
      "|    policy_loss        | -2.59       |\n",
      "|    reward             | 0.038069185 |\n",
      "|    std                | 12.5        |\n",
      "|    value_loss         | 0.000141    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/              |              |\n",
      "|    ep_len_mean        | 5.79e+03     |\n",
      "|    ep_rew_mean        | 0.707        |\n",
      "| time/                 |              |\n",
      "|    fps                | 5            |\n",
      "|    iterations         | 5200         |\n",
      "|    time_elapsed       | 4799         |\n",
      "|    total_timesteps    | 26000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.37e+03    |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 5199         |\n",
      "|    policy_loss        | -19          |\n",
      "|    reward             | -0.009674346 |\n",
      "|    std                | 13           |\n",
      "|    value_loss         | 0.000202     |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 5.79e+03   |\n",
      "|    ep_rew_mean        | 0.707      |\n",
      "| time/                 |            |\n",
      "|    fps                | 5          |\n",
      "|    iterations         | 5300       |\n",
      "|    time_elapsed       | 4900       |\n",
      "|    total_timesteps    | 26500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.38e+03  |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5299       |\n",
      "|    policy_loss        | -9.55      |\n",
      "|    reward             | 0.00469558 |\n",
      "|    std                | 13.6       |\n",
      "|    value_loss         | 6e-05      |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/              |              |\n",
      "|    ep_len_mean        | 5.79e+03     |\n",
      "|    ep_rew_mean        | 0.707        |\n",
      "| time/                 |              |\n",
      "|    fps                | 5            |\n",
      "|    iterations         | 5400         |\n",
      "|    time_elapsed       | 4995         |\n",
      "|    total_timesteps    | 27000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.4e+03     |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 5399         |\n",
      "|    policy_loss        | -5.26        |\n",
      "|    reward             | 0.0032139577 |\n",
      "|    std                | 14.2         |\n",
      "|    value_loss         | 1.85e-05     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/              |              |\n",
      "|    ep_len_mean        | 5.79e+03     |\n",
      "|    ep_rew_mean        | 0.707        |\n",
      "| time/                 |              |\n",
      "|    fps                | 5            |\n",
      "|    iterations         | 5500         |\n",
      "|    time_elapsed       | 5102         |\n",
      "|    total_timesteps    | 27500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.41e+03    |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 5499         |\n",
      "|    policy_loss        | 9.33         |\n",
      "|    reward             | 3.182837e-05 |\n",
      "|    std                | 15           |\n",
      "|    value_loss         | 5.2e-05      |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/              |                |\n",
      "|    ep_len_mean        | 5.79e+03       |\n",
      "|    ep_rew_mean        | 0.707          |\n",
      "| time/                 |                |\n",
      "|    fps                | 5              |\n",
      "|    iterations         | 5600           |\n",
      "|    time_elapsed       | 5201           |\n",
      "|    total_timesteps    | 28000          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -1.43e+03      |\n",
      "|    explained_variance | -1.19e-07      |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 5599           |\n",
      "|    policy_loss        | 0.55           |\n",
      "|    reward             | -0.00060570694 |\n",
      "|    std                | 15.8           |\n",
      "|    value_loss         | 1.48e-05       |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/              |              |\n",
      "|    ep_len_mean        | 5.79e+03     |\n",
      "|    ep_rew_mean        | 0.707        |\n",
      "| time/                 |              |\n",
      "|    fps                | 5            |\n",
      "|    iterations         | 5700         |\n",
      "|    time_elapsed       | 5293         |\n",
      "|    total_timesteps    | 28500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.45e+03    |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 5699         |\n",
      "|    policy_loss        | -0.87        |\n",
      "|    reward             | -0.004324714 |\n",
      "|    std                | 16.6         |\n",
      "|    value_loss         | 1.13e-05     |\n",
      "----------------------------------------\n",
      "=================================\n",
      "Initial portfolio value:1000000\n",
      "Final portfolio value: 1504826.0\n",
      "Final accumulative portfolio value: 1.504826\n",
      "Maximum DrawDown: -0.5910397081453133\n",
      "Sharpe ratio: 0.18965838763609216\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| rollout/              |               |\n",
      "|    ep_len_mean        | 5.79e+03      |\n",
      "|    ep_rew_mean        | 0.65          |\n",
      "| time/                 |               |\n",
      "|    fps                | 5             |\n",
      "|    iterations         | 5800          |\n",
      "|    time_elapsed       | 5390          |\n",
      "|    total_timesteps    | 29000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.47e+03     |\n",
      "|    explained_variance | 0.456         |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 5799          |\n",
      "|    policy_loss        | 1.33          |\n",
      "|    reward             | -0.0021671874 |\n",
      "|    std                | 17.4          |\n",
      "|    value_loss         | 3.26e-05      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/              |              |\n",
      "|    ep_len_mean        | 5.79e+03     |\n",
      "|    ep_rew_mean        | 0.65         |\n",
      "| time/                 |              |\n",
      "|    fps                | 5            |\n",
      "|    iterations         | 5900         |\n",
      "|    time_elapsed       | 5484         |\n",
      "|    total_timesteps    | 29500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.48e+03    |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 5899         |\n",
      "|    policy_loss        | -3.3         |\n",
      "|    reward             | -0.004876863 |\n",
      "|    std                | 18.2         |\n",
      "|    value_loss         | 0.0001       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/              |               |\n",
      "|    ep_len_mean        | 5.79e+03      |\n",
      "|    ep_rew_mean        | 0.65          |\n",
      "| time/                 |               |\n",
      "|    fps                | 5             |\n",
      "|    iterations         | 6000          |\n",
      "|    time_elapsed       | 5580          |\n",
      "|    total_timesteps    | 30000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.5e+03      |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 5999          |\n",
      "|    policy_loss        | 24.9          |\n",
      "|    reward             | -0.0040340624 |\n",
      "|    std                | 19            |\n",
      "|    value_loss         | 0.000418      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/              |               |\n",
      "|    ep_len_mean        | 5.79e+03      |\n",
      "|    ep_rew_mean        | 0.65          |\n",
      "| time/                 |               |\n",
      "|    fps                | 5             |\n",
      "|    iterations         | 6100          |\n",
      "|    time_elapsed       | 5678          |\n",
      "|    total_timesteps    | 30500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.51e+03     |\n",
      "|    explained_variance | -1.19e-07     |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 6099          |\n",
      "|    policy_loss        | -4.36         |\n",
      "|    reward             | -0.0018519014 |\n",
      "|    std                | 20            |\n",
      "|    value_loss         | 1.41e-05      |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/              |             |\n",
      "|    ep_len_mean        | 5.79e+03    |\n",
      "|    ep_rew_mean        | 0.65        |\n",
      "| time/                 |             |\n",
      "|    fps                | 5           |\n",
      "|    iterations         | 6200        |\n",
      "|    time_elapsed       | 5770        |\n",
      "|    total_timesteps    | 31000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.53e+03   |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6199        |\n",
      "|    policy_loss        | 65          |\n",
      "|    reward             | 0.007463429 |\n",
      "|    std                | 21          |\n",
      "|    value_loss         | 0.0021      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/              |              |\n",
      "|    ep_len_mean        | 5.79e+03     |\n",
      "|    ep_rew_mean        | 0.65         |\n",
      "| time/                 |              |\n",
      "|    fps                | 5            |\n",
      "|    iterations         | 6300         |\n",
      "|    time_elapsed       | 5864         |\n",
      "|    total_timesteps    | 31500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.54e+03    |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 6299         |\n",
      "|    policy_loss        | 16.8         |\n",
      "|    reward             | -0.006949316 |\n",
      "|    std                | 21.8         |\n",
      "|    value_loss         | 0.000124     |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/              |                |\n",
      "|    ep_len_mean        | 5.79e+03       |\n",
      "|    ep_rew_mean        | 0.65           |\n",
      "| time/                 |                |\n",
      "|    fps                | 5              |\n",
      "|    iterations         | 6400           |\n",
      "|    time_elapsed       | 5957           |\n",
      "|    total_timesteps    | 32000          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -1.56e+03      |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 6399           |\n",
      "|    policy_loss        | 11.5           |\n",
      "|    reward             | -0.00084370194 |\n",
      "|    std                | 22.7           |\n",
      "|    value_loss         | 0.00013        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/              |               |\n",
      "|    ep_len_mean        | 5.79e+03      |\n",
      "|    ep_rew_mean        | 0.65          |\n",
      "| time/                 |               |\n",
      "|    fps                | 5             |\n",
      "|    iterations         | 6500          |\n",
      "|    time_elapsed       | 6056          |\n",
      "|    total_timesteps    | 32500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.57e+03     |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 6499          |\n",
      "|    policy_loss        | 0.903         |\n",
      "|    reward             | -0.0009200632 |\n",
      "|    std                | 23.8          |\n",
      "|    value_loss         | 1.93e-05      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/              |              |\n",
      "|    ep_len_mean        | 5.79e+03     |\n",
      "|    ep_rew_mean        | 0.65         |\n",
      "| time/                 |              |\n",
      "|    fps                | 5            |\n",
      "|    iterations         | 6600         |\n",
      "|    time_elapsed       | 6159         |\n",
      "|    total_timesteps    | 33000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.59e+03    |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 6599         |\n",
      "|    policy_loss        | 41.1         |\n",
      "|    reward             | 0.0033273117 |\n",
      "|    std                | 25           |\n",
      "|    value_loss         | 0.000768     |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/              |             |\n",
      "|    ep_len_mean        | 5.79e+03    |\n",
      "|    ep_rew_mean        | 0.65        |\n",
      "| time/                 |             |\n",
      "|    fps                | 5           |\n",
      "|    iterations         | 6700        |\n",
      "|    time_elapsed       | 6264        |\n",
      "|    total_timesteps    | 33500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.61e+03   |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6699        |\n",
      "|    policy_loss        | 22.8        |\n",
      "|    reward             | 0.010764036 |\n",
      "|    std                | 26.4        |\n",
      "|    value_loss         | 0.000248    |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(models):\n\u001b[0;32m     36\u001b[0m     model \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_model(m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m], model_kwargs\u001b[38;5;241m=\u001b[39mm[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 37\u001b[0m     ppo_model \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100_000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m     training_summary \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[0;32m     41\u001b[0m         {\n\u001b[0;32m     42\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_environment\u001b[38;5;241m.\u001b[39m_date_memory,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m         }\n\u001b[0;32m     49\u001b[0m     )\n\u001b[0;32m     50\u001b[0m     prediction_summary \u001b[38;5;241m=\u001b[39m DRL_prediction(ppo_model, test_environment, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\versu\\Documents\\Thesis\\models.py:120\u001b[0m, in \u001b[0;36mDRLAgent.train_model\u001b[1;34m(model, tb_log_name, total_timesteps)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(\n\u001b[0;32m    118\u001b[0m     model, tb_log_name, total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m\n\u001b[0;32m    119\u001b[0m ):  \u001b[38;5;66;03m# this function is static method, so it can be called without creating an instance of the class\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTensorboardCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\versu\\anaconda3\\envs\\Thesis\\lib\\site-packages\\stable_baselines3\\a2c\\a2c.py:201\u001b[0m, in \u001b[0;36mA2C.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfA2C,\n\u001b[0;32m    194\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    199\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    200\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfA2C:\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\versu\\anaconda3\\envs\\Thesis\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:300\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 300\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\versu\\anaconda3\\envs\\Thesis\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:195\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[0;32m    193\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[1;32m--> 195\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\versu\\anaconda3\\envs\\Thesis\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\versu\\anaconda3\\envs\\Thesis\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[1;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[1;32mc:\\Users\\versu\\anaconda3\\envs\\Thesis\\lib\\site-packages\\stable_baselines3\\common\\monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "File \u001b[1;32mc:\\Users\\versu\\anaconda3\\envs\\Thesis\\lib\\site-packages\\shimmy\\openai_gym_compatibility.py:251\u001b[0m, in \u001b[0;36mGymV21CompatibilityV0.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: ActType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[Any, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[0;32m    243\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment.\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \n\u001b[0;32m    245\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m        (observation, reward, terminated, truncated, info)\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgym_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[1;32mc:\\Users\\versu\\Documents\\Thesis\\portfolio_optimization_env.py:309\u001b[0m, in \u001b[0;36mPortfolioOptimizationEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;66;03m# load next state\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time_index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 309\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_state_and_info_from_time_index\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_time_index\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;66;03m# if using weights vector modifier, we need to modify weights vector\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_comission_fee_model \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwvm\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\versu\\Documents\\Thesis\\portfolio_optimization_env.py:470\u001b[0m, in \u001b[0;36mPortfolioOptimizationEnv._get_state_and_info_from_time_index\u001b[1;34m(self, time_index)\u001b[0m\n\u001b[0;32m    466\u001b[0m features \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_features \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mstrip(\n\u001b[0;32m    467\u001b[0m )\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tic \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tic_list:\n\u001b[1;32m--> 470\u001b[0m     tic_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tic_column\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtic\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    471\u001b[0m     tic_data \u001b[38;5;241m=\u001b[39m tic_data[features\n\u001b[0;32m    472\u001b[0m                         ]\u001b[38;5;241m.\u001b[39mto_numpy()\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m    473\u001b[0m     tic_data \u001b[38;5;241m=\u001b[39m tic_data[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, np\u001b[38;5;241m.\u001b[39mnewaxis]\n",
      "File \u001b[1;32mc:\\Users\\versu\\anaconda3\\envs\\Thesis\\lib\\site-packages\\pandas\\core\\frame.py:4093\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4091\u001b[0m \u001b[38;5;66;03m# Do we have a (boolean) 1d indexer?\u001b[39;00m\n\u001b[0;32m   4092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_bool_indexer(key):\n\u001b[1;32m-> 4093\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_bool_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4095\u001b[0m \u001b[38;5;66;03m# We are left with two options: a single key, and a collection of keys,\u001b[39;00m\n\u001b[0;32m   4096\u001b[0m \u001b[38;5;66;03m# We interpret tuples as collections only for non-MultiIndex\u001b[39;00m\n\u001b[0;32m   4097\u001b[0m is_single_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(key)\n",
      "File \u001b[1;32mc:\\Users\\versu\\anaconda3\\envs\\Thesis\\lib\\site-packages\\pandas\\core\\frame.py:4155\u001b[0m, in \u001b[0;36mDataFrame._getitem_bool_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   4154\u001b[0m indexer \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 4155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_with_is_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\versu\\anaconda3\\envs\\Thesis\\lib\\site-packages\\pandas\\core\\generic.py:4153\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[1;34m(self, indices, axis)\u001b[0m\n\u001b[0;32m   4142\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   4143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_with_is_copy\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices, axis: Axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m   4144\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4145\u001b[0m \u001b[38;5;124;03m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[0;32m   4146\u001b[0m \u001b[38;5;124;03m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4151\u001b[0m \u001b[38;5;124;03m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[0;32m   4152\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4153\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4154\u001b[0m     \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[0;32m   4155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39m_get_axis(axis)\u001b[38;5;241m.\u001b[39mequals(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(axis)):\n",
      "File \u001b[1;32mc:\\Users\\versu\\anaconda3\\envs\\Thesis\\lib\\site-packages\\pandas\\core\\generic.py:4133\u001b[0m, in \u001b[0;36mNDFrame.take\u001b[1;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[0;32m   4128\u001b[0m     \u001b[38;5;66;03m# We can get here with a slice via DataFrame.__getitem__\u001b[39;00m\n\u001b[0;32m   4129\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\n\u001b[0;32m   4130\u001b[0m         indices\u001b[38;5;241m.\u001b[39mstart, indices\u001b[38;5;241m.\u001b[39mstop, indices\u001b[38;5;241m.\u001b[39mstep, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp\n\u001b[0;32m   4131\u001b[0m     )\n\u001b[1;32m-> 4133\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4135\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4137\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[0;32m   4139\u001b[0m     \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtake\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4140\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\versu\\anaconda3\\envs\\Thesis\\lib\\site-packages\\pandas\\core\\internals\\managers.py:894\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[1;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[0;32m    891\u001b[0m indexer \u001b[38;5;241m=\u001b[39m maybe_convert_indices(indexer, n, verify\u001b[38;5;241m=\u001b[39mverify)\n\u001b[0;32m    893\u001b[0m new_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    898\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\versu\\anaconda3\\envs\\Thesis\\lib\\site-packages\\pandas\\core\\internals\\managers.py:687\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[0;32m    680\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice_take_blocks_ax0(\n\u001b[0;32m    681\u001b[0m         indexer,\n\u001b[0;32m    682\u001b[0m         fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[0;32m    683\u001b[0m         only_slice\u001b[38;5;241m=\u001b[39monly_slice,\n\u001b[0;32m    684\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39muse_na_proxy,\n\u001b[0;32m    685\u001b[0m     )\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 687\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    688\u001b[0m         blk\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[0;32m    689\u001b[0m             indexer,\n\u001b[0;32m    690\u001b[0m             axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    691\u001b[0m             fill_value\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    692\u001b[0m                 fill_value \u001b[38;5;28;01mif\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mfill_value\n\u001b[0;32m    693\u001b[0m             ),\n\u001b[0;32m    694\u001b[0m         )\n\u001b[0;32m    695\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[0;32m    696\u001b[0m     ]\n\u001b[0;32m    698\u001b[0m new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m    699\u001b[0m new_axes[axis] \u001b[38;5;241m=\u001b[39m new_axis\n",
      "File \u001b[1;32mc:\\Users\\versu\\anaconda3\\envs\\Thesis\\lib\\site-packages\\pandas\\core\\internals\\managers.py:688\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    680\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice_take_blocks_ax0(\n\u001b[0;32m    681\u001b[0m         indexer,\n\u001b[0;32m    682\u001b[0m         fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[0;32m    683\u001b[0m         only_slice\u001b[38;5;241m=\u001b[39monly_slice,\n\u001b[0;32m    684\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39muse_na_proxy,\n\u001b[0;32m    685\u001b[0m     )\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    687\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 688\u001b[0m         \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m            \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    695\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[0;32m    696\u001b[0m     ]\n\u001b[0;32m    698\u001b[0m new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m    699\u001b[0m new_axes[axis] \u001b[38;5;241m=\u001b[39m new_axis\n",
      "File \u001b[1;32mc:\\Users\\versu\\anaconda3\\envs\\Thesis\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:1307\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[0;32m   1304\u001b[0m     allow_fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[1;32m-> 1307\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43malgos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[0;32m   1309\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1311\u001b[0m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[0;32m   1314\u001b[0m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[0;32m   1315\u001b[0m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\versu\\anaconda3\\envs\\Thesis\\lib\\site-packages\\pandas\\core\\array_algos\\take.py:110\u001b[0m, in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_1d_only_ea_dtype(arr\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[0;32m    108\u001b[0m         \u001b[38;5;66;03m# i.e. DatetimeArray, TimedeltaArray\u001b[39;00m\n\u001b[0;32m    109\u001b[0m         arr \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNDArrayBackedExtensionArray\u001b[39m\u001b[38;5;124m\"\u001b[39m, arr)\n\u001b[1;32m--> 110\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mtake(indexer, fill_value\u001b[38;5;241m=\u001b[39mfill_value, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill)\n\u001b[0;32m    116\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(arr)\n",
      "File \u001b[1;32mc:\\Users\\versu\\anaconda3\\envs\\Thesis\\lib\\site-packages\\pandas\\core\\arrays\\_mixins.py:168\u001b[0m, in \u001b[0;36mNDArrayBackedExtensionArray.take\u001b[1;34m(self, indices, allow_fill, fill_value, axis)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_fill:\n\u001b[0;32m    166\u001b[0m     fill_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_scalar(fill_value)\n\u001b[1;32m--> 168\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ndarray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_from_backing_data(new_data)\n",
      "File \u001b[1;32mc:\\Users\\versu\\anaconda3\\envs\\Thesis\\lib\\site-packages\\pandas\\core\\algorithms.py:1233\u001b[0m, in \u001b[0;36mtake\u001b[1;34m(arr, indices, axis, allow_fill, fill_value)\u001b[0m\n\u001b[0;32m   1229\u001b[0m indices \u001b[38;5;241m=\u001b[39m ensure_platform_int(indices)\n\u001b[0;32m   1231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_fill:\n\u001b[0;32m   1232\u001b[0m     \u001b[38;5;66;03m# Pandas style, -1 means NA\u001b[39;00m\n\u001b[1;32m-> 1233\u001b[0m     \u001b[43mvalidate_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1234\u001b[0m     result \u001b[38;5;241m=\u001b[39m take_nd(\n\u001b[0;32m   1235\u001b[0m         arr, indices, axis\u001b[38;5;241m=\u001b[39maxis, allow_fill\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fill_value\u001b[38;5;241m=\u001b[39mfill_value\n\u001b[0;32m   1236\u001b[0m     )\n\u001b[0;32m   1237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1238\u001b[0m     \u001b[38;5;66;03m# NumPy style\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\versu\\anaconda3\\envs\\Thesis\\lib\\site-packages\\pandas\\core\\indexers\\utils.py:230\u001b[0m, in \u001b[0;36mvalidate_indices\u001b[1;34m(indices, n)\u001b[0m\n\u001b[0;32m    227\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindices\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m contains values less than allowed (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmin_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m < -1)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m--> 230\u001b[0m max_idx \u001b[38;5;241m=\u001b[39m \u001b[43mindices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_idx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m n:\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindices are out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "final_result = []\n",
    "models = [\n",
    "    {'name': 'ppo', 'args': {\n",
    "        \"n_steps\": 2048,\n",
    "        \"ent_coef\": 0.01,\n",
    "        \"learning_rate\": 0.0003,\n",
    "        \"batch_size\": 128,\n",
    "    }},\n",
    "    {'name': 'a2c', 'args': None}\n",
    "]\n",
    "env_kwargs = {\n",
    "    \"initial_amount\": 100_0000,\n",
    "    \"normalize_df\": None,\n",
    "    \"features\": [\"close\", \"return\",\"turbulence\"],\n",
    "    'comission_fee_pct': 0.001,\n",
    "    'time_window': 1\n",
    "}\n",
    "ticks = []\n",
    "number_of_stocks = [3, 6 ,10]\n",
    "for index, number_of_stock in enumerate(number_of_stocks):\n",
    "    result = {}\n",
    "\n",
    "    train_data_sample = train_data.copy()\n",
    "    test_data_sample = test_data.copy()\n",
    "\n",
    "    sample_tics = np.random.choice(\n",
    "        train_data.tic.unique(), number_of_stock, replace=True)\n",
    "    train_data_sample = train_data_sample[train_data_sample['tic'].isin(\n",
    "        sample_tics)]\n",
    "    test_data_sample = test_data_sample[train_data_sample['tic'].isin(sample_tics)]\n",
    "\n",
    "    train_environment = PortfolioOptimizationEnv(df=train_data, **env_kwargs)\n",
    "    test_environment = PortfolioOptimizationEnv(df=test_data, **env_kwargs)\n",
    "    agent = DRLAgent(env=train_environment)\n",
    "    for i, m in enumerate(models):\n",
    "        model = agent.get_model(m['name'], model_kwargs=m['args'])\n",
    "        ppo_model = agent.train_model(model=model,\n",
    "                                      tb_log_name = m['name'],\n",
    "                                      total_timesteps=100_000)\n",
    "        training_summary = pd.DataFrame(\n",
    "            {\n",
    "                \"date\": train_environment._date_memory,\n",
    "                \"actions\": train_environment._actions_memory,\n",
    "                \"weights\": train_environment._final_weights,\n",
    "                \"returns\": train_environment._portfolio_return_memory,\n",
    "                \"rewards\": train_environment._portfolio_reward_memory,\n",
    "                \"portfolio_values\": train_environment._asset_memory[\"final\"],\n",
    "            }\n",
    "        )\n",
    "        prediction_summary = DRL_prediction(ppo_model, test_environment, 1)\n",
    "        result[\"train\"] = training_summary\n",
    "        result[\"test\"] = prediction_summary\n",
    "        result[\"name\"] = m['name']\n",
    "        result[\"sample_size\"] = sample_tics\n",
    "        final_result.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./models/test.pkl','wb') as f:\n",
    "    pickle.dump(final_result,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypfopt import expected_returns\n",
    "\n",
    "df = df.sort_values(['date', 'tic'], ignore_index=True)\n",
    "df.index = df.date.factorize()[0]\n",
    "cov_list = []\n",
    "mu= []\n",
    "# look back is one year\n",
    "lookback = 252\n",
    "for i in range(lookback, len(df.index.unique())):\n",
    "    data_lookback = df.loc[i-lookback:i, :]\n",
    "    price_lookback = data_lookback.pivot_table(\n",
    "        index='date', columns='tic', values='close')\n",
    "    return_lookback = price_lookback.pct_change().dropna()\n",
    "    covs = return_lookback.cov().values\n",
    "    mu.append(expected_returns.mean_historical_return(price_lookback))\n",
    "    cov_list.append(covs)\n",
    "df_cov = pd.DataFrame(\n",
    "{'time': df.date.unique()[lookback:], 'cov_list': cov_list,'returns':mu})\n",
    "df = df.merge(df_cov, left_on='date',right_on='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.evn_mvo import StockPortfolioEnv\n",
    "from agents.mvo_agent import MarkowitzAgent\n",
    "\n",
    "test_df = data_split(\n",
    "df,\n",
    "start=TEST_START_DATE,\n",
    "end=TEST_END_DATE\n",
    ")\n",
    "\n",
    "stock_dimension = len(test_df.tic.unique())\n",
    "state_space = stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n",
    "\n",
    "env_kwargs = {\n",
    "\"hmax\": 100,\n",
    "\"initial_amount\": 1000000,\n",
    "\"transaction_cost_pct\": 0.001,\n",
    "\"state_space\": state_space,\n",
    "\"stock_dim\": stock_dimension,\n",
    "\"tech_indicator_list\": INDICATORS,\n",
    "\"action_space\": stock_dimension,\n",
    "\"reward_scaling\": 1e-4\n",
    "\n",
    "}\n",
    "e_test_gym = StockPortfolioEnv(df=test_df, **env_kwargs)\n",
    "agent = MarkowitzAgent(e_test_gym)\n",
    "markowitz_history_df = agent.prediction(e_test_gym)\n",
    "markowitz_history_df[\"method\"] = \"markowitz\"\n",
    "\n",
    "stock_dimension = len(test_df.tic.unique())\n",
    "state_space = stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n",
    "\n",
    "env_kwargs = {\n",
    "\"hmax\": 100,\n",
    "\"initial_amount\": 1000000,\n",
    "\"transaction_cost_pct\": 0.001,\n",
    "\"state_space\": state_space,\n",
    "\"stock_dim\": stock_dimension,\n",
    "\"tech_indicator_list\": INDICATORS,\n",
    "\"action_space\": stock_dimension,\n",
    "\"reward_scaling\": 1e-4\n",
    "\n",
    "}\n",
    "e_test_gym = StockPortfolioEnv(df=test_df, **env_kwargs)\n",
    "agent = MarkowitzAgent(e_test_gym,objective='sharp')\n",
    "shrop_mvo = agent.prediction(e_test_gym)\n",
    "shrop_mvo[\"method\"] = \"markowitz\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
