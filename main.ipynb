{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use('Agg')\n",
    "from finrl.config_tickers import DOW_30_TICKER,NAS_100_TICKER,HSI_50_TICKER,DAX_30_TICKER,SP_500_TICKER\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "from finrl.config_tickers import DOW_30_TICKER\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import data_split\n",
    "from stock_env import StockTradingEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent,DRLEnsembleAgent\n",
    "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline,trx_plot\n",
    "from feature_engineer import FeatureEngineer\n",
    "from models import DRLAgent\n",
    "from portfolio_optimization_env import PortfolioOptimizationEnv\n",
    "from pprint import pprint\n",
    "import scienceplots\n",
    "import sys\n",
    "from agents.evn_mvo import StockPortfolioEnv\n",
    "from agents.mvo_agent import MarkowitzAgent\n",
    "sys.path.append(\"../FinRL-Library\")\n",
    "import quantstats as qs\n",
    "plt.style.use('science')\n",
    "import os\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.config import (\n",
    "    DATA_SAVE_DIR,\n",
    "    TRAINED_MODEL_DIR,\n",
    "    TENSORBOARD_LOG_DIR,\n",
    "    RESULTS_DIR,\n",
    "    INDICATORS,\n",
    "    TRAIN_START_DATE,\n",
    "    TRAIN_END_DATE,\n",
    "    TEST_START_DATE,\n",
    "    TEST_END_DATE,\n",
    "    TRADE_START_DATE,\n",
    "    TRADE_END_DATE,\n",
    ")\n",
    "from pypfopt import expected_returns\n",
    "\n",
    "check_and_make_directories([DATA_SAVE_DIR, TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR])\n",
    "\n",
    "def DRL_prediction(model, environment,time_window, deterministic=True):\n",
    "    \"\"\"make a prediction and get results\"\"\"\n",
    "    test_env, test_obs = environment.get_sb_env()\n",
    "    account_memory = None  # This help avoid unnecessary list creation\n",
    "    actions_memory = None  # optimize memory consumption\n",
    "    # state_memory=[] #add memory pool to store states\n",
    "\n",
    "    test_env.reset()\n",
    "    max_steps = len(environment._df.index.unique()) - time_window - 1\n",
    "\n",
    "    for i in range(len(environment._df.index.unique())):\n",
    "        action, _states = model.predict(test_obs, deterministic=deterministic)\n",
    "        # account_memory = test_env.env_method(method_name=\"save_asset_memory\")\n",
    "        # actions_memory = test_env.env_method(method_name=\"save_action_memory\")\n",
    "        test_obs, rewards, dones, info = test_env.step(action)\n",
    "        if i == max_steps:  # more descriptive condition for early termination to clarify the logic\n",
    "            date_list = environment._date_memory\n",
    "            portfolio_return = environment._portfolio_return_memory\n",
    "            # print(len(date_list))\n",
    "            # print(len(asset_list))\n",
    "            df_account_value = pd.DataFrame(\n",
    "                {\"date\": date_list, \"daily_return\": portfolio_return,'account' :  environment._asset_memory[\"final\"],'weights':environment._final_weights}\n",
    "            )\n",
    "            df_date = pd.DataFrame(date_list)\n",
    "            df_date.columns = [\"date\"]\n",
    "\n",
    "            action_list = environment._actions_memory\n",
    "            df_actions = pd.DataFrame(action_list)\n",
    "            tiks = environment._tic_list\n",
    "            df_actions.columns = np.insert(tiks,0,'POS')\n",
    "            df_actions.index = df_date.date\n",
    "            # df_actions = pd.DataFrame({'date':date_list,'actions':action_list})\n",
    "            account_memory = df_account_value\n",
    "            actions_memory = df_actions\n",
    "        # add current state to state memory\n",
    "        # state_memory=test_env.env_method(method_name=\"save_state_memory\")\n",
    "\n",
    "        if dones[0]:\n",
    "            print(\"hit end!\")\n",
    "            break\n",
    "    return account_memory, actions_memory,test_obs\n",
    "TRAIN_START_DATE = '1990-01-01'\n",
    "TRAIN_END_DATE = '2022-12-30'\n",
    "TEST_START_DATE = '2023-01-01'\n",
    "TEST_END_DATE = '2024-10-01'\n",
    "from pandas import read_csv\n",
    "from agents.evn_mvo import StockPortfolioEnv\n",
    "from agents.mvo_agent import MarkowitzAgent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_dow = YahooDownloader(start_date = TRAIN_START_DATE,\n",
    "                     end_date = TEST_END_DATE,\n",
    "                     ticker_list = DOW_30_TICKER).fetch_data()\n",
    "df_nasdaq = YahooDownloader(start_date = TRAIN_START_DATE,\n",
    "                     end_date = TEST_END_DATE,\n",
    "                     ticker_list = NAS_100_TICKER).fetch_data()\n",
    "df_hsi = YahooDownloader(start_date = TRAIN_START_DATE,\n",
    "                     end_date = TEST_END_DATE,\n",
    "                     ticker_list = HSI_50_TICKER).fetch_data()\n",
    "df_dax = YahooDownloader(start_date = TRAIN_START_DATE,\n",
    "                     end_date = TEST_END_DATE,\n",
    "                     ticker_list = DAX_30_TICKER).fetch_data()\n",
    "\n",
    "df_sp500 = YahooDownloader(start_date = TRAIN_START_DATE,\n",
    "                     end_date = TEST_END_DATE,\n",
    "                     ticker_list = SP_500_TICKER).fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dow.to_csv('./data/dow.csv')\n",
    "df_nasdaq.to_csv('./data/nasdaq.csv')\n",
    "df_hsi.to_csv('./data/hsi.csv')\n",
    "df_dax.to_csv('./data/dax.csv')\n",
    "df_sp500.to_csv('./data/sp500.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dow =read_csv('./data/dow.csv')\n",
    "df_nasdaq =read_csv('./data/nasdaq.csv')\n",
    "df_hsi = read_csv('./data/hsi.csv')\n",
    "df_dax = read_csv('./data/dax.csv')\n",
    "df_sp500 = read_csv('./data/sp500.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added technical indicators\n",
      "Successfully added turbulence index\n",
      "Successfully added user defined features\n",
      "Stock Dimension: 25, State Space: 126\n"
     ]
    }
   ],
   "source": [
    "INDICATORS = [\n",
    "    \"close_5_ema\",\n",
    "    \"close_21_ema\",\n",
    "    \"close_62_ema\",\n",
    "]\n",
    "fe = FeatureEngineer(use_technical_indicator=True,\n",
    "                     tech_indicator_list = INDICATORS,\n",
    "                     use_turbulence=True,\n",
    "                     user_defined_feature = True)\n",
    "\n",
    "processed_dax = fe.preprocess_data(df_dow)\n",
    "processed_dax = processed_dax.copy()\n",
    "processed_dax = processed_dax.fillna(0)\n",
    "processed_dax = processed_dax.replace(np.inf,0)\n",
    "train_data = data_split(processed_dax, TRAIN_START_DATE, TRAIN_END_DATE)\n",
    "test_data = data_split(processed_dax, TEST_START_DATE, TEST_END_DATE)\n",
    "stock_dimension = len(train_data.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DRLAgent.__init__() missing 3 required positional arguments: 'price_array', 'tech_array', and 'turbulence_array'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m e_train_gym \u001b[38;5;241m=\u001b[39m PortfolioOptimizationEnv(df \u001b[38;5;241m=\u001b[39m train_data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39menv_kwargs)\n\u001b[0;32m     12\u001b[0m est \u001b[38;5;241m=\u001b[39m PortfolioOptimizationEnv(df \u001b[38;5;241m=\u001b[39m test_data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39menv_kwargs)\n\u001b[1;32m---> 13\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mDRLAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43me_train_gym\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# PPO_PARAMS = {\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m#     \"n_steps\": 2048,\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#     \"ent_coef\": 0.01,\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#     \"learning_rate\": 0.0003,\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#     \"batch_size\": 128,\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# }\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: DRLAgent.__init__() missing 3 required positional arguments: 'price_array', 'tech_array', and 'turbulence_array'"
     ]
    }
   ],
   "source": [
    "from finrl.agents.rllib.models import DRLAgent,DRLEnsembleAgent\n",
    "ppo_result ={}\n",
    "env_kwargs = {\n",
    "    \"initial_amount\": 1000000, \n",
    "    \"normalize_df\":None,\n",
    "    \"features\" :[\"close\",\"return\"],\n",
    "    'comission_fee_pct':0.001,\n",
    "    'time_window':5\n",
    "}\n",
    "e_train_gym = PortfolioOptimizationEnv(df = train_data, **env_kwargs)\n",
    "est = PortfolioOptimizationEnv(df = test_data, **env_kwargs)\n",
    "agent = DRLAgent(env = e_train_gym)\n",
    "# PPO_PARAMS = {\n",
    "#     \"n_steps\": 2048,\n",
    "#     \"ent_coef\": 0.01,\n",
    "#     \"learning_rate\": 0.0003,\n",
    "#     \"batch_size\": 128,\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100,\n",
    "    \"initial_amount\": 1000000,\n",
    "    \"num_stock_shares\": num_stock_shares,\n",
    "    \"buy_cost_pct\": buy_cost_list,\n",
    "    \"sell_cost_pct\": sell_cost_list,\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension,\n",
    "    \"reward_scaling\": 1e-4\n",
    "}\n",
    "\n",
    "\n",
    "train_environment = StockTradingEnv(df = train_data, **env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.0003, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 58           |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 35           |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | 0.0063373083 |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = e_train_gym)\n",
    "PPO_PARAMS = {\n",
    "    \"n_steps\": 2048,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"learning_rate\": 0.0003,\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "model_ppo = agent.get_model(\"ppo\"\n",
    "                            ,model_kwargs = PPO_PARAMS)\n",
    "\n",
    "ppo_model = agent.train_model(model=model_ppo, \n",
    "                            tb_log_name='ppo',\n",
    "                            total_timesteps=1000)\n",
    "ppo_model.save('./data/ppo')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
