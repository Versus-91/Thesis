{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from FinRL.finrl import config\n",
    "from FinRL.finrl import config_tickers\n",
    "\n",
    "if not os.path.exists(\"./\" + config.DATA_SAVE_DIR):\n",
    "    os.makedirs(\"./\" + config.DATA_SAVE_DIR)\n",
    "if not os.path.exists(\"./\" + config.TRAINED_MODEL_DIR):\n",
    "    os.makedirs(\"./\" + config.TRAINED_MODEL_DIR)\n",
    "if not os.path.exists(\"./\" + config.TENSORBOARD_LOG_DIR):\n",
    "    os.makedirs(\"./\" + config.TENSORBOARD_LOG_DIR)\n",
    "if not os.path.exists(\"./\" + config.RESULTS_DIR):\n",
    "    os.makedirs(\"./\" + config.RESULTS_DIR)\n",
    "\n",
    "\n",
    "from FinRL.finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from FinRL.finrl.meta.preprocessor.preprocessors import (\n",
    "    FeatureEngineer,\n",
    "    data_split,\n",
    ")\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (106025, 8)\n",
      "Successfully added technical indicators\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from FinRL.finrl import config\n",
    "from FinRL.finrl import config_tickers\n",
    "\n",
    "if not os.path.exists(\"./\" + config.DATA_SAVE_DIR):\n",
    "    os.makedirs(\"./\" + config.DATA_SAVE_DIR)\n",
    "if not os.path.exists(\"./\" + config.TRAINED_MODEL_DIR):\n",
    "    os.makedirs(\"./\" + config.TRAINED_MODEL_DIR)\n",
    "if not os.path.exists(\"./\" + config.TENSORBOARD_LOG_DIR):\n",
    "    os.makedirs(\"./\" + config.TENSORBOARD_LOG_DIR)\n",
    "if not os.path.exists(\"./\" + config.RESULTS_DIR):\n",
    "    os.makedirs(\"./\" + config.RESULTS_DIR)\n",
    "\n",
    "\n",
    "from FinRL.finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from FinRL.finrl.meta.preprocessor.preprocessors import (\n",
    "    FeatureEngineer,\n",
    "    data_split,\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "df = YahooDownloader(\n",
    "    start_date=\"2008-01-01\",\n",
    "    end_date=\"2022-06-02\",\n",
    "    ticker_list=config_tickers.DOW_30_TICKER,\n",
    ").fetch_data()\n",
    "\n",
    "fe = FeatureEngineer(\n",
    "    use_technical_indicator=True,\n",
    "    use_turbulence=False,\n",
    "    user_defined_feature=False,\n",
    ")\n",
    "\n",
    "df = fe.preprocess_data(df)\n",
    "# add covariance matrix as states\n",
    "df = df.sort_values([\"date\", \"tic\"], ignore_index=True)\n",
    "df.index = df.date.factorize()[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-01-02</td>\n",
       "      <td>7.116786</td>\n",
       "      <td>7.152143</td>\n",
       "      <td>6.876786</td>\n",
       "      <td>5.883143</td>\n",
       "      <td>1079178800</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.888343</td>\n",
       "      <td>5.880659</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>5.883143</td>\n",
       "      <td>5.883143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-01-02</td>\n",
       "      <td>46.599998</td>\n",
       "      <td>47.040001</td>\n",
       "      <td>46.259998</td>\n",
       "      <td>33.263275</td>\n",
       "      <td>7934400</td>\n",
       "      <td>AMGN</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.888343</td>\n",
       "      <td>5.880659</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>33.263275</td>\n",
       "      <td>33.263275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-01-02</td>\n",
       "      <td>52.090000</td>\n",
       "      <td>52.320000</td>\n",
       "      <td>50.790001</td>\n",
       "      <td>39.338627</td>\n",
       "      <td>8053700</td>\n",
       "      <td>AXP</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.888343</td>\n",
       "      <td>5.880659</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.338627</td>\n",
       "      <td>39.338627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-01-02</td>\n",
       "      <td>87.570000</td>\n",
       "      <td>87.839996</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>63.481628</td>\n",
       "      <td>4303000</td>\n",
       "      <td>BA</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.888343</td>\n",
       "      <td>5.880659</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>63.481628</td>\n",
       "      <td>63.481628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-01-02</td>\n",
       "      <td>72.559998</td>\n",
       "      <td>72.669998</td>\n",
       "      <td>70.050003</td>\n",
       "      <td>45.230282</td>\n",
       "      <td>6337800</td>\n",
       "      <td>CAT</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.888343</td>\n",
       "      <td>5.880659</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>45.230282</td>\n",
       "      <td>45.230282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date       open       high        low      close      volume   tic  \\\n",
       "0  2008-01-02   7.116786   7.152143   6.876786   5.883143  1079178800  AAPL   \n",
       "0  2008-01-02  46.599998  47.040001  46.259998  33.263275     7934400  AMGN   \n",
       "0  2008-01-02  52.090000  52.320000  50.790001  39.338627     8053700   AXP   \n",
       "0  2008-01-02  87.570000  87.839996  86.000000  63.481628     4303000    BA   \n",
       "0  2008-01-02  72.559998  72.669998  70.050003  45.230282     6337800   CAT   \n",
       "\n",
       "   day  macd   boll_ub   boll_lb  rsi_30     cci_30  dx_30  close_30_sma  \\\n",
       "0    2   0.0  5.888343  5.880659   100.0 -66.666667  100.0      5.883143   \n",
       "0    2   0.0  5.888343  5.880659   100.0 -66.666667  100.0     33.263275   \n",
       "0    2   0.0  5.888343  5.880659   100.0 -66.666667  100.0     39.338627   \n",
       "0    2   0.0  5.888343  5.880659   100.0 -66.666667  100.0     63.481628   \n",
       "0    2   0.0  5.888343  5.880659   100.0 -66.666667  100.0     45.230282   \n",
       "\n",
       "   close_60_sma  \n",
       "0      5.883143  \n",
       "0     33.263275  \n",
       "0     39.338627  \n",
       "0     63.481628  \n",
       "0     45.230282  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "1086\n",
      "1087\n",
      "1088\n",
      "1089\n",
      "1090\n",
      "1091\n",
      "1092\n",
      "1093\n",
      "1094\n",
      "1095\n",
      "1096\n",
      "1097\n",
      "1098\n",
      "1099\n",
      "1100\n",
      "1101\n",
      "1102\n",
      "1103\n",
      "1104\n",
      "1105\n",
      "1106\n",
      "1107\n",
      "1108\n",
      "1109\n",
      "1110\n",
      "1111\n",
      "1112\n",
      "1113\n",
      "1114\n",
      "1115\n",
      "1116\n",
      "1117\n",
      "1118\n",
      "1119\n",
      "1120\n",
      "1121\n",
      "1122\n",
      "1123\n",
      "1124\n",
      "1125\n",
      "1126\n",
      "1127\n",
      "1128\n",
      "1129\n",
      "1130\n",
      "1131\n",
      "1132\n",
      "1133\n",
      "1134\n",
      "1135\n",
      "1136\n",
      "1137\n",
      "1138\n",
      "1139\n",
      "1140\n",
      "1141\n",
      "1142\n",
      "1143\n",
      "1144\n",
      "1145\n",
      "1146\n",
      "1147\n",
      "1148\n",
      "1149\n",
      "1150\n",
      "1151\n",
      "1152\n",
      "1153\n",
      "1154\n",
      "1155\n",
      "1156\n",
      "1157\n",
      "1158\n",
      "1159\n",
      "1160\n",
      "1161\n",
      "1162\n",
      "1163\n",
      "1164\n",
      "1165\n",
      "1166\n",
      "1167\n",
      "1168\n",
      "1169\n",
      "1170\n",
      "1171\n",
      "1172\n",
      "1173\n",
      "1174\n",
      "1175\n",
      "1176\n",
      "1177\n",
      "1178\n",
      "1179\n",
      "1180\n",
      "1181\n",
      "1182\n",
      "1183\n",
      "1184\n",
      "1185\n",
      "1186\n",
      "1187\n",
      "1188\n",
      "1189\n",
      "1190\n",
      "1191\n",
      "1192\n",
      "1193\n",
      "1194\n",
      "1195\n",
      "1196\n",
      "1197\n",
      "1198\n",
      "1199\n",
      "1200\n",
      "1201\n",
      "1202\n",
      "1203\n",
      "1204\n",
      "1205\n",
      "1206\n",
      "1207\n",
      "1208\n",
      "1209\n",
      "1210\n",
      "1211\n",
      "1212\n",
      "1213\n",
      "1214\n",
      "1215\n",
      "1216\n",
      "1217\n",
      "1218\n",
      "1219\n",
      "1220\n",
      "1221\n",
      "1222\n",
      "1223\n",
      "1224\n",
      "1225\n",
      "1226\n",
      "1227\n",
      "1228\n",
      "1229\n",
      "1230\n",
      "1231\n",
      "1232\n",
      "1233\n",
      "1234\n",
      "1235\n",
      "1236\n",
      "1237\n",
      "1238\n",
      "1239\n",
      "1240\n",
      "1241\n",
      "1242\n",
      "1243\n",
      "1244\n",
      "1245\n",
      "1246\n",
      "1247\n",
      "1248\n",
      "1249\n",
      "1250\n",
      "1251\n",
      "1252\n",
      "1253\n",
      "1254\n",
      "1255\n",
      "1256\n",
      "1257\n",
      "1258\n",
      "1259\n",
      "1260\n",
      "1261\n",
      "1262\n",
      "1263\n",
      "1264\n",
      "1265\n",
      "1266\n",
      "1267\n",
      "1268\n",
      "1269\n",
      "1270\n",
      "1271\n",
      "1272\n",
      "1273\n",
      "1274\n",
      "1275\n",
      "1276\n",
      "1277\n",
      "1278\n",
      "1279\n",
      "1280\n",
      "1281\n",
      "1282\n",
      "1283\n",
      "1284\n",
      "1285\n",
      "1286\n",
      "1287\n",
      "1288\n",
      "1289\n",
      "1290\n",
      "1291\n",
      "1292\n",
      "1293\n",
      "1294\n",
      "1295\n",
      "1296\n",
      "1297\n",
      "1298\n",
      "1299\n",
      "1300\n",
      "1301\n",
      "1302\n",
      "1303\n",
      "1304\n",
      "1305\n",
      "1306\n",
      "1307\n",
      "1308\n",
      "1309\n",
      "1310\n",
      "1311\n",
      "1312\n",
      "1313\n",
      "1314\n",
      "1315\n",
      "1316\n",
      "1317\n",
      "1318\n",
      "1319\n",
      "1320\n",
      "1321\n",
      "1322\n",
      "1323\n",
      "1324\n",
      "1325\n",
      "1326\n",
      "1327\n",
      "1328\n",
      "1329\n",
      "1330\n",
      "1331\n",
      "1332\n",
      "1333\n",
      "1334\n",
      "1335\n",
      "1336\n",
      "1337\n",
      "1338\n",
      "1339\n",
      "1340\n",
      "1341\n",
      "1342\n",
      "1343\n",
      "1344\n",
      "1345\n",
      "1346\n",
      "1347\n",
      "1348\n",
      "1349\n",
      "1350\n",
      "1351\n",
      "1352\n",
      "1353\n",
      "1354\n",
      "1355\n",
      "1356\n",
      "1357\n",
      "1358\n",
      "1359\n",
      "1360\n",
      "1361\n",
      "1362\n",
      "1363\n",
      "1364\n",
      "1365\n",
      "1366\n",
      "1367\n",
      "1368\n",
      "1369\n",
      "1370\n",
      "1371\n",
      "1372\n",
      "1373\n",
      "1374\n",
      "1375\n",
      "1376\n",
      "1377\n",
      "1378\n",
      "1379\n",
      "1380\n",
      "1381\n",
      "1382\n",
      "1383\n",
      "1384\n",
      "1385\n",
      "1386\n",
      "1387\n",
      "1388\n",
      "1389\n",
      "1390\n",
      "1391\n",
      "1392\n",
      "1393\n",
      "1394\n",
      "1395\n",
      "1396\n",
      "1397\n",
      "1398\n",
      "1399\n",
      "1400\n",
      "1401\n",
      "1402\n",
      "1403\n",
      "1404\n",
      "1405\n",
      "1406\n",
      "1407\n",
      "1408\n",
      "1409\n",
      "1410\n",
      "1411\n",
      "1412\n",
      "1413\n",
      "1414\n",
      "1415\n",
      "1416\n",
      "1417\n",
      "1418\n",
      "1419\n",
      "1420\n",
      "1421\n",
      "1422\n",
      "1423\n",
      "1424\n",
      "1425\n",
      "1426\n",
      "1427\n",
      "1428\n",
      "1429\n",
      "1430\n",
      "1431\n",
      "1432\n",
      "1433\n",
      "1434\n",
      "1435\n",
      "1436\n",
      "1437\n",
      "1438\n",
      "1439\n",
      "1440\n",
      "1441\n",
      "1442\n",
      "1443\n",
      "1444\n",
      "1445\n",
      "1446\n",
      "1447\n",
      "1448\n",
      "1449\n",
      "1450\n",
      "1451\n",
      "1452\n",
      "1453\n",
      "1454\n",
      "1455\n",
      "1456\n",
      "1457\n",
      "1458\n",
      "1459\n",
      "1460\n",
      "1461\n",
      "1462\n",
      "1463\n",
      "1464\n",
      "1465\n",
      "1466\n",
      "1467\n",
      "1468\n",
      "1469\n",
      "1470\n",
      "1471\n",
      "1472\n",
      "1473\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1552\n",
      "1553\n",
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "1600\n",
      "1601\n",
      "1602\n",
      "1603\n",
      "1604\n",
      "1605\n",
      "1606\n",
      "1607\n",
      "1608\n",
      "1609\n",
      "1610\n",
      "1611\n",
      "1612\n",
      "1613\n",
      "1614\n",
      "1615\n",
      "1616\n",
      "1617\n",
      "1618\n",
      "1619\n",
      "1620\n",
      "1621\n",
      "1622\n",
      "1623\n",
      "1624\n",
      "1625\n",
      "1626\n",
      "1627\n",
      "1628\n",
      "1629\n",
      "1630\n",
      "1631\n",
      "1632\n",
      "1633\n",
      "1634\n",
      "1635\n",
      "1636\n",
      "1637\n",
      "1638\n",
      "1639\n",
      "1640\n",
      "1641\n",
      "1642\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1646\n",
      "1647\n",
      "1648\n",
      "1649\n",
      "1650\n",
      "1651\n",
      "1652\n",
      "1653\n",
      "1654\n",
      "1655\n",
      "1656\n",
      "1657\n",
      "1658\n",
      "1659\n",
      "1660\n",
      "1661\n",
      "1662\n",
      "1663\n",
      "1664\n",
      "1665\n",
      "1666\n",
      "1667\n",
      "1668\n",
      "1669\n",
      "1670\n",
      "1671\n",
      "1672\n",
      "1673\n",
      "1674\n",
      "1675\n",
      "1676\n",
      "1677\n",
      "1678\n",
      "1679\n",
      "1680\n",
      "1681\n",
      "1682\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1688\n",
      "1689\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1693\n",
      "1694\n",
      "1695\n",
      "1696\n",
      "1697\n",
      "1698\n",
      "1699\n",
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n",
      "1759\n",
      "1760\n",
      "1761\n",
      "1762\n",
      "1763\n",
      "1764\n",
      "1765\n",
      "1766\n",
      "1767\n",
      "1768\n",
      "1769\n",
      "1770\n",
      "1771\n",
      "1772\n",
      "1773\n",
      "1774\n",
      "1775\n",
      "1776\n",
      "1777\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1790\n",
      "1791\n",
      "1792\n",
      "1793\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1797\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1812\n",
      "1813\n",
      "1814\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1823\n",
      "1824\n",
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n",
      "1839\n",
      "1840\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "1844\n",
      "1845\n",
      "1846\n",
      "1847\n",
      "1848\n",
      "1849\n",
      "1850\n",
      "1851\n",
      "1852\n",
      "1853\n",
      "1854\n",
      "1855\n",
      "1856\n",
      "1857\n",
      "1858\n",
      "1859\n",
      "1860\n",
      "1861\n",
      "1862\n",
      "1863\n",
      "1864\n",
      "1865\n",
      "1866\n",
      "1867\n",
      "1868\n",
      "1869\n",
      "1870\n",
      "1871\n",
      "1872\n",
      "1873\n",
      "1874\n",
      "1875\n",
      "1876\n",
      "1877\n",
      "1878\n",
      "1879\n",
      "1880\n",
      "1881\n",
      "1882\n",
      "1883\n",
      "1884\n",
      "1885\n",
      "1886\n",
      "1887\n",
      "1888\n",
      "1889\n",
      "1890\n",
      "1891\n",
      "1892\n",
      "1893\n",
      "1894\n",
      "1895\n",
      "1896\n",
      "1897\n",
      "1898\n",
      "1899\n",
      "1900\n",
      "1901\n",
      "1902\n",
      "1903\n",
      "1904\n",
      "1905\n",
      "1906\n",
      "1907\n",
      "1908\n",
      "1909\n",
      "1910\n",
      "1911\n",
      "1912\n",
      "1913\n",
      "1914\n",
      "1915\n",
      "1916\n",
      "1917\n",
      "1918\n",
      "1919\n",
      "1920\n",
      "1921\n",
      "1922\n",
      "1923\n",
      "1924\n",
      "1925\n",
      "1926\n",
      "1927\n",
      "1928\n",
      "1929\n",
      "1930\n",
      "1931\n",
      "1932\n",
      "1933\n",
      "1934\n",
      "1935\n",
      "1936\n",
      "1937\n",
      "1938\n",
      "1939\n",
      "1940\n",
      "1941\n",
      "1942\n",
      "1943\n",
      "1944\n",
      "1945\n",
      "1946\n",
      "1947\n",
      "1948\n",
      "1949\n",
      "1950\n",
      "1951\n",
      "1952\n",
      "1953\n",
      "1954\n",
      "1955\n",
      "1956\n",
      "1957\n",
      "1958\n",
      "1959\n",
      "1960\n",
      "1961\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1965\n",
      "1966\n",
      "1967\n",
      "1968\n",
      "1969\n",
      "1970\n",
      "1971\n",
      "1972\n",
      "1973\n",
      "1974\n",
      "1975\n",
      "1976\n",
      "1977\n",
      "1978\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n",
      "2024\n",
      "2025\n",
      "2026\n",
      "2027\n",
      "2028\n",
      "2029\n",
      "2030\n",
      "2031\n",
      "2032\n",
      "2033\n",
      "2034\n",
      "2035\n",
      "2036\n",
      "2037\n",
      "2038\n",
      "2039\n",
      "2040\n",
      "2041\n",
      "2042\n",
      "2043\n",
      "2044\n",
      "2045\n",
      "2046\n",
      "2047\n",
      "2048\n",
      "2049\n",
      "2050\n",
      "2051\n",
      "2052\n",
      "2053\n",
      "2054\n",
      "2055\n",
      "2056\n",
      "2057\n",
      "2058\n",
      "2059\n",
      "2060\n",
      "2061\n",
      "2062\n",
      "2063\n",
      "2064\n",
      "2065\n",
      "2066\n",
      "2067\n",
      "2068\n",
      "2069\n",
      "2070\n",
      "2071\n",
      "2072\n",
      "2073\n",
      "2074\n",
      "2075\n",
      "2076\n",
      "2077\n",
      "2078\n",
      "2079\n",
      "2080\n",
      "2081\n",
      "2082\n",
      "2083\n",
      "2084\n",
      "2085\n",
      "2086\n",
      "2087\n",
      "2088\n",
      "2089\n",
      "2090\n",
      "2091\n",
      "2092\n",
      "2093\n",
      "2094\n",
      "2095\n",
      "2096\n",
      "2097\n",
      "2098\n",
      "2099\n",
      "2100\n",
      "2101\n",
      "2102\n",
      "2103\n",
      "2104\n",
      "2105\n",
      "2106\n",
      "2107\n",
      "2108\n",
      "2109\n",
      "2110\n",
      "2111\n",
      "2112\n",
      "2113\n",
      "2114\n",
      "2115\n",
      "2116\n",
      "2117\n",
      "2118\n",
      "2119\n",
      "2120\n",
      "2121\n",
      "2122\n",
      "2123\n",
      "2124\n",
      "2125\n",
      "2126\n",
      "2127\n",
      "2128\n",
      "2129\n",
      "2130\n",
      "2131\n",
      "2132\n",
      "2133\n",
      "2134\n",
      "2135\n",
      "2136\n",
      "2137\n",
      "2138\n",
      "2139\n",
      "2140\n",
      "2141\n",
      "2142\n",
      "2143\n",
      "2144\n",
      "2145\n",
      "2146\n",
      "2147\n",
      "2148\n",
      "2149\n",
      "2150\n",
      "2151\n",
      "2152\n",
      "2153\n",
      "2154\n",
      "2155\n",
      "2156\n",
      "2157\n",
      "2158\n",
      "2159\n",
      "2160\n",
      "2161\n",
      "2162\n",
      "2163\n",
      "2164\n",
      "2165\n",
      "2166\n",
      "2167\n",
      "2168\n",
      "2169\n",
      "2170\n",
      "2171\n",
      "2172\n",
      "2173\n",
      "2174\n",
      "2175\n",
      "2176\n",
      "2177\n",
      "2178\n",
      "2179\n",
      "2180\n",
      "2181\n",
      "2182\n",
      "2183\n",
      "2184\n",
      "2185\n",
      "2186\n",
      "2187\n",
      "2188\n",
      "2189\n",
      "2190\n",
      "2191\n",
      "2192\n",
      "2193\n",
      "2194\n",
      "2195\n",
      "2196\n",
      "2197\n",
      "2198\n",
      "2199\n",
      "2200\n",
      "2201\n",
      "2202\n",
      "2203\n",
      "2204\n",
      "2205\n",
      "2206\n",
      "2207\n",
      "2208\n",
      "2209\n",
      "2210\n",
      "2211\n",
      "2212\n",
      "2213\n",
      "2214\n",
      "2215\n",
      "2216\n",
      "2217\n",
      "2218\n",
      "2219\n",
      "2220\n",
      "2221\n",
      "2222\n",
      "2223\n",
      "2224\n",
      "2225\n",
      "2226\n",
      "2227\n",
      "2228\n",
      "2229\n",
      "2230\n",
      "2231\n",
      "2232\n",
      "2233\n",
      "2234\n",
      "2235\n",
      "2236\n",
      "2237\n",
      "2238\n",
      "2239\n",
      "2240\n",
      "2241\n",
      "2242\n",
      "2243\n",
      "2244\n",
      "2245\n",
      "2246\n",
      "2247\n",
      "2248\n",
      "2249\n",
      "2250\n",
      "2251\n",
      "2252\n",
      "2253\n",
      "2254\n",
      "2255\n",
      "2256\n",
      "2257\n",
      "2258\n",
      "2259\n",
      "2260\n",
      "2261\n",
      "2262\n",
      "2263\n",
      "2264\n",
      "2265\n",
      "2266\n",
      "2267\n",
      "2268\n",
      "2269\n",
      "2270\n",
      "2271\n",
      "2272\n",
      "2273\n",
      "2274\n",
      "2275\n",
      "2276\n",
      "2277\n",
      "2278\n",
      "2279\n",
      "2280\n",
      "2281\n",
      "2282\n",
      "2283\n",
      "2284\n",
      "2285\n",
      "2286\n",
      "2287\n",
      "2288\n",
      "2289\n",
      "2290\n",
      "2291\n",
      "2292\n",
      "2293\n",
      "2294\n",
      "2295\n",
      "2296\n",
      "2297\n",
      "2298\n",
      "2299\n",
      "2300\n",
      "2301\n",
      "2302\n",
      "2303\n",
      "2304\n",
      "2305\n",
      "2306\n",
      "2307\n",
      "2308\n",
      "2309\n",
      "2310\n",
      "2311\n",
      "2312\n",
      "2313\n",
      "2314\n",
      "2315\n",
      "2316\n",
      "2317\n",
      "2318\n",
      "2319\n",
      "2320\n",
      "2321\n",
      "2322\n",
      "2323\n",
      "2324\n",
      "2325\n",
      "2326\n",
      "2327\n",
      "2328\n",
      "2329\n",
      "2330\n",
      "2331\n",
      "2332\n",
      "2333\n",
      "2334\n",
      "2335\n",
      "2336\n",
      "2337\n",
      "2338\n",
      "2339\n",
      "2340\n",
      "2341\n",
      "2342\n",
      "2343\n",
      "2344\n",
      "2345\n",
      "2346\n",
      "2347\n",
      "2348\n",
      "2349\n",
      "2350\n",
      "2351\n",
      "2352\n",
      "2353\n",
      "2354\n",
      "2355\n",
      "2356\n",
      "2357\n",
      "2358\n",
      "2359\n",
      "2360\n",
      "2361\n",
      "2362\n",
      "2363\n",
      "2364\n",
      "2365\n",
      "2366\n",
      "2367\n",
      "2368\n",
      "2369\n",
      "2370\n",
      "2371\n",
      "2372\n",
      "2373\n",
      "2374\n",
      "2375\n",
      "2376\n",
      "2377\n",
      "2378\n",
      "2379\n",
      "2380\n",
      "2381\n",
      "2382\n",
      "2383\n",
      "2384\n",
      "2385\n",
      "2386\n",
      "2387\n",
      "2388\n",
      "2389\n",
      "2390\n",
      "2391\n",
      "2392\n",
      "2393\n",
      "2394\n",
      "2395\n",
      "2396\n",
      "2397\n",
      "2398\n",
      "2399\n",
      "2400\n",
      "2401\n",
      "2402\n",
      "2403\n",
      "2404\n",
      "2405\n",
      "2406\n",
      "2407\n",
      "2408\n",
      "2409\n",
      "2410\n",
      "2411\n",
      "2412\n",
      "2413\n",
      "2414\n",
      "2415\n",
      "2416\n",
      "2417\n",
      "2418\n",
      "2419\n",
      "2420\n",
      "2421\n",
      "2422\n",
      "2423\n",
      "2424\n",
      "2425\n",
      "2426\n",
      "2427\n",
      "2428\n",
      "2429\n",
      "2430\n",
      "2431\n",
      "2432\n",
      "2433\n",
      "2434\n",
      "2435\n",
      "2436\n",
      "2437\n",
      "2438\n",
      "2439\n",
      "2440\n",
      "2441\n",
      "2442\n",
      "2443\n",
      "2444\n",
      "2445\n",
      "2446\n",
      "2447\n",
      "2448\n",
      "2449\n",
      "2450\n",
      "2451\n",
      "2452\n",
      "2453\n",
      "2454\n",
      "2455\n",
      "2456\n",
      "2457\n",
      "2458\n",
      "2459\n",
      "2460\n",
      "2461\n",
      "2462\n",
      "2463\n",
      "2464\n",
      "2465\n",
      "2466\n",
      "2467\n",
      "2468\n",
      "2469\n",
      "2470\n",
      "2471\n",
      "2472\n",
      "2473\n",
      "2474\n",
      "2475\n",
      "2476\n",
      "2477\n",
      "2478\n",
      "2479\n",
      "2480\n",
      "2481\n",
      "2482\n",
      "2483\n",
      "2484\n",
      "2485\n",
      "2486\n",
      "2487\n",
      "2488\n",
      "2489\n",
      "2490\n",
      "2491\n",
      "2492\n",
      "2493\n",
      "2494\n",
      "2495\n",
      "2496\n",
      "2497\n",
      "2498\n",
      "2499\n",
      "2500\n",
      "2501\n",
      "2502\n",
      "2503\n",
      "2504\n",
      "2505\n",
      "2506\n",
      "2507\n",
      "2508\n",
      "2509\n",
      "2510\n",
      "2511\n",
      "2512\n",
      "2513\n",
      "2514\n",
      "2515\n",
      "2516\n",
      "2517\n",
      "2518\n",
      "2519\n",
      "2520\n",
      "2521\n",
      "2522\n",
      "2523\n",
      "2524\n",
      "2525\n",
      "2526\n",
      "2527\n",
      "2528\n",
      "2529\n",
      "2530\n",
      "2531\n",
      "2532\n",
      "2533\n",
      "2534\n",
      "2535\n",
      "2536\n",
      "2537\n",
      "2538\n",
      "2539\n",
      "2540\n",
      "2541\n",
      "2542\n",
      "2543\n",
      "2544\n",
      "2545\n",
      "2546\n",
      "2547\n",
      "2548\n",
      "2549\n",
      "2550\n",
      "2551\n",
      "2552\n",
      "2553\n",
      "2554\n",
      "2555\n",
      "2556\n",
      "2557\n",
      "2558\n",
      "2559\n",
      "2560\n",
      "2561\n",
      "2562\n",
      "2563\n",
      "2564\n",
      "2565\n",
      "2566\n",
      "2567\n",
      "2568\n",
      "2569\n",
      "2570\n",
      "2571\n",
      "2572\n",
      "2573\n",
      "2574\n",
      "2575\n",
      "2576\n",
      "2577\n",
      "2578\n",
      "2579\n",
      "2580\n",
      "2581\n",
      "2582\n",
      "2583\n",
      "2584\n",
      "2585\n",
      "2586\n",
      "2587\n",
      "2588\n",
      "2589\n",
      "2590\n",
      "2591\n",
      "2592\n",
      "2593\n",
      "2594\n",
      "2595\n",
      "2596\n",
      "2597\n",
      "2598\n",
      "2599\n",
      "2600\n",
      "2601\n",
      "2602\n",
      "2603\n",
      "2604\n",
      "2605\n",
      "2606\n",
      "2607\n",
      "2608\n",
      "2609\n",
      "2610\n",
      "2611\n",
      "2612\n",
      "2613\n",
      "2614\n",
      "2615\n",
      "2616\n",
      "2617\n",
      "2618\n",
      "2619\n",
      "2620\n",
      "2621\n",
      "2622\n",
      "2623\n",
      "2624\n",
      "2625\n",
      "2626\n",
      "2627\n",
      "2628\n",
      "2629\n",
      "2630\n",
      "2631\n",
      "2632\n",
      "2633\n",
      "2634\n",
      "2635\n",
      "2636\n",
      "2637\n",
      "2638\n",
      "2639\n",
      "2640\n",
      "2641\n",
      "2642\n",
      "2643\n",
      "2644\n",
      "2645\n",
      "2646\n",
      "2647\n",
      "2648\n",
      "2649\n",
      "2650\n",
      "2651\n",
      "2652\n",
      "2653\n",
      "2654\n",
      "2655\n",
      "2656\n",
      "2657\n",
      "2658\n",
      "2659\n",
      "2660\n",
      "2661\n",
      "2662\n",
      "2663\n",
      "2664\n",
      "2665\n",
      "2666\n",
      "2667\n",
      "2668\n",
      "2669\n",
      "2670\n",
      "2671\n",
      "2672\n",
      "2673\n",
      "2674\n",
      "2675\n",
      "2676\n",
      "2677\n",
      "2678\n",
      "2679\n",
      "2680\n",
      "2681\n",
      "2682\n",
      "2683\n",
      "2684\n",
      "2685\n",
      "2686\n",
      "2687\n",
      "2688\n",
      "2689\n",
      "2690\n",
      "2691\n",
      "2692\n",
      "2693\n",
      "2694\n",
      "2695\n",
      "2696\n",
      "2697\n",
      "2698\n",
      "2699\n",
      "2700\n",
      "2701\n",
      "2702\n",
      "2703\n",
      "2704\n",
      "2705\n",
      "2706\n",
      "2707\n",
      "2708\n",
      "2709\n",
      "2710\n",
      "2711\n",
      "2712\n",
      "2713\n",
      "2714\n",
      "2715\n",
      "2716\n",
      "2717\n",
      "2718\n",
      "2719\n",
      "2720\n",
      "2721\n",
      "2722\n",
      "2723\n",
      "2724\n",
      "2725\n",
      "2726\n",
      "2727\n",
      "2728\n",
      "2729\n",
      "2730\n",
      "2731\n",
      "2732\n",
      "2733\n",
      "2734\n",
      "2735\n",
      "2736\n",
      "2737\n",
      "2738\n",
      "2739\n",
      "2740\n",
      "2741\n",
      "2742\n",
      "2743\n",
      "2744\n",
      "2745\n",
      "2746\n",
      "2747\n",
      "2748\n",
      "2749\n",
      "2750\n",
      "2751\n",
      "2752\n",
      "2753\n",
      "2754\n",
      "2755\n",
      "2756\n",
      "2757\n",
      "2758\n",
      "2759\n",
      "2760\n",
      "2761\n",
      "2762\n",
      "2763\n",
      "2764\n",
      "2765\n",
      "2766\n",
      "2767\n",
      "2768\n",
      "2769\n",
      "2770\n",
      "2771\n",
      "2772\n",
      "2773\n",
      "2774\n",
      "2775\n",
      "2776\n",
      "2777\n",
      "2778\n",
      "2779\n",
      "2780\n",
      "2781\n",
      "2782\n",
      "2783\n",
      "2784\n",
      "2785\n",
      "2786\n",
      "2787\n",
      "2788\n",
      "2789\n",
      "2790\n",
      "2791\n",
      "2792\n",
      "2793\n",
      "2794\n",
      "2795\n",
      "2796\n",
      "2797\n",
      "2798\n",
      "2799\n",
      "2800\n",
      "2801\n",
      "2802\n",
      "2803\n",
      "2804\n",
      "2805\n",
      "2806\n",
      "2807\n",
      "2808\n",
      "2809\n",
      "2810\n",
      "2811\n",
      "2812\n",
      "2813\n",
      "2814\n",
      "2815\n",
      "2816\n",
      "2817\n",
      "2818\n",
      "2819\n",
      "2820\n",
      "2821\n",
      "2822\n",
      "2823\n",
      "2824\n",
      "2825\n",
      "2826\n",
      "2827\n",
      "2828\n",
      "2829\n",
      "2830\n",
      "2831\n",
      "2832\n",
      "2833\n",
      "2834\n",
      "2835\n",
      "2836\n",
      "2837\n",
      "2838\n",
      "2839\n",
      "2840\n",
      "2841\n",
      "2842\n",
      "2843\n",
      "2844\n",
      "2845\n",
      "2846\n",
      "2847\n",
      "2848\n",
      "2849\n",
      "2850\n",
      "2851\n",
      "2852\n",
      "2853\n",
      "2854\n",
      "2855\n",
      "2856\n",
      "2857\n",
      "2858\n",
      "2859\n",
      "2860\n",
      "2861\n",
      "2862\n",
      "2863\n",
      "2864\n",
      "2865\n",
      "2866\n",
      "2867\n",
      "2868\n",
      "2869\n",
      "2870\n",
      "2871\n",
      "2872\n",
      "2873\n",
      "2874\n",
      "2875\n",
      "2876\n",
      "2877\n",
      "2878\n",
      "2879\n",
      "2880\n",
      "2881\n",
      "2882\n",
      "2883\n",
      "2884\n",
      "2885\n",
      "2886\n",
      "2887\n",
      "2888\n",
      "2889\n",
      "2890\n",
      "2891\n",
      "2892\n",
      "2893\n",
      "2894\n",
      "2895\n",
      "2896\n",
      "2897\n",
      "2898\n",
      "2899\n",
      "2900\n",
      "2901\n",
      "2902\n",
      "2903\n",
      "2904\n",
      "2905\n",
      "2906\n",
      "2907\n",
      "2908\n",
      "2909\n",
      "2910\n",
      "2911\n",
      "2912\n",
      "2913\n",
      "2914\n",
      "2915\n",
      "2916\n",
      "2917\n",
      "2918\n",
      "2919\n",
      "2920\n",
      "2921\n",
      "2922\n",
      "2923\n",
      "2924\n",
      "2925\n",
      "2926\n",
      "2927\n",
      "2928\n",
      "2929\n",
      "2930\n",
      "2931\n",
      "2932\n",
      "2933\n",
      "2934\n",
      "2935\n",
      "2936\n",
      "2937\n",
      "2938\n",
      "2939\n",
      "2940\n",
      "2941\n",
      "2942\n",
      "2943\n",
      "2944\n",
      "2945\n",
      "2946\n",
      "2947\n",
      "2948\n",
      "2949\n",
      "2950\n",
      "2951\n",
      "2952\n",
      "2953\n",
      "2954\n",
      "2955\n",
      "2956\n",
      "2957\n",
      "2958\n",
      "2959\n",
      "2960\n",
      "2961\n",
      "2962\n",
      "2963\n",
      "2964\n",
      "2965\n",
      "2966\n",
      "2967\n",
      "2968\n",
      "2969\n",
      "2970\n",
      "2971\n",
      "2972\n",
      "2973\n",
      "2974\n",
      "2975\n",
      "2976\n",
      "2977\n",
      "2978\n",
      "2979\n",
      "2980\n",
      "2981\n",
      "2982\n",
      "2983\n",
      "2984\n",
      "2985\n",
      "2986\n",
      "2987\n",
      "2988\n",
      "2989\n",
      "2990\n",
      "2991\n",
      "2992\n",
      "2993\n",
      "2994\n",
      "2995\n",
      "2996\n",
      "2997\n",
      "2998\n",
      "2999\n",
      "3000\n",
      "3001\n",
      "3002\n",
      "3003\n",
      "3004\n",
      "3005\n",
      "3006\n",
      "3007\n",
      "3008\n",
      "3009\n",
      "3010\n",
      "3011\n",
      "3012\n",
      "3013\n",
      "3014\n",
      "3015\n",
      "3016\n",
      "3017\n",
      "3018\n",
      "3019\n",
      "3020\n",
      "3021\n",
      "3022\n",
      "3023\n",
      "3024\n",
      "3025\n",
      "3026\n",
      "3027\n",
      "3028\n",
      "3029\n",
      "3030\n",
      "3031\n",
      "3032\n",
      "3033\n",
      "3034\n",
      "3035\n",
      "3036\n",
      "3037\n",
      "3038\n",
      "3039\n",
      "3040\n",
      "3041\n",
      "3042\n",
      "3043\n",
      "3044\n",
      "3045\n",
      "3046\n",
      "3047\n",
      "3048\n",
      "3049\n",
      "3050\n",
      "3051\n",
      "3052\n",
      "3053\n",
      "3054\n",
      "3055\n",
      "3056\n",
      "3057\n",
      "3058\n",
      "3059\n",
      "3060\n",
      "3061\n",
      "3062\n",
      "3063\n",
      "3064\n",
      "3065\n",
      "3066\n",
      "3067\n",
      "3068\n",
      "3069\n",
      "3070\n",
      "3071\n",
      "3072\n",
      "3073\n",
      "3074\n",
      "3075\n",
      "3076\n",
      "3077\n",
      "3078\n",
      "3079\n",
      "3080\n",
      "3081\n",
      "3082\n",
      "3083\n",
      "3084\n",
      "3085\n",
      "3086\n",
      "3087\n",
      "3088\n",
      "3089\n",
      "3090\n",
      "3091\n",
      "3092\n",
      "3093\n",
      "3094\n",
      "3095\n",
      "3096\n",
      "3097\n",
      "3098\n",
      "3099\n",
      "3100\n",
      "3101\n",
      "3102\n",
      "3103\n",
      "3104\n",
      "3105\n",
      "3106\n",
      "3107\n",
      "3108\n",
      "3109\n",
      "3110\n",
      "3111\n",
      "3112\n",
      "3113\n",
      "3114\n",
      "3115\n",
      "3116\n",
      "3117\n",
      "3118\n",
      "3119\n",
      "3120\n",
      "3121\n",
      "3122\n",
      "3123\n",
      "3124\n",
      "3125\n",
      "3126\n",
      "3127\n",
      "3128\n",
      "3129\n",
      "3130\n",
      "3131\n",
      "3132\n",
      "3133\n",
      "3134\n",
      "3135\n",
      "3136\n",
      "3137\n",
      "3138\n",
      "3139\n",
      "3140\n",
      "3141\n",
      "3142\n",
      "3143\n",
      "3144\n",
      "3145\n",
      "3146\n",
      "3147\n",
      "3148\n",
      "3149\n",
      "3150\n",
      "3151\n",
      "3152\n",
      "3153\n",
      "3154\n",
      "3155\n",
      "3156\n",
      "3157\n",
      "3158\n",
      "3159\n",
      "3160\n",
      "3161\n",
      "3162\n",
      "3163\n",
      "3164\n",
      "3165\n",
      "3166\n",
      "3167\n",
      "3168\n",
      "3169\n",
      "3170\n",
      "3171\n",
      "3172\n",
      "3173\n",
      "3174\n",
      "3175\n",
      "3176\n",
      "3177\n",
      "3178\n",
      "3179\n",
      "3180\n",
      "3181\n",
      "3182\n",
      "3183\n",
      "3184\n",
      "3185\n",
      "3186\n",
      "3187\n",
      "3188\n",
      "3189\n",
      "3190\n",
      "3191\n",
      "3192\n",
      "3193\n",
      "3194\n",
      "3195\n",
      "3196\n",
      "3197\n",
      "3198\n",
      "3199\n",
      "3200\n",
      "3201\n",
      "3202\n",
      "3203\n",
      "3204\n",
      "3205\n",
      "3206\n",
      "3207\n",
      "3208\n",
      "3209\n",
      "3210\n",
      "3211\n",
      "3212\n",
      "3213\n",
      "3214\n",
      "3215\n",
      "3216\n",
      "3217\n",
      "3218\n",
      "3219\n",
      "3220\n",
      "3221\n",
      "3222\n",
      "3223\n",
      "3224\n",
      "3225\n",
      "3226\n",
      "3227\n",
      "3228\n",
      "3229\n",
      "3230\n",
      "3231\n",
      "3232\n",
      "3233\n",
      "3234\n",
      "3235\n",
      "3236\n",
      "3237\n",
      "3238\n",
      "3239\n",
      "3240\n",
      "3241\n",
      "3242\n",
      "3243\n",
      "3244\n",
      "3245\n",
      "3246\n",
      "3247\n",
      "3248\n",
      "3249\n",
      "3250\n",
      "3251\n",
      "3252\n",
      "3253\n",
      "3254\n",
      "3255\n",
      "3256\n",
      "3257\n",
      "3258\n",
      "3259\n",
      "3260\n",
      "3261\n",
      "3262\n",
      "3263\n",
      "3264\n",
      "3265\n",
      "3266\n",
      "3267\n",
      "3268\n",
      "3269\n",
      "3270\n",
      "3271\n",
      "3272\n",
      "3273\n",
      "3274\n",
      "3275\n",
      "3276\n",
      "3277\n",
      "3278\n",
      "3279\n",
      "3280\n",
      "3281\n",
      "3282\n",
      "3283\n",
      "3284\n",
      "3285\n",
      "3286\n",
      "3287\n",
      "3288\n",
      "3289\n",
      "3290\n",
      "3291\n",
      "3292\n",
      "3293\n",
      "3294\n",
      "3295\n",
      "3296\n",
      "3297\n",
      "3298\n",
      "3299\n",
      "3300\n",
      "3301\n",
      "3302\n",
      "3303\n",
      "3304\n",
      "3305\n",
      "3306\n",
      "3307\n",
      "3308\n",
      "3309\n",
      "3310\n",
      "3311\n",
      "3312\n",
      "3313\n",
      "3314\n",
      "3315\n",
      "3316\n",
      "3317\n",
      "3318\n",
      "3319\n",
      "3320\n",
      "3321\n",
      "3322\n",
      "3323\n",
      "3324\n",
      "3325\n",
      "3326\n",
      "3327\n",
      "3328\n",
      "3329\n",
      "3330\n",
      "3331\n",
      "3332\n",
      "3333\n",
      "3334\n",
      "3335\n",
      "3336\n",
      "3337\n",
      "3338\n",
      "3339\n",
      "3340\n",
      "3341\n",
      "3342\n",
      "3343\n",
      "3344\n",
      "3345\n",
      "3346\n",
      "3347\n",
      "3348\n",
      "3349\n",
      "3350\n",
      "3351\n",
      "3352\n",
      "3353\n",
      "3354\n",
      "3355\n",
      "3356\n",
      "3357\n",
      "3358\n",
      "3359\n",
      "3360\n",
      "3361\n",
      "3362\n",
      "3363\n",
      "3364\n",
      "3365\n",
      "3366\n",
      "3367\n",
      "3368\n",
      "3369\n",
      "3370\n",
      "3371\n",
      "3372\n",
      "3373\n",
      "3374\n",
      "3375\n",
      "3376\n",
      "3377\n",
      "3378\n",
      "3379\n",
      "3380\n",
      "3381\n",
      "3382\n",
      "3383\n",
      "3384\n",
      "3385\n",
      "3386\n",
      "3387\n",
      "3388\n",
      "3389\n",
      "3390\n",
      "3391\n",
      "3392\n",
      "3393\n",
      "3394\n",
      "3395\n",
      "3396\n",
      "3397\n",
      "3398\n",
      "3399\n",
      "3400\n",
      "3401\n",
      "3402\n",
      "3403\n",
      "3404\n",
      "3405\n",
      "3406\n",
      "3407\n",
      "3408\n",
      "3409\n",
      "3410\n",
      "3411\n",
      "3412\n",
      "3413\n",
      "3414\n",
      "3415\n",
      "3416\n",
      "3417\n",
      "3418\n",
      "3419\n",
      "3420\n",
      "3421\n",
      "3422\n",
      "3423\n",
      "3424\n",
      "3425\n",
      "3426\n",
      "3427\n",
      "3428\n",
      "3429\n",
      "3430\n",
      "3431\n",
      "3432\n",
      "3433\n",
      "3434\n",
      "3435\n",
      "3436\n",
      "3437\n",
      "3438\n",
      "3439\n",
      "3440\n",
      "3441\n",
      "3442\n",
      "3443\n",
      "3444\n",
      "3445\n",
      "3446\n",
      "3447\n",
      "3448\n",
      "3449\n",
      "3450\n",
      "3451\n",
      "3452\n",
      "3453\n",
      "3454\n",
      "3455\n",
      "3456\n",
      "3457\n",
      "3458\n",
      "3459\n",
      "3460\n",
      "3461\n",
      "3462\n",
      "3463\n",
      "3464\n",
      "3465\n",
      "3466\n",
      "3467\n",
      "3468\n",
      "3469\n",
      "3470\n",
      "3471\n",
      "3472\n",
      "3473\n",
      "3474\n",
      "3475\n",
      "3476\n",
      "3477\n",
      "3478\n",
      "3479\n",
      "3480\n",
      "3481\n",
      "3482\n",
      "3483\n",
      "3484\n",
      "3485\n",
      "3486\n",
      "3487\n",
      "3488\n",
      "3489\n",
      "3490\n",
      "3491\n",
      "3492\n",
      "3493\n",
      "3494\n",
      "3495\n",
      "3496\n",
      "3497\n",
      "3498\n",
      "3499\n",
      "3500\n",
      "3501\n",
      "3502\n",
      "3503\n",
      "3504\n",
      "3505\n",
      "3506\n",
      "3507\n",
      "3508\n",
      "3509\n",
      "3510\n",
      "3511\n",
      "3512\n",
      "3513\n",
      "3514\n",
      "3515\n",
      "3516\n",
      "3517\n",
      "3518\n",
      "3519\n",
      "3520\n",
      "3521\n",
      "3522\n",
      "3523\n",
      "3524\n",
      "3525\n",
      "3526\n",
      "3527\n",
      "3528\n",
      "3529\n",
      "3530\n",
      "3531\n",
      "3532\n",
      "3533\n",
      "3534\n",
      "3535\n",
      "3536\n",
      "3537\n",
      "3538\n",
      "3539\n",
      "3540\n",
      "3541\n",
      "3542\n",
      "3543\n",
      "3544\n",
      "3545\n",
      "3546\n",
      "3547\n",
      "3548\n",
      "3549\n",
      "3550\n",
      "3551\n",
      "3552\n",
      "3553\n",
      "3554\n",
      "3555\n",
      "3556\n",
      "3557\n",
      "3558\n",
      "3559\n",
      "3560\n",
      "3561\n",
      "3562\n",
      "3563\n",
      "3564\n",
      "3565\n",
      "3566\n",
      "3567\n",
      "3568\n",
      "3569\n",
      "3570\n",
      "3571\n",
      "3572\n",
      "3573\n",
      "3574\n",
      "3575\n",
      "3576\n",
      "3577\n",
      "3578\n",
      "3579\n",
      "3580\n",
      "3581\n",
      "3582\n",
      "3583\n",
      "3584\n",
      "3585\n",
      "3586\n",
      "3587\n",
      "3588\n",
      "3589\n",
      "3590\n",
      "3591\n",
      "3592\n",
      "3593\n",
      "3594\n",
      "3595\n",
      "3596\n",
      "3597\n",
      "3598\n",
      "3599\n",
      "3600\n",
      "3601\n",
      "3602\n",
      "3603\n",
      "3604\n",
      "3605\n",
      "3606\n",
      "3607\n",
      "3608\n",
      "3609\n",
      "3610\n",
      "3611\n",
      "3612\n",
      "3613\n",
      "3614\n",
      "3615\n",
      "3616\n",
      "3617\n",
      "3618\n",
      "3619\n",
      "3620\n",
      "3621\n",
      "3622\n",
      "3623\n",
      "3624\n",
      "3625\n",
      "3626\n",
      "3627\n",
      "3628\n",
      "3629\n"
     ]
    }
   ],
   "source": [
    "cov_list = []\n",
    "return_list = []\n",
    "\n",
    "# look back is one year\n",
    "lookback = 252\n",
    "for i in range(lookback, len(df.index.unique())):\n",
    "    print(i)\n",
    "    data_lookback = df.loc[i - lookback : i, :]\n",
    "    price_lookback = data_lookback.pivot_table(\n",
    "        index=\"date\", columns=\"tic\", values=\"close\"\n",
    "    )\n",
    "    return_lookback = price_lookback.pct_change().dropna()\n",
    "    return_list.append(return_lookback)\n",
    "\n",
    "    covs = return_lookback.cov().values\n",
    "    cov_list.append(covs)\n",
    "\n",
    "df_cov = pd.DataFrame(\n",
    "    {\n",
    "        \"date\": df.date.unique()[lookback:],\n",
    "        \"cov_list\": cov_list,\n",
    "        \"return_list\": return_list,\n",
    "    }\n",
    ")\n",
    "df = df.merge(df_cov, on=\"date\")\n",
    "df = df.sort_values([\"date\", \"tic\"]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>cov_list</th>\n",
       "      <th>return_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-12-31</td>\n",
       "      <td>[[0.0013489677302049618, 0.0004284120609250488...</td>\n",
       "      <td>tic             AAPL      AMGN       AXP      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>[[0.0013661495405356042, 0.0004339375976497252...</td>\n",
       "      <td>tic             AAPL      AMGN       AXP      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-05</td>\n",
       "      <td>[[0.0013520207419411223, 0.0004294703188548183...</td>\n",
       "      <td>tic             AAPL      AMGN       AXP      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-06</td>\n",
       "      <td>[[0.0013523443333224704, 0.0004313706504827134...</td>\n",
       "      <td>tic             AAPL      AMGN       AXP      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-07</td>\n",
       "      <td>[[0.0013492616697994262, 0.0004342989585065654...</td>\n",
       "      <td>tic             AAPL      AMGN       AXP      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3373</th>\n",
       "      <td>2022-05-25</td>\n",
       "      <td>[[0.0003068596399267057, 5.032318023578087e-05...</td>\n",
       "      <td>tic             AAPL      AMGN       AXP      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3374</th>\n",
       "      <td>2022-05-26</td>\n",
       "      <td>[[0.0003082087003525896, 4.956585473510691e-05...</td>\n",
       "      <td>tic             AAPL      AMGN       AXP      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3375</th>\n",
       "      <td>2022-05-27</td>\n",
       "      <td>[[0.0003144135347649201, 5.1139723907575336e-0...</td>\n",
       "      <td>tic             AAPL      AMGN       AXP      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3376</th>\n",
       "      <td>2022-05-31</td>\n",
       "      <td>[[0.0003145188250431145, 5.07420910444132e-05,...</td>\n",
       "      <td>tic             AAPL      AMGN       AXP      ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3377</th>\n",
       "      <td>2022-06-01</td>\n",
       "      <td>[[0.0003144151165615493, 5.070254621163054e-05...</td>\n",
       "      <td>tic             AAPL      AMGN       AXP      ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3378 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date                                           cov_list  \\\n",
       "0     2008-12-31  [[0.0013489677302049618, 0.0004284120609250488...   \n",
       "1     2009-01-02  [[0.0013661495405356042, 0.0004339375976497252...   \n",
       "2     2009-01-05  [[0.0013520207419411223, 0.0004294703188548183...   \n",
       "3     2009-01-06  [[0.0013523443333224704, 0.0004313706504827134...   \n",
       "4     2009-01-07  [[0.0013492616697994262, 0.0004342989585065654...   \n",
       "...          ...                                                ...   \n",
       "3373  2022-05-25  [[0.0003068596399267057, 5.032318023578087e-05...   \n",
       "3374  2022-05-26  [[0.0003082087003525896, 4.956585473510691e-05...   \n",
       "3375  2022-05-27  [[0.0003144135347649201, 5.1139723907575336e-0...   \n",
       "3376  2022-05-31  [[0.0003145188250431145, 5.07420910444132e-05,...   \n",
       "3377  2022-06-01  [[0.0003144151165615493, 5.070254621163054e-05...   \n",
       "\n",
       "                                            return_list  \n",
       "0     tic             AAPL      AMGN       AXP      ...  \n",
       "1     tic             AAPL      AMGN       AXP      ...  \n",
       "2     tic             AAPL      AMGN       AXP      ...  \n",
       "3     tic             AAPL      AMGN       AXP      ...  \n",
       "4     tic             AAPL      AMGN       AXP      ...  \n",
       "...                                                 ...  \n",
       "3373  tic             AAPL      AMGN       AXP      ...  \n",
       "3374  tic             AAPL      AMGN       AXP      ...  \n",
       "3375  tic             AAPL      AMGN       AXP      ...  \n",
       "3376  tic             AAPL      AMGN       AXP      ...  \n",
       "3377  tic             AAPL      AMGN       AXP      ...  \n",
       "\n",
       "[3378 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 28, State Space: 28\n",
      "Feature Dimension: 4\n",
      "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n",
      "{'n_steps': 10, 'ent_coef': 0.005, 'learning_rate': 0.0004}\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\versu\\anaconda3\\envs\\Thesis\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning:\n",
      "\n",
      "You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 196       |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39.5     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0004    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | 4.96e+08  |\n",
      "|    reward             | 2429998.8 |\n",
      "|    std                | 0.993     |\n",
      "|    value_loss         | 2.03e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 190       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39.5     |\n",
      "|    explained_variance | 2.38e-07  |\n",
      "|    learning_rate      | 0.0004    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 9.27e+08  |\n",
      "|    reward             | 4439573.0 |\n",
      "|    std                | 0.993     |\n",
      "|    value_loss         | 6.96e+14  |\n",
      "-------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:5861398.7822818095\n",
      "Sharpe:  1.0382088813503267\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 188       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39.5     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0004    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | 3.02e+08  |\n",
      "|    reward             | 1581578.1 |\n",
      "|    std                | 0.993     |\n",
      "|    value_loss         | 8.35e+13  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 188       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0004    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | 6e+08     |\n",
      "|    reward             | 3039548.0 |\n",
      "|    std                | 0.991     |\n",
      "|    value_loss         | 3.06e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 187       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 26        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39.5     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0004    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 1.19e+09  |\n",
      "|    reward             | 5195028.5 |\n",
      "|    std                | 0.99      |\n",
      "|    value_loss         | 1.11e+15  |\n",
      "-------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:5659172.882683877\n",
      "Sharpe:  1.0211472674395639\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 183       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 32        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0004    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 3.96e+08  |\n",
      "|    reward             | 1850024.4 |\n",
      "|    std                | 0.99      |\n",
      "|    value_loss         | 1.19e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 183       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 38        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0004    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 7.74e+08  |\n",
      "|    reward             | 3533433.2 |\n",
      "|    std                | 0.99      |\n",
      "|    value_loss         | 4.46e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 183       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 43        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0004    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 1.15e+09  |\n",
      "|    reward             | 5776537.5 |\n",
      "|    std                | 0.988     |\n",
      "|    value_loss         | 1.09e+15  |\n",
      "-------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:5857874.40769748\n",
      "Sharpe:  1.0342330452346085\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 183       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 48        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0004    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 3.97e+08  |\n",
      "|    reward             | 1893572.0 |\n",
      "|    std                | 0.988     |\n",
      "|    value_loss         | 1.19e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 185       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 53        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0004    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 7.54e+08  |\n",
      "|    reward             | 3692434.8 |\n",
      "|    std                | 0.988     |\n",
      "|    value_loss         | 4.92e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 186       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 58        |\n",
      "|    total_timesteps    | 11000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0004    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | 1.45e+09  |\n",
      "|    reward             | 6737937.5 |\n",
      "|    std                | 0.988     |\n",
      "|    value_loss         | 1.58e+15  |\n",
      "-------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:5705631.026507605\n",
      "Sharpe:  1.0272655255549403\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 187       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 63        |\n",
      "|    total_timesteps    | 12000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0004    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 4.96e+08  |\n",
      "|    reward             | 2390331.5 |\n",
      "|    std                | 0.988     |\n",
      "|    value_loss         | 1.93e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 187       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 69        |\n",
      "|    total_timesteps    | 13000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39.4     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0004    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 8.94e+08  |\n",
      "|    reward             | 4724073.0 |\n",
      "|    std                | 0.987     |\n",
      "|    value_loss         | 7.36e+14  |\n",
      "-------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:6661240.28740104\n",
      "Sharpe:  1.0997991047249152\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 188       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 74        |\n",
      "|    total_timesteps    | 14000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0004    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 2.91e+08  |\n",
      "|    reward             | 1491935.8 |\n",
      "|    std                | 0.988     |\n",
      "|    value_loss         | 7.1e+13   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 189       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 79        |\n",
      "|    total_timesteps    | 15000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39.4     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0004    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 5.63e+08  |\n",
      "|    reward             | 2758229.8 |\n",
      "|    std                | 0.988     |\n",
      "|    value_loss         | 2.69e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 190       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 83        |\n",
      "|    total_timesteps    | 16000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39.4     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0004    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 1.01e+09  |\n",
      "|    reward             | 4977263.5 |\n",
      "|    std                | 0.987     |\n",
      "|    value_loss         | 8.45e+14  |\n",
      "-------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:5802473.47736592\n",
      "Sharpe:  1.0289802601750648\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 191       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 88        |\n",
      "|    total_timesteps    | 17000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0004    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 3.44e+08  |\n",
      "|    reward             | 1682375.6 |\n",
      "|    std                | 0.987     |\n",
      "|    value_loss         | 9.69e+13  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 192       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 93        |\n",
      "|    total_timesteps    | 18000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39.4     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0004    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 7.12e+08  |\n",
      "|    reward             | 3288199.5 |\n",
      "|    std                | 0.987     |\n",
      "|    value_loss         | 4.1e+14   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 192       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 98        |\n",
      "|    total_timesteps    | 19000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39.3     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0004    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | 1.3e+09   |\n",
      "|    reward             | 6287723.5 |\n",
      "|    std                | 0.987     |\n",
      "|    value_loss         | 1.37e+15  |\n",
      "-------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:6279596.145769753\n",
      "Sharpe:  1.070172936157019\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 192       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 103       |\n",
      "|    total_timesteps    | 20000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0004    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 3.53e+08  |\n",
      "|    reward             | 1687363.6 |\n",
      "|    std                | 0.986     |\n",
      "|    value_loss         | 1.04e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 192       |\n",
      "|    iterations         | 2100      |\n",
      "|    time_elapsed       | 108       |\n",
      "|    total_timesteps    | 21000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0004    |\n",
      "|    n_updates          | 2099      |\n",
      "|    policy_loss        | 7.25e+08  |\n",
      "|    reward             | 3521847.5 |\n",
      "|    std                | 0.986     |\n",
      "|    value_loss         | 4.44e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 192       |\n",
      "|    iterations         | 2200      |\n",
      "|    time_elapsed       | 114       |\n",
      "|    total_timesteps    | 22000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0004    |\n",
      "|    n_updates          | 2199      |\n",
      "|    policy_loss        | 1.28e+09  |\n",
      "|    reward             | 6395615.0 |\n",
      "|    std                | 0.986     |\n",
      "|    value_loss         | 1.42e+15  |\n",
      "-------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:6076678.062573772\n",
      "Sharpe:  1.0610053137246493\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 191       |\n",
      "|    iterations         | 2300      |\n",
      "|    time_elapsed       | 120       |\n",
      "|    total_timesteps    | 23000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0004    |\n",
      "|    n_updates          | 2299      |\n",
      "|    policy_loss        | 4.52e+08  |\n",
      "|    reward             | 2149087.0 |\n",
      "|    std                | 0.985     |\n",
      "|    value_loss         | 1.58e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 191       |\n",
      "|    iterations         | 2400      |\n",
      "|    time_elapsed       | 125       |\n",
      "|    total_timesteps    | 24000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39.3     |\n",
      "|    explained_variance | 2.38e-07  |\n",
      "|    learning_rate      | 0.0004    |\n",
      "|    n_updates          | 2399      |\n",
      "|    policy_loss        | 8.15e+08  |\n",
      "|    reward             | 3951011.2 |\n",
      "|    std                | 0.985     |\n",
      "|    value_loss         | 5.46e+14  |\n",
      "-------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:5439576.2346422365\n",
      "Sharpe:  0.9979430721397191\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 191       |\n",
      "|    iterations         | 2500      |\n",
      "|    time_elapsed       | 130       |\n",
      "|    total_timesteps    | 25000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0004    |\n",
      "|    n_updates          | 2499      |\n",
      "|    policy_loss        | 2.66e+08  |\n",
      "|    reward             | 1301427.5 |\n",
      "|    std                | 0.984     |\n",
      "|    value_loss         | 5.35e+13  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 191       |\n",
      "|    iterations         | 2600      |\n",
      "|    time_elapsed       | 135       |\n",
      "|    total_timesteps    | 26000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39.3     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0004    |\n",
      "|    n_updates          | 2599      |\n",
      "|    policy_loss        | 4.98e+08  |\n",
      "|    reward             | 2538438.0 |\n",
      "|    std                | 0.984     |\n",
      "|    value_loss         | 2.25e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 192       |\n",
      "|    iterations         | 2700      |\n",
      "|    time_elapsed       | 140       |\n",
      "|    total_timesteps    | 27000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39.2     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0004    |\n",
      "|    n_updates          | 2699      |\n",
      "|    policy_loss        | 8.53e+08  |\n",
      "|    reward             | 4157102.2 |\n",
      "|    std                | 0.982     |\n",
      "|    value_loss         | 6.08e+14  |\n",
      "-------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:4929133.26463395\n",
      "Sharpe:  0.9519400136558493\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 192       |\n",
      "|    iterations         | 2800      |\n",
      "|    time_elapsed       | 145       |\n",
      "|    total_timesteps    | 28000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39.1     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0004    |\n",
      "|    n_updates          | 2799      |\n",
      "|    policy_loss        | 3.15e+08  |\n",
      "|    reward             | 1461373.5 |\n",
      "|    std                | 0.979     |\n",
      "|    value_loss         | 7.97e+13  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 192       |\n",
      "|    iterations         | 2900      |\n",
      "|    time_elapsed       | 150       |\n",
      "|    total_timesteps    | 29000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0004    |\n",
      "|    n_updates          | 2899      |\n",
      "|    policy_loss        | 6.23e+08  |\n",
      "|    reward             | 3019307.0 |\n",
      "|    std                | 0.979     |\n",
      "|    value_loss         | 3.22e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 192       |\n",
      "|    iterations         | 3000      |\n",
      "|    time_elapsed       | 155       |\n",
      "|    total_timesteps    | 30000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39.1     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0004    |\n",
      "|    n_updates          | 2999      |\n",
      "|    policy_loss        | 1.06e+09  |\n",
      "|    reward             | 5038840.5 |\n",
      "|    std                | 0.977     |\n",
      "|    value_loss         | 9.02e+14  |\n",
      "-------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:5276150.9674403975\n",
      "Sharpe:  0.9895614063061918\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 193       |\n",
      "|    iterations         | 3100      |\n",
      "|    time_elapsed       | 160       |\n",
      "|    total_timesteps    | 31000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0004    |\n",
      "|    n_updates          | 3099      |\n",
      "|    policy_loss        | 3.64e+08  |\n",
      "|    reward             | 1797481.6 |\n",
      "|    std                | 0.976     |\n",
      "|    value_loss         | 1.22e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 193       |\n",
      "|    iterations         | 3200      |\n",
      "|    time_elapsed       | 165       |\n",
      "|    total_timesteps    | 32000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0004    |\n",
      "|    n_updates          | 3199      |\n",
      "|    policy_loss        | 7.92e+08  |\n",
      "|    reward             | 3642011.8 |\n",
      "|    std                | 0.976     |\n",
      "|    value_loss         | 4.8e+14   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 193       |\n",
      "|    iterations         | 3300      |\n",
      "|    time_elapsed       | 170       |\n",
      "|    total_timesteps    | 33000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0004    |\n",
      "|    n_updates          | 3299      |\n",
      "|    policy_loss        | 1.31e+09  |\n",
      "|    reward             | 6103835.0 |\n",
      "|    std                | 0.975     |\n",
      "|    value_loss         | 1.38e+15  |\n",
      "-------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:5465129.50217662\n",
      "Sharpe:  1.0075615798652258\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 193       |\n",
      "|    iterations         | 3400      |\n",
      "|    time_elapsed       | 175       |\n",
      "|    total_timesteps    | 34000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0004    |\n",
      "|    n_updates          | 3399      |\n",
      "|    policy_loss        | 4.52e+08  |\n",
      "|    reward             | 2157999.5 |\n",
      "|    std                | 0.975     |\n",
      "|    value_loss         | 1.74e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 194       |\n",
      "|    iterations         | 3500      |\n",
      "|    time_elapsed       | 180       |\n",
      "|    total_timesteps    | 35000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39.1     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0004    |\n",
      "|    n_updates          | 3499      |\n",
      "|    policy_loss        | 8.78e+08  |\n",
      "|    reward             | 3914141.8 |\n",
      "|    std                | 0.976     |\n",
      "|    value_loss         | 5.6e+14   |\n",
      "-------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:5872723.637257791\n",
      "Sharpe:  1.0346311878519012\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 194       |\n",
      "|    iterations         | 3600      |\n",
      "|    time_elapsed       | 185       |\n",
      "|    total_timesteps    | 36000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0004    |\n",
      "|    n_updates          | 3599      |\n",
      "|    policy_loss        | 3.06e+08  |\n",
      "|    reward             | 1076983.6 |\n",
      "|    std                | 0.975     |\n",
      "|    value_loss         | 1.09e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 194       |\n",
      "|    iterations         | 3700      |\n",
      "|    time_elapsed       | 190       |\n",
      "|    total_timesteps    | 37000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39       |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0004    |\n",
      "|    n_updates          | 3699      |\n",
      "|    policy_loss        | 4.96e+08  |\n",
      "|    reward             | 2560052.2 |\n",
      "|    std                | 0.974     |\n",
      "|    value_loss         | 2.27e+14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 194       |\n",
      "|    iterations         | 3800      |\n",
      "|    time_elapsed       | 195       |\n",
      "|    total_timesteps    | 38000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0004    |\n",
      "|    n_updates          | 3799      |\n",
      "|    policy_loss        | 8.99e+08  |\n",
      "|    reward             | 4339597.0 |\n",
      "|    std                | 0.974     |\n",
      "|    value_loss         | 6.9e+14   |\n",
      "-------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:5576712.837952836\n",
      "Sharpe:  1.0092728537406925\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 194       |\n",
      "|    iterations         | 3900      |\n",
      "|    time_elapsed       | 200       |\n",
      "|    total_timesteps    | 39000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0004    |\n",
      "|    n_updates          | 3899      |\n",
      "|    policy_loss        | 3.09e+08  |\n",
      "|    reward             | 1586175.2 |\n",
      "|    std                | 0.974     |\n",
      "|    value_loss         | 8.47e+13  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 194       |\n",
      "|    iterations         | 4000      |\n",
      "|    time_elapsed       | 205       |\n",
      "|    total_timesteps    | 40000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -39       |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0004    |\n",
      "|    n_updates          | 3999      |\n",
      "|    policy_loss        | 5.99e+08  |\n",
      "|    reward             | 2948776.5 |\n",
      "|    std                | 0.973     |\n",
      "|    value_loss         | 3.07e+14  |\n",
      "-------------------------------------\n",
      "{'n_steps': 2048, 'ent_coef': 0.005, 'learning_rate': 0.001, 'batch_size': 128}\n",
      "Using cuda device\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    fps             | 237       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 8         |\n",
      "|    total_timesteps | 2048      |\n",
      "| train/             |           |\n",
      "|    reward          | 3971309.5 |\n",
      "----------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:5020691.902390935\n",
      "Sharpe:  0.9674629464957293\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 243          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.284122e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -39.7        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 9.17e+14     |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -5.69e-06    |\n",
      "|    reward               | 3280868.2    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.82e+15     |\n",
      "------------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:5367831.081203733\n",
      "Sharpe:  0.9940842588515922\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 245          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 24           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.032657e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -39.7        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 1.61e+15     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -4.03e-06    |\n",
      "|    reward               | 1786799.1    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.29e+15     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 248           |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 32            |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.4983185e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -39.7         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.001         |\n",
      "|    loss                 | 2.37e+15      |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -3.35e-06     |\n",
      "|    reward               | 6443330.5     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 4.83e+15      |\n",
      "-------------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:5626426.064940609\n",
      "Sharpe:  1.019012081767638\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 249          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 41           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.411007e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -39.7        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 2.47e+15     |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -2.87e-06    |\n",
      "|    reward               | 4088138.8    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.64e+15     |\n",
      "------------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:5468377.499801045\n",
      "Sharpe:  1.0038294470214613\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 249          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 49           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.556526e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -39.7        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 1.14e+15     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -4.25e-06    |\n",
      "|    reward               | 3045168.0    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.42e+15     |\n",
      "------------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:6140128.9445697805\n",
      "Sharpe:  1.0725604633438495\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 248          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 57           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.556526e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -39.7        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 2.05e+15     |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -3.49e-06    |\n",
      "|    reward               | 1804229.6    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.17e+15     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 249         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 65          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 8.20728e-09 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -39.7       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 3.08e+15    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -2.83e-06   |\n",
      "|    reward               | 6308868.5   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 5.6e+15     |\n",
      "-----------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:5684200.350277098\n",
      "Sharpe:  1.0218332529146414\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 249          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 73           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.537892e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -39.7        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 2.12e+15     |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -3.25e-06    |\n",
      "|    reward               | 3599011.0    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.2e+15      |\n",
      "------------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:5555009.227800655\n",
      "Sharpe:  1.013397990742589\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 249          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 82           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.080395e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -39.7        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 1.41e+15     |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -4.43e-06    |\n",
      "|    reward               | 2903945.0    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.91e+15     |\n",
      "------------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:6033641.88899506\n",
      "Sharpe:  1.0587915309460005\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 249          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 90           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.789357e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -39.7        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 1.91e+15     |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -3.5e-06     |\n",
      "|    reward               | 1691472.6    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.05e+15     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 251          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 97           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.789357e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -39.7        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 2.59e+15     |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -2.6e-06     |\n",
      "|    reward               | 5852910.5    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 5.73e+15     |\n",
      "------------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:5900073.567601181\n",
      "Sharpe:  1.0420415133694667\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 252          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 105          |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.508788e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -39.7        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 1.71e+15     |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -3.36e-06    |\n",
      "|    reward               | 3177676.2    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.7e+15      |\n",
      "------------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:5509065.682173784\n",
      "Sharpe:  1.0052349331002914\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 253          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 113          |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.469215e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -39.7        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 1.53e+15     |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -3.74e-06    |\n",
      "|    reward               | 2372554.0    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.2e+15      |\n",
      "------------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:5658003.186946508\n",
      "Sharpe:  1.0199766754211768\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 254          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 120          |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.352799e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -39.7        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 1.9e+15      |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -3.48e-06    |\n",
      "|    reward               | 1601352.1    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.12e+15     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 255          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 128          |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.294592e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -39.7        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 2.93e+15     |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -2.86e-06    |\n",
      "|    reward               | 5323721.5    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 5.09e+15     |\n",
      "------------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:5372154.3260090295\n",
      "Sharpe:  0.997768772976139\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 255          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 136          |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.527422e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -39.7        |\n",
      "|    explained_variance   | -2.38e-07    |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 1.54e+15     |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -4.04e-06    |\n",
      "|    reward               | 3417173.8    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.04e+15     |\n",
      "------------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:5461460.917055697\n",
      "Sharpe:  1.0035780478946967\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 256          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 143          |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.284122e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -39.7        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 1.53e+15     |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -3.97e-06    |\n",
      "|    reward               | 2228145.2    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.23e+15     |\n",
      "------------------------------------------\n",
      "=================================\n",
      "begin_total_asset:1000000\n",
      "end_total_asset:5721620.103082287\n",
      "Sharpe:  1.0332549164082256\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 257          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 151          |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.469215e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -39.7        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 2.22e+15     |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -3.62e-06    |\n",
      "|    reward               | 1484047.9    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.1e+15      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 257          |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 158          |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.702045e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -39.7        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.001        |\n",
      "|    loss                 | 2.49e+15     |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -3.06e-06    |\n",
      "|    reward               | 5753696.0    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 5.05e+15     |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train = data_split(df, \"2009-04-01\", \"2020-03-31\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gym.utils import seeding\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "\n",
    "class StockPortfolioEnv(gym.Env):\n",
    "    \"\"\"A portfolio allocation environment for OpenAI gym\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "        df: DataFrame\n",
    "            input data\n",
    "        stock_dim : int\n",
    "            number of unique stocks\n",
    "        hmax : int\n",
    "            maximum number of shares to trade\n",
    "        initial_amount : int\n",
    "            start money\n",
    "        transaction_cost_pct: float\n",
    "            transaction cost percentage per trade\n",
    "        reward_scaling: float\n",
    "            scaling factor for reward, good for training\n",
    "        state_space: int\n",
    "            the dimension of input features\n",
    "        action_space: int\n",
    "            equals stock dimension\n",
    "        tech_indicator_list: list\n",
    "            a list of technical indicator names\n",
    "        turbulence_threshold: int\n",
    "            a threshold to control risk aversion\n",
    "        day: int\n",
    "            an increment number to control date\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    _sell_stock()\n",
    "        perform sell action based on the sign of the action\n",
    "    _buy_stock()\n",
    "        perform buy action based on the sign of the action\n",
    "    step()\n",
    "        at each step the agent will return actions, then\n",
    "        we will calculate the reward, and return the next observation.\n",
    "    reset()\n",
    "        reset the environment\n",
    "    render()\n",
    "        use render to return other functions\n",
    "    save_asset_memory()\n",
    "        return account value at each time step\n",
    "    save_action_memory()\n",
    "        return actions/positions at each time step\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render.modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        stock_dim,\n",
    "        hmax,\n",
    "        initial_amount,\n",
    "        transaction_cost_pct,\n",
    "        reward_scaling,\n",
    "        state_space,\n",
    "        action_space,\n",
    "        tech_indicator_list,\n",
    "        turbulence_threshold=None,\n",
    "        lookback=252,\n",
    "        day=0,\n",
    "    ):\n",
    "        # super(StockEnv, self).__init__()\n",
    "        # money = 10 , scope = 1\n",
    "        self.day = day\n",
    "        self.lookback = lookback\n",
    "        self.df = df\n",
    "        self.stock_dim = stock_dim\n",
    "        self.hmax = hmax\n",
    "        self.initial_amount = initial_amount\n",
    "        self.transaction_cost_pct = transaction_cost_pct\n",
    "        self.reward_scaling = reward_scaling\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.tech_indicator_list = tech_indicator_list\n",
    "\n",
    "        # action_space normalization and shape is self.stock_dim\n",
    "        self.action_space = spaces.Box(low=0, high=1, shape=(self.action_space,))\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=(\n",
    "                self.state_space + len(self.tech_indicator_list),\n",
    "                self.state_space,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # load data from a pandas dataframe\n",
    "        self.data = self.df.loc[self.day, :]\n",
    "        self.covs = self.data[\"cov_list\"].values[0]\n",
    "        self.state = np.append(\n",
    "            np.array(self.covs),\n",
    "            [self.data[tech].values.tolist() for tech in self.tech_indicator_list],\n",
    "            axis=0,\n",
    "        )\n",
    "        self.terminal = False\n",
    "        self.turbulence_threshold = turbulence_threshold\n",
    "        # initalize state: inital portfolio return + individual stock return + individual weights\n",
    "        self.portfolio_value = self.initial_amount\n",
    "\n",
    "        # memorize portfolio value each step\n",
    "        self.asset_memory = [self.initial_amount]\n",
    "        # memorize portfolio return each step\n",
    "        self.portfolio_return_memory = [0]\n",
    "        self.actions_memory = [[1 / self.stock_dim] * self.stock_dim]\n",
    "        self.date_memory = [self.data.date.unique()[0]]\n",
    "\n",
    "    def step(self, actions):\n",
    "        self.terminal = self.day >= len(self.df.index.unique()) - 1\n",
    "\n",
    "        if self.terminal:\n",
    "            df = pd.DataFrame(self.portfolio_return_memory)\n",
    "            df.columns = [\"daily_return\"]\n",
    "            plt.plot(df.daily_return.cumsum(), \"r\")\n",
    "            plt.savefig(\"results/cumulative_reward.png\")\n",
    "            plt.close()\n",
    "\n",
    "            plt.plot(self.portfolio_return_memory, \"r\")\n",
    "            plt.savefig(\"results/rewards.png\")\n",
    "            plt.close()\n",
    "\n",
    "            print(\"=================================\")\n",
    "            print(\"begin_total_asset:{}\".format(self.asset_memory[0]))\n",
    "            print(\"end_total_asset:{}\".format(self.portfolio_value))\n",
    "\n",
    "            df_daily_return = pd.DataFrame(self.portfolio_return_memory)\n",
    "            df_daily_return.columns = [\"daily_return\"]\n",
    "            if df_daily_return[\"daily_return\"].std() != 0:\n",
    "                sharpe = (\n",
    "                    (252**0.5)\n",
    "                    * df_daily_return[\"daily_return\"].mean()\n",
    "                    / df_daily_return[\"daily_return\"].std()\n",
    "                )\n",
    "                print(\"Sharpe: \", sharpe)\n",
    "            print(\"=================================\")\n",
    "\n",
    "            return self.state, self.reward, self.terminal, {}\n",
    "\n",
    "        else:\n",
    "            weights = self.softmax_normalization(actions)\n",
    "            self.actions_memory.append(weights)\n",
    "            last_day_memory = self.data\n",
    "\n",
    "            # load next state\n",
    "            self.day += 1\n",
    "            self.data = self.df.loc[self.day, :]\n",
    "            self.covs = self.data[\"cov_list\"].values[0]\n",
    "            self.state = np.append(\n",
    "                np.array(self.covs),\n",
    "                [self.data[tech].values.tolist() for tech in self.tech_indicator_list],\n",
    "                axis=0,\n",
    "            )\n",
    "            portfolio_return = sum(\n",
    "                ((self.data.close.values / last_day_memory.close.values) - 1) * weights\n",
    "            )\n",
    "            log_portfolio_return = np.log(\n",
    "                sum((self.data.close.values / last_day_memory.close.values) * weights)\n",
    "            )\n",
    "            # update portfolio value\n",
    "            new_portfolio_value = self.portfolio_value * (1 + portfolio_return)\n",
    "            self.portfolio_value = new_portfolio_value\n",
    "\n",
    "            # save into memory\n",
    "            self.portfolio_return_memory.append(portfolio_return)\n",
    "            self.date_memory.append(self.data.date.unique()[0])\n",
    "            self.asset_memory.append(new_portfolio_value)\n",
    "\n",
    "            # the reward is the new portfolio value or end portfolo value\n",
    "            self.reward = new_portfolio_value\n",
    "\n",
    "        return self.state, self.reward, self.terminal, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.asset_memory = [self.initial_amount]\n",
    "        self.day = 0\n",
    "        self.data = self.df.loc[self.day, :]\n",
    "        # load states\n",
    "        self.covs = self.data[\"cov_list\"].values[0]\n",
    "        self.state = np.append(\n",
    "            np.array(self.covs),\n",
    "            [self.data[tech].values.tolist() for tech in self.tech_indicator_list],\n",
    "            axis=0,\n",
    "        )\n",
    "        self.portfolio_value = self.initial_amount\n",
    "        # self.cost = 0\n",
    "        # self.trades = 0\n",
    "        self.terminal = False\n",
    "        self.portfolio_return_memory = [0]\n",
    "        self.actions_memory = [[1 / self.stock_dim] * self.stock_dim]\n",
    "        self.date_memory = [self.data.date.unique()[0]]\n",
    "        return self.state\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        return self.state\n",
    "\n",
    "    def softmax_normalization(self, actions):\n",
    "        numerator = np.exp(actions)\n",
    "        denominator = np.sum(np.exp(actions))\n",
    "        softmax_output = numerator / denominator\n",
    "        return softmax_output\n",
    "\n",
    "    def save_asset_memory(self):\n",
    "        date_list = self.date_memory\n",
    "        portfolio_return = self.portfolio_return_memory\n",
    "        # print(len(date_list))\n",
    "        # print(len(asset_list))\n",
    "        df_account_value = pd.DataFrame(\n",
    "            {\"date\": date_list, \"daily_return\": portfolio_return}\n",
    "        )\n",
    "        return df_account_value\n",
    "\n",
    "    def save_action_memory(self):\n",
    "        # date and close price length must match actions length\n",
    "        date_list = self.date_memory\n",
    "        df_date = pd.DataFrame(date_list)\n",
    "        df_date.columns = [\"date\"]\n",
    "\n",
    "        action_list = self.actions_memory\n",
    "        df_actions = pd.DataFrame(action_list)\n",
    "        df_actions.columns = self.data.tic.values\n",
    "        df_actions.index = df_date.date\n",
    "        # df_actions = pd.DataFrame({'date':date_list,'actions':action_list})\n",
    "        return df_actions\n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def get_sb_env(self):\n",
    "        e = DummyVecEnv([lambda: self])\n",
    "        obs = e.reset()\n",
    "        return e, obs\n",
    "\n",
    "\n",
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n",
    "tech_indicator_list = [\"macd\", \"rsi_30\", \"cci_30\", \"dx_30\"]\n",
    "feature_dimension = len(tech_indicator_list)\n",
    "print(f\"Feature Dimension: {feature_dimension}\")\n",
    "\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100,\n",
    "    \"initial_amount\": 1000000,\n",
    "    \"transaction_cost_pct\": 0,\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": tech_indicator_list,\n",
    "    \"action_space\": stock_dimension,\n",
    "    \"reward_scaling\": 1e-1,\n",
    "}\n",
    "\n",
    "e_train_gym = StockPortfolioEnv(df=train, **env_kwargs)\n",
    "\n",
    "env_train, _ = e_train_gym.get_sb_env()\n",
    "print(type(env_train))\n",
    "\n",
    "from FinRL.finrl.agents.stablebaselines3.models import DRLAgent\n",
    "\n",
    "agent = DRLAgent(env=env_train)\n",
    "\n",
    "A2C_PARAMS = {\"n_steps\": 10, \"ent_coef\": 0.005, \"learning_rate\": 0.0004}\n",
    "model_a2c = agent.get_model(model_name=\"a2c\", model_kwargs=A2C_PARAMS)\n",
    "\n",
    "trained_a2c = agent.train_model(\n",
    "    model=model_a2c, tb_log_name=\"a2c\", total_timesteps=40000\n",
    ")\n",
    "\n",
    "agent = DRLAgent(env=env_train)\n",
    "PPO_PARAMS = {\n",
    "    \"n_steps\": 2048,\n",
    "    \"ent_coef\": 0.005,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "model_ppo = agent.get_model(\"ppo\", model_kwargs=PPO_PARAMS)\n",
    "\n",
    "trained_ppo = agent.train_model(\n",
    "    model=model_ppo, tb_log_name=\"ppo\", total_timesteps=40000\n",
    ")\n",
    "\n",
    "trade = data_split(df, \"2020-04-01\", \"2022-05-31\")\n",
    "e_trade_gym = StockPortfolioEnv(df=trade, **env_kwargs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "from pypfopt import risk_models\n",
    "import pandas as pd\n",
    "from pypfopt import EfficientFrontier\n",
    "from pypfopt import risk_models\n",
    "from pypfopt import expected_returns\n",
    "from pypfopt import objective_functions\n",
    "\n",
    "unique_tic = trade.tic.unique()\n",
    "unique_trade_date = trade.date.unique()\n",
    "\n",
    "import pyfolio\n",
    "\n",
    "from finrl.plot import (\n",
    "    backtest_stats,\n",
    "    backtest_plot,\n",
    "    get_daily_return,\n",
    "    get_baseline,\n",
    "    convert_daily_return_to_pyfolio_ts,\n",
    ")\n",
    "\n",
    "baseline_df = get_baseline(ticker=\"^DJI\", start=\"2020-07-01\", end=\"2021-09-01\")\n",
    "\n",
    "baseline_df_stats = backtest_stats(baseline_df, value_col_name=\"close\")\n",
    "baseline_returns = get_daily_return(baseline_df, value_col_name=\"close\")\n",
    "\n",
    "dji_cumpod = (baseline_returns + 1).cumprod() - 1\n",
    "\n",
    "from pyfolio import timeseries\n",
    "\n",
    "df_daily_return_a2c, df_actions_a2c = DRLAgent.DRL_prediction(\n",
    "    model=trained_a2c, environment=e_trade_gym\n",
    ")\n",
    "df_daily_return_ppo, df_actions_ppo = DRLAgent.DRL_prediction(\n",
    "    model=trained_ppo, environment=e_trade_gym\n",
    ")\n",
    "time_ind = pd.Series(df_daily_return_a2c.date)\n",
    "a2c_cumpod = (df_daily_return_a2c.daily_return + 1).cumprod() - 1\n",
    "ppo_cumpod = (df_daily_return_ppo.daily_return + 1).cumprod() - 1\n",
    "DRL_strat_a2c = convert_daily_return_to_pyfolio_ts(df_daily_return_a2c)\n",
    "DRL_strat_ppo = convert_daily_return_to_pyfolio_ts(df_daily_return_ppo)\n",
    "\n",
    "perf_func = timeseries.perf_stats\n",
    "perf_stats_all_a2c = perf_func(\n",
    "    returns=DRL_strat_a2c,\n",
    "    factor_returns=DRL_strat_a2c,\n",
    "    positions=None,\n",
    "    transactions=None,\n",
    "    turnover_denom=\"AGB\",\n",
    ")\n",
    "perf_stats_all_ppo = perf_func(\n",
    "    returns=DRL_strat_ppo,\n",
    "    factor_returns=DRL_strat_ppo,\n",
    "    positions=None,\n",
    "    transactions=None,\n",
    "    turnover_denom=\"AGB\",\n",
    ")\n",
    "\n",
    "\n",
    "def extract_weights(drl_actions_list):\n",
    "    a2c_weight_df = {\"date\": [], \"weights\": []}\n",
    "    for i in range(len(drl_actions_list)):\n",
    "        date = drl_actions_list.index[i]\n",
    "        tic_list = list(drl_actions_list.columns)\n",
    "        weights_list = (\n",
    "            drl_actions_list.reset_index()[list(drl_actions_list.columns)]\n",
    "            .iloc[i]\n",
    "            .values\n",
    "        )\n",
    "        weight_dict = {\"tic\": [], \"weight\": []}\n",
    "        for j in range(len(tic_list)):\n",
    "            weight_dict[\"tic\"] += [tic_list[j]]\n",
    "            weight_dict[\"weight\"] += [weights_list[j]]\n",
    "\n",
    "        a2c_weight_df[\"date\"] += [date]\n",
    "        a2c_weight_df[\"weights\"] += [pd.DataFrame(weight_dict)]\n",
    "\n",
    "    a2c_weights = pd.DataFrame(a2c_weight_df)\n",
    "    return a2c_weights\n",
    "\n",
    "\n",
    "a2c_weights = extract_weights(df_actions_a2c)\n",
    "ppo_weights = extract_weights(df_actions_ppo)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "def prepare_data(trainData):\n",
    "    train_date = sorted(set(trainData.date.values))\n",
    "    X = []\n",
    "    for i in range(0, len(train_date) - 1):\n",
    "        d = train_date[i]\n",
    "        d_next = train_date[i + 1]\n",
    "        y = (\n",
    "            train.loc[train[\"date\"] == d_next]\n",
    "            .return_list.iloc[0]\n",
    "            .loc[d_next]\n",
    "            .reset_index()\n",
    "        )\n",
    "        y.columns = [\"tic\", \"return\"]\n",
    "        x = train.loc[train[\"date\"] == d][[\"tic\", \"macd\", \"rsi_30\", \"cci_30\", \"dx_30\"]]\n",
    "        train_piece = pd.merge(x, y, on=\"tic\")\n",
    "        train_piece[\"date\"] = [d] * len(train_piece)\n",
    "        X += [train_piece]\n",
    "    trainDataML = pd.concat(X)\n",
    "    X = trainDataML[tech_indicator_list].values\n",
    "    Y = trainDataML[[\"return\"]].values\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "train_X, train_Y = prepare_data(train)\n",
    "rf_model = RandomForestRegressor(\n",
    "    max_depth=35, min_samples_split=10, random_state=0\n",
    ").fit(train_X, train_Y.reshape(-1))\n",
    "dt_model = DecisionTreeRegressor(\n",
    "    random_state=0, max_depth=35, min_samples_split=10\n",
    ").fit(train_X, train_Y.reshape(-1))\n",
    "svm_model = SVR(epsilon=0.14).fit(train_X, train_Y.reshape(-1))\n",
    "lr_model = LinearRegression().fit(train_X, train_Y)\n",
    "\n",
    "\n",
    "def output_predict(model, reference_model=False):\n",
    "    meta_coefficient = {\"date\": [], \"weights\": []}\n",
    "\n",
    "    portfolio = pd.DataFrame(index=range(1), columns=unique_trade_date)\n",
    "    initial_capital = 1000000\n",
    "    portfolio.loc[0, unique_trade_date[0]] = initial_capital\n",
    "\n",
    "    for i in range(len(unique_trade_date) - 1):\n",
    "\n",
    "        current_date = unique_trade_date[i]\n",
    "        next_date = unique_trade_date[i + 1]\n",
    "        df_current = df[df.date == current_date].reset_index(drop=True)\n",
    "        tics = df_current[\"tic\"].values\n",
    "        features = df_current[tech_indicator_list].values\n",
    "        df_next = df[df.date == next_date].reset_index(drop=True)\n",
    "        if not reference_model:\n",
    "            predicted_y = model.predict(features)\n",
    "            mu = predicted_y\n",
    "            Sigma = risk_models.sample_cov(df_current.return_list[0], returns_data=True)\n",
    "        else:\n",
    "            mu = df_next.return_list[0].loc[next_date].values\n",
    "            Sigma = risk_models.sample_cov(df_next.return_list[0], returns_data=True)\n",
    "        predicted_y_df = pd.DataFrame(\n",
    "            {\n",
    "                \"tic\": tics.reshape(\n",
    "                    -1,\n",
    "                ),\n",
    "                \"predicted_y\": mu.reshape(\n",
    "                    -1,\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "        min_weight, max_weight = 0, 1\n",
    "        ef = EfficientFrontier(mu, Sigma)\n",
    "        weights = ef.nonconvex_objective(\n",
    "            objective_functions.sharpe_ratio,\n",
    "            objective_args=(ef.expected_returns, ef.cov_matrix),\n",
    "            weights_sum_to_one=True,\n",
    "            constraints=[\n",
    "                {\n",
    "                    \"type\": \"ineq\",\n",
    "                    \"fun\": lambda w: w - min_weight,\n",
    "                },  # greater than min_weight\n",
    "                {\n",
    "                    \"type\": \"ineq\",\n",
    "                    \"fun\": lambda w: max_weight - w,\n",
    "                },  # less than max_weight\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        weight_df = {\"tic\": [], \"weight\": []}\n",
    "        meta_coefficient[\"date\"] += [current_date]\n",
    "        # it = 0\n",
    "        for item in weights:\n",
    "            weight_df[\"tic\"] += [item]\n",
    "            weight_df[\"weight\"] += [weights[item]]\n",
    "\n",
    "        weight_df = pd.DataFrame(weight_df).merge(predicted_y_df, on=[\"tic\"])\n",
    "        meta_coefficient[\"weights\"] += [weight_df]\n",
    "        cap = portfolio.iloc[0, i]\n",
    "        # current cash invested for each stock\n",
    "        current_cash = [element * cap for element in list(weights.values())]\n",
    "        # current held shares\n",
    "        current_shares = list(np.array(current_cash) / np.array(df_current.close))\n",
    "        # next time period price\n",
    "        next_price = np.array(df_next.close)\n",
    "        portfolio.iloc[0, i + 1] = np.dot(current_shares, next_price)\n",
    "\n",
    "    portfolio = portfolio.T\n",
    "    portfolio.columns = [\"account_value\"]\n",
    "    portfolio = portfolio.reset_index()\n",
    "    portfolio.columns = [\"date\", \"account_value\"]\n",
    "    stats = backtest_stats(portfolio, value_col_name=\"account_value\")\n",
    "    portfolio_cumprod = (portfolio.account_value.pct_change() + 1).cumprod() - 1\n",
    "\n",
    "    return portfolio, stats, portfolio_cumprod, pd.DataFrame(meta_coefficient)\n",
    "\n",
    "\n",
    "lr_portfolio, lr_stats, lr_cumprod, lr_weights = output_predict(lr_model)\n",
    "dt_portfolio, dt_stats, dt_cumprod, dt_weights = output_predict(dt_model)\n",
    "svm_portfolio, svm_stats, svm_cumprod, svm_weights = output_predict(svm_model)\n",
    "rf_portfolio, rf_stats, rf_cumprod, rf_weights = output_predict(rf_model)\n",
    "(\n",
    "    reference_portfolio,\n",
    "    reference_stats,\n",
    "    reference_cumprod,\n",
    "    reference_weights,\n",
    ") = output_predict(None, True)\n",
    "\n",
    "\n",
    "def calculate_gradient(\n",
    "    model, interpolated_input, actions, feature_idx, stock_idx, h=1e-1\n",
    "):\n",
    "    forward_input = interpolated_input\n",
    "    forward_input[feature_idx + stock_dimension][stock_idx] += h\n",
    "    forward_Q = model.policy.evaluate_actions(\n",
    "        torch.FloatTensor(forward_input).reshape(\n",
    "            -1, stock_dimension * (stock_dimension + feature_dimension)\n",
    "        ),\n",
    "        torch.FloatTensor(actions).reshape(-1, stock_dimension),\n",
    "    )\n",
    "    interpolated_Q = model.policy.evaluate_actions(\n",
    "        torch.FloatTensor(interpolated_input).reshape(\n",
    "            -1, stock_dimension * (stock_dimension + feature_dimension)\n",
    "        ),\n",
    "        torch.FloatTensor(actions).reshape(-1, stock_dimension),\n",
    "    )\n",
    "    forward_Q = forward_Q[0].detach().cpu().numpy()[0]\n",
    "    interpolated_Q = interpolated_Q[0].detach().cpu().numpy()[0]\n",
    "    return (forward_Q - interpolated_Q) / h\n",
    "\n",
    "\n",
    "import copy\n",
    "\n",
    "meta_Q = {\"date\": [], \"feature\": [], \"Saliency Map\": [], \"algo\": []}\n",
    "\n",
    "for algo in {\"A2C\", \"PPO\"}:\n",
    "    if algo == \"A2C\":\n",
    "        prec_step = 1e-2\n",
    "    else:\n",
    "        prec_step = 1e-1\n",
    "\n",
    "    model = eval(\"trained_\" + algo.lower())\n",
    "    df_actions = eval(\"df_actions_\" + algo.lower())\n",
    "    for i in range(len(unique_trade_date) - 1):\n",
    "        date = unique_trade_date[i]\n",
    "        covs = trade[trade[\"date\"] == date].cov_list.iloc[0]\n",
    "        features = trade[trade[\"date\"] == date][tech_indicator_list].values  # N x K\n",
    "        actions = df_actions.loc[date].values\n",
    "\n",
    "        for feature_idx in range(len(tech_indicator_list)):\n",
    "\n",
    "            int_grad_per_feature = 0\n",
    "            for stock_idx in range(features.shape[0]):  # N\n",
    "\n",
    "                int_grad_per_stock = 0\n",
    "                avg_interpolated_grad = 0\n",
    "                for alpha in range(1, 51):\n",
    "                    scale = 1 / 50\n",
    "                    baseline_features = copy.deepcopy(features)\n",
    "                    baseline_noise = np.random.normal(0, 1, stock_dimension)\n",
    "                    baseline_features[:, feature_idx] = [0] * stock_dimension\n",
    "                    interpolated_features = baseline_features + scale * alpha * (\n",
    "                        features - baseline_features\n",
    "                    )  # N x K\n",
    "                    interpolated_input = np.append(\n",
    "                        covs, interpolated_features.T, axis=0\n",
    "                    )\n",
    "                    interpolated_gradient = calculate_gradient(\n",
    "                        model,\n",
    "                        interpolated_input,\n",
    "                        actions,\n",
    "                        feature_idx,\n",
    "                        stock_idx,\n",
    "                        h=prec_step,\n",
    "                    )[0]\n",
    "\n",
    "                    avg_interpolated_grad += interpolated_gradient * scale\n",
    "                int_grad_per_stock = (\n",
    "                    features[stock_idx][feature_idx] - 0\n",
    "                ) * avg_interpolated_grad\n",
    "                int_grad_per_feature += int_grad_per_stock\n",
    "\n",
    "            meta_Q[\"date\"] += [date]\n",
    "            meta_Q[\"algo\"] += [algo]\n",
    "            meta_Q[\"feature\"] += [tech_indicator_list[feature_idx]]\n",
    "            meta_Q[\"Saliency Map\"] += [int_grad_per_feature]\n",
    "\n",
    "meta_Q = pd.DataFrame(meta_Q)\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "meta_score_coef = {\"date\": [], \"coef\": [], \"algo\": []}\n",
    "\n",
    "for algo in [\"LR\", \"RF\", \"Reference Model\", \"SVM\", \"DT\", \"A2C\", \"PPO\"]:\n",
    "    if algo == \"LR\":\n",
    "        weights = lr_weights\n",
    "    elif algo == \"RF\":\n",
    "        weights = rf_weights\n",
    "    elif algo == \"DT\":\n",
    "        weights = dt_weights\n",
    "    elif algo == \"SVM\":\n",
    "        weights = svm_weights\n",
    "    elif algo == \"A2C\":\n",
    "        weights = a2c_weights\n",
    "    elif algo == \"PPO\":\n",
    "        weights = ppo_weights\n",
    "    else:\n",
    "        weights = reference_weights\n",
    "\n",
    "    for i in range(len(unique_trade_date) - 1):\n",
    "        date = unique_trade_date[i]\n",
    "        next_date = unique_trade_date[i + 1]\n",
    "        df_temp = df[df.date == date].reset_index(drop=True)\n",
    "        df_temp_next = df[df.date == next_date].reset_index(drop=True)\n",
    "        weight_piece = weights[weights.date == date].iloc[0][\"weights\"]\n",
    "        piece_return = pd.DataFrame(\n",
    "            df_temp_next.return_list.iloc[0].loc[next_date]\n",
    "        ).reset_index()\n",
    "        piece_return.columns = [\"tic\", \"return\"]\n",
    "        X = df_temp[[\"macd\", \"rsi_30\", \"cci_30\", \"dx_30\", \"tic\"]]\n",
    "        X_next = df_temp_next[[\"macd\", \"rsi_30\", \"cci_30\", \"dx_30\", \"tic\"]]\n",
    "        piece = weight_piece.merge(X, on=\"tic\").merge(piece_return, on=\"tic\")\n",
    "        piece[\"Y\"] = piece[\"return\"] * piece[\"weight\"]\n",
    "        X = piece[[\"macd\", \"rsi_30\", \"cci_30\", \"dx_30\"]]\n",
    "        X = sm.add_constant(X)\n",
    "        Y = piece[[\"Y\"]]\n",
    "        model = sm.OLS(Y, X)\n",
    "        results = model.fit()\n",
    "        meta_score_coef[\"coef\"] += [(X * results.params).sum(axis=0)]\n",
    "        meta_score_coef[\"date\"] += [date]\n",
    "        meta_score_coef[\"algo\"] += [algo]\n",
    "\n",
    "meta_score_coef = pd.DataFrame(meta_score_coef)\n",
    "\n",
    "performance_score = {\"date\": [], \"algo\": [], \"score\": []}\n",
    "\n",
    "for i in range(0, len(unique_trade_date)):\n",
    "    date_ = unique_trade_date[i]\n",
    "    if len(meta_score_coef[(meta_score_coef[\"date\"] == date_)]) == 0:\n",
    "        continue\n",
    "    lr_coef = (\n",
    "        meta_score_coef[\n",
    "            (meta_score_coef[\"date\"] == date_) & (meta_score_coef[\"algo\"] == \"LR\")\n",
    "        ][\"coef\"]\n",
    "        .values[0][[\"macd\", \"rsi_30\", \"cci_30\", \"dx_30\"]]\n",
    "        .values\n",
    "    )\n",
    "    rf_coef = (\n",
    "        meta_score_coef[\n",
    "            (meta_score_coef[\"date\"] == date_) & (meta_score_coef[\"algo\"] == \"RF\")\n",
    "        ][\"coef\"]\n",
    "        .values[0][[\"macd\", \"rsi_30\", \"cci_30\", \"dx_30\"]]\n",
    "        .values\n",
    "    )\n",
    "    reference_coef = (\n",
    "        meta_score_coef[\n",
    "            (meta_score_coef[\"date\"] == date_)\n",
    "            & (meta_score_coef[\"algo\"] == \"Reference Model\")\n",
    "        ][\"coef\"]\n",
    "        .values[0][[\"macd\", \"rsi_30\", \"cci_30\", \"dx_30\"]]\n",
    "        .values\n",
    "    )\n",
    "    dt_coef = (\n",
    "        meta_score_coef[\n",
    "            (meta_score_coef[\"date\"] == date_) & (meta_score_coef[\"algo\"] == \"DT\")\n",
    "        ][\"coef\"]\n",
    "        .values[0][[\"macd\", \"rsi_30\", \"cci_30\", \"dx_30\"]]\n",
    "        .values\n",
    "    )\n",
    "    svm_coef = (\n",
    "        meta_score_coef[\n",
    "            (meta_score_coef[\"date\"] == date_) & (meta_score_coef[\"algo\"] == \"SVM\")\n",
    "        ][\"coef\"]\n",
    "        .values[0][[\"macd\", \"rsi_30\", \"cci_30\", \"dx_30\"]]\n",
    "        .values\n",
    "    )\n",
    "\n",
    "    saliency_coef_a2c = meta_Q[(meta_Q[\"date\"] == date_) & (meta_Q[\"algo\"] == \"A2C\")][\n",
    "        \"Saliency Map\"\n",
    "    ].values\n",
    "    saliency_coef_ppo = meta_Q[(meta_Q[\"date\"] == date_) & (meta_Q[\"algo\"] == \"PPO\")][\n",
    "        \"Saliency Map\"\n",
    "    ].values\n",
    "\n",
    "    lr_score = np.corrcoef(lr_coef, reference_coef)[0][1]\n",
    "    rf_score = np.corrcoef(rf_coef, reference_coef)[0][1]\n",
    "    dt_score = np.corrcoef(dt_coef, reference_coef)[0][1]\n",
    "    svm_score = np.corrcoef(svm_coef, reference_coef)[0][1]\n",
    "    saliency_score_a2c = np.corrcoef(saliency_coef_a2c, reference_coef)[0][1]\n",
    "    saliency_score_ppo = np.corrcoef(saliency_coef_ppo, reference_coef)[0][1]\n",
    "\n",
    "    for algo in [\"LR\", \"A2C\", \"PPO\", \"RF\", \"DT\", \"SVM\"]:\n",
    "        performance_score[\"date\"] += [date_]\n",
    "        performance_score[\"algo\"] += [algo]\n",
    "        if algo == \"LR\":\n",
    "            score = lr_score\n",
    "        elif algo == \"RF\":\n",
    "            score = rf_score\n",
    "        elif algo == \"DT\":\n",
    "            score = dt_score\n",
    "        elif algo == \"A2C\":\n",
    "            score = saliency_score_a2c\n",
    "        elif algo == \"SVM\":\n",
    "            score = svm_score\n",
    "        else:\n",
    "            score = saliency_score_ppo\n",
    "        performance_score[\"score\"] += [score]\n",
    "\n",
    "performance_score = pd.DataFrame(performance_score)\n",
    "\n",
    "multi_performance_score = {\"date\": [], \"algo\": [], \"score\": []}\n",
    "window = 20\n",
    "for i in range(len(unique_trade_date) - window):\n",
    "    date_ = unique_trade_date[i]\n",
    "    if len(meta_score_coef[(meta_score_coef[\"date\"] == date_)]) == 0:\n",
    "        continue\n",
    "    lr_coef = (\n",
    "        meta_score_coef[\n",
    "            (meta_score_coef[\"date\"] == date_) & (meta_score_coef[\"algo\"] == \"LR\")\n",
    "        ][\"coef\"]\n",
    "        .values[0][[\"macd\", \"rsi_30\", \"cci_30\", \"dx_30\"]]\n",
    "        .values\n",
    "    )\n",
    "    rf_coef = (\n",
    "        meta_score_coef[\n",
    "            (meta_score_coef[\"date\"] == date_) & (meta_score_coef[\"algo\"] == \"RF\")\n",
    "        ][\"coef\"]\n",
    "        .values[0][[\"macd\", \"rsi_30\", \"cci_30\", \"dx_30\"]]\n",
    "        .values\n",
    "    )\n",
    "    reference_coef = (\n",
    "        meta_score_coef[\n",
    "            (meta_score_coef[\"date\"] == date_)\n",
    "            & (meta_score_coef[\"algo\"] == \"Reference Model\")\n",
    "        ][\"coef\"]\n",
    "        .values[0][[\"macd\", \"rsi_30\", \"cci_30\", \"dx_30\"]]\n",
    "        .values\n",
    "    )\n",
    "    for w in range(1, window):\n",
    "        date_f = unique_trade_date[i + w]\n",
    "        prx_coef = (\n",
    "            meta_score_coef[\n",
    "                (meta_score_coef[\"date\"] == date_f)\n",
    "                & (meta_score_coef[\"algo\"] == \"Reference Model\")\n",
    "            ][\"coef\"]\n",
    "            .values[0][[\"macd\", \"rsi_30\", \"cci_30\", \"dx_30\"]]\n",
    "            .values\n",
    "        )\n",
    "        reference_coef += prx_coef\n",
    "    reference_coef = reference_coef / window\n",
    "    dt_coef = (\n",
    "        meta_score_coef[\n",
    "            (meta_score_coef[\"date\"] == date_) & (meta_score_coef[\"algo\"] == \"DT\")\n",
    "        ][\"coef\"]\n",
    "        .values[0][[\"macd\", \"rsi_30\", \"cci_30\", \"dx_30\"]]\n",
    "        .values\n",
    "    )\n",
    "    svm_coef = (\n",
    "        meta_score_coef[\n",
    "            (meta_score_coef[\"date\"] == date_) & (meta_score_coef[\"algo\"] == \"SVM\")\n",
    "        ][\"coef\"]\n",
    "        .values[0][[\"macd\", \"rsi_30\", \"cci_30\", \"dx_30\"]]\n",
    "        .values\n",
    "    )\n",
    "    saliency_coef_a2c = meta_Q[(meta_Q[\"date\"] == date_) & (meta_Q[\"algo\"] == \"A2C\")][\n",
    "        \"Saliency Map\"\n",
    "    ].values\n",
    "    saliency_coef_ppo = meta_Q[(meta_Q[\"date\"] == date_) & (meta_Q[\"algo\"] == \"PPO\")][\n",
    "        \"Saliency Map\"\n",
    "    ].values\n",
    "    lr_score = np.corrcoef(lr_coef, reference_coef)[0][1]\n",
    "    rf_score = np.corrcoef(rf_coef, reference_coef)[0][1]\n",
    "    dt_score = np.corrcoef(dt_coef, reference_coef)[0][1]\n",
    "    svm_score = np.corrcoef(svm_coef, reference_coef)[0][1]\n",
    "    saliency_score_a2c = np.corrcoef(saliency_coef_a2c, reference_coef)[0][1]\n",
    "    saliency_score_ppo = np.corrcoef(saliency_coef_ppo, reference_coef)[0][1]\n",
    "\n",
    "    for algo in [\"LR\", \"A2C\", \"RF\", \"PPO\", \"DT\", \"SVM\"]:\n",
    "        multi_performance_score[\"date\"] += [date_]\n",
    "        multi_performance_score[\"algo\"] += [algo]\n",
    "        if algo == \"LR\":\n",
    "            score = lr_score\n",
    "        elif algo == \"RF\":\n",
    "            score = rf_score\n",
    "        elif algo == \"DT\":\n",
    "            score = dt_score\n",
    "        elif algo == \"A2C\":\n",
    "            score = saliency_score_a2c\n",
    "        elif algo == \"SVM\":\n",
    "            score = svm_score\n",
    "        else:\n",
    "            score = saliency_score_ppo\n",
    "        multi_performance_score[\"score\"] += [score]\n",
    "\n",
    "multi_performance_score = pd.DataFrame(multi_performance_score)\n",
    "\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "trace1_portfolio = go.Scatter(x=time_ind, y=a2c_cumpod, mode=\"lines\", name=\"A2C\")\n",
    "trace2_portfolio = go.Scatter(x=time_ind, y=ppo_cumpod, mode=\"lines\", name=\"PPO\")\n",
    "trace3_portfolio = go.Scatter(x=time_ind, y=dji_cumpod, mode=\"lines\", name=\"DJIA\")\n",
    "trace4_portfolio = go.Scatter(x=time_ind, y=lr_cumprod, mode=\"lines\", name=\"LR\")\n",
    "trace5_portfolio = go.Scatter(x=time_ind, y=rf_cumprod, mode=\"lines\", name=\"RF\")\n",
    "trace6_portfolio = go.Scatter(x=time_ind, y=dt_cumprod, mode=\"lines\", name=\"DT\")\n",
    "trace7_portfolio = go.Scatter(x=time_ind, y=svm_cumprod, mode=\"lines\", name=\"SVM\")\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(trace1_portfolio)\n",
    "fig.add_trace(trace2_portfolio)\n",
    "\n",
    "fig.add_trace(trace3_portfolio)\n",
    "\n",
    "fig.add_trace(trace4_portfolio)\n",
    "fig.add_trace(trace5_portfolio)\n",
    "fig.add_trace(trace6_portfolio)\n",
    "fig.add_trace(trace7_portfolio)\n",
    "\n",
    "fig.update_layout(\n",
    "    legend=dict(\n",
    "        x=0,\n",
    "        y=1,\n",
    "        traceorder=\"normal\",\n",
    "        font=dict(family=\"sans-serif\", size=15, color=\"black\"),\n",
    "        bgcolor=\"White\",\n",
    "        bordercolor=\"white\",\n",
    "        borderwidth=2,\n",
    "    ),\n",
    ")\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        # 'text': \"Cumulative Return using FinRL\",\n",
    "        \"y\": 0.85,\n",
    "        \"x\": 0.5,\n",
    "        \"xanchor\": \"center\",\n",
    "        \"yanchor\": \"top\",\n",
    "    }\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    paper_bgcolor=\"rgba(1,1,0,0)\",\n",
    "    plot_bgcolor=\"rgba(1, 1, 0, 0)\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis=dict(titlefont=dict(size=30), title=\"Cumulative Return\"),\n",
    "    font=dict(\n",
    "        size=40,\n",
    "    ),\n",
    ")\n",
    "fig.update_layout(font_size=20)\n",
    "fig.update_traces(line=dict(width=2))\n",
    "\n",
    "fig.update_xaxes(\n",
    "    showline=True,\n",
    "    linecolor=\"black\",\n",
    "    showgrid=True,\n",
    "    gridwidth=1,\n",
    "    gridcolor=\"LightSteelBlue\",\n",
    "    mirror=True,\n",
    ")\n",
    "fig.update_yaxes(\n",
    "    showline=True,\n",
    "    linecolor=\"black\",\n",
    "    showgrid=True,\n",
    "    gridwidth=1,\n",
    "    gridcolor=\"LightSteelBlue\",\n",
    "    mirror=True,\n",
    ")\n",
    "fig.update_yaxes(zeroline=True, zerolinewidth=1, zerolinecolor=\"LightSteelBlue\")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "meta_score = {\n",
    "    \"Annual return\": [],\n",
    "    \"Annual volatility\": [],\n",
    "    \"Max drawdown\": [],\n",
    "    \"Sharpe ratio\": [],\n",
    "    \"Algorithm\": [],\n",
    "    \"Calmar ratio\": [],\n",
    "}\n",
    "for name in [\"LR\", \"A2C\", \"RF\", \"Reference Model\", \"PPO\", \"SVM\", \"DT\", \"DJI\"]:\n",
    "    if name == \"DT\":\n",
    "        annualreturn = dt_stats[\"Annual return\"]\n",
    "        annualvol = dt_stats[\"Annual volatility\"]\n",
    "        sharpeRatio = dt_stats[\"Sharpe ratio\"]\n",
    "        maxdradown = dt_stats[\"Max drawdown\"]\n",
    "        calmarratio = dt_stats[\"Calmar ratio\"]\n",
    "    elif name == \"LR\":\n",
    "        annualreturn = lr_stats[\"Annual return\"]\n",
    "        annualvol = lr_stats[\"Annual volatility\"]\n",
    "        sharpeRatio = lr_stats[\"Sharpe ratio\"]\n",
    "        maxdradown = lr_stats[\"Max drawdown\"]\n",
    "        calmarratio = lr_stats[\"Calmar ratio\"]\n",
    "    elif name == \"SVM\":\n",
    "        annualreturn = svm_stats[\"Annual return\"]\n",
    "        annualvol = svm_stats[\"Annual volatility\"]\n",
    "        sharpeRatio = svm_stats[\"Sharpe ratio\"]\n",
    "        maxdradown = svm_stats[\"Max drawdown\"]\n",
    "        calmarratio = svm_stats[\"Calmar ratio\"]\n",
    "    elif name == \"RF\":\n",
    "        annualreturn = rf_stats[\"Annual return\"]\n",
    "        annualvol = rf_stats[\"Annual volatility\"]\n",
    "        sharpeRatio = rf_stats[\"Sharpe ratio\"]\n",
    "        maxdradown = rf_stats[\"Max drawdown\"]\n",
    "        calmarratio = rf_stats[\"Calmar ratio\"]\n",
    "    elif name == \"Reference Model\":\n",
    "        annualreturn = reference_stats[\"Annual return\"]\n",
    "        annualvol = reference_stats[\"Annual volatility\"]\n",
    "        sharpeRatio = reference_stats[\"Sharpe ratio\"]\n",
    "        maxdradown = reference_stats[\"Max drawdown\"]\n",
    "        calmarratio = reference_stats[\"Calmar ratio\"]\n",
    "    elif name == \"PPO\":\n",
    "        annualreturn = perf_stats_all_ppo[\"Annual return\"]\n",
    "        annualvol = perf_stats_all_ppo[\"Annual volatility\"]\n",
    "        sharpeRatio = perf_stats_all_ppo[\"Sharpe ratio\"]\n",
    "        maxdradown = perf_stats_all_ppo[\"Max drawdown\"]\n",
    "        calmarratio = perf_stats_all_ppo[\"Calmar ratio\"]\n",
    "    elif name == \"DJI\":\n",
    "        annualreturn = baseline_df_stats[\"Annual return\"]\n",
    "        annualvol = baseline_df_stats[\"Annual volatility\"]\n",
    "        sharpeRatio = baseline_df_stats[\"Sharpe ratio\"]\n",
    "        maxdradown = baseline_df_stats[\"Max drawdown\"]\n",
    "        calmarratio = baseline_df_stats[\"Calmar ratio\"]\n",
    "    else:\n",
    "        annualreturn = perf_stats_all_a2c[\"Annual return\"]\n",
    "        annualvol = perf_stats_all_a2c[\"Annual volatility\"]\n",
    "        sharpeRatio = perf_stats_all_a2c[\"Sharpe ratio\"]\n",
    "        maxdradown = perf_stats_all_a2c[\"Max drawdown\"]\n",
    "        calmarratio = perf_stats_all_a2c[\"Calmar ratio\"]\n",
    "    meta_score[\"Algorithm\"] += [name]\n",
    "    meta_score[\"Annual return\"] += [annualreturn]\n",
    "    meta_score[\"Annual volatility\"] += [annualvol]\n",
    "    meta_score[\"Max drawdown\"] += [maxdradown]\n",
    "    meta_score[\"Sharpe ratio\"] += [sharpeRatio]\n",
    "    meta_score[\"Calmar ratio\"] += [calmarratio]\n",
    "\n",
    "meta_score = pd.DataFrame(meta_score).sort_values(\"Sharpe ratio\")\n",
    "\n",
    "\n",
    "postiveRatio = pd.DataFrame(\n",
    "    performance_score.groupby(\"algo\").apply(lambda x: np.mean(x[\"score\"]))\n",
    ")\n",
    "\n",
    "postiveRatio = postiveRatio.reset_index()\n",
    "postiveRatio.columns = [\"algo\", \"avg_correlation_coefficient\"]\n",
    "postiveRatio[\"Sharpe Ratio\"] = [0] * 6\n",
    "\n",
    "# postiveRatio.plot.bar(x = 'algo', y = 'avg_correlation_coefficient')\n",
    "\n",
    "postiveRatiom = pd.DataFrame(\n",
    "    multi_performance_score.groupby(\"algo\").apply(lambda x: np.mean(x[\"score\"]))\n",
    ")\n",
    "postiveRatiom = postiveRatiom.reset_index()\n",
    "postiveRatiom.columns = [\"algo\", \"avg_correlation_coefficient\"]\n",
    "postiveRatiom[\"Sharpe Ratio\"] = [0] * 6\n",
    "\n",
    "# postiveRatiom.plot.bar(x = 'algo', y = 'avg_correlation_coefficient')\n",
    "\n",
    "\n",
    "for algo in [\"A2C\", \"PPO\", \"LR\", \"DT\", \"RF\", \"SVM\"]:\n",
    "    postiveRatio.loc[postiveRatio[\"algo\"] == algo, \"Sharpe Ratio\"] = meta_score.loc[\n",
    "        meta_score[\"Algorithm\"] == algo, \"Sharpe ratio\"\n",
    "    ].values[0]\n",
    "    postiveRatiom.loc[postiveRatio[\"algo\"] == algo, \"Sharpe Ratio\"] = meta_score.loc[\n",
    "        meta_score[\"Algorithm\"] == algo, \"Sharpe ratio\"\n",
    "    ].values[0]\n",
    "\n",
    "postiveRatio.sort_values(\"Sharpe Ratio\", inplace=True)\n",
    "\n",
    "postiveRatiom.sort_values(\"Sharpe Ratio\", inplace=True)\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Create figure with secondary y-axis\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# Add traces\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=postiveRatiom[\"algo\"],\n",
    "        y=postiveRatiom[\"Sharpe Ratio\"],\n",
    "        name=\"Sharpe Ratio\",\n",
    "        marker_size=15,\n",
    "        line_width=5,\n",
    "    ),\n",
    "    secondary_y=True,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=postiveRatiom[\"algo\"],\n",
    "        y=postiveRatiom[\"avg_correlation_coefficient\"],\n",
    "        name=\"Multi-Step Average Correlation Coefficient          \",\n",
    "        width=0.38,\n",
    "    ),\n",
    "    secondary_y=False,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=postiveRatio[\"algo\"],\n",
    "        y=postiveRatio[\"avg_correlation_coefficient\"],\n",
    "        name=\"Single-Step Average Correlation Coefficient           \",\n",
    "        width=0.38,\n",
    "    ),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    paper_bgcolor=\"rgba(1,1,0,0)\",\n",
    "    plot_bgcolor=\"rgba(1, 1, 0, 0)\",\n",
    ")\n",
    "fig.update_layout(legend=dict(yanchor=\"top\", y=1.5, xanchor=\"right\", x=0.95))\n",
    "fig.update_layout(font_size=15)\n",
    "\n",
    "# Set x-axis title\n",
    "fig.update_xaxes(title_text=\"Model\")\n",
    "fig.update_xaxes(\n",
    "    showline=True,\n",
    "    linecolor=\"black\",\n",
    "    showgrid=True,\n",
    "    gridwidth=1,\n",
    "    gridcolor=\"LightSteelBlue\",\n",
    "    mirror=True,\n",
    ")\n",
    "fig.update_yaxes(\n",
    "    showline=True,\n",
    "    linecolor=\"black\",\n",
    "    showgrid=True,\n",
    "    secondary_y=False,\n",
    "    gridwidth=1,\n",
    "    gridcolor=\"LightSteelBlue\",\n",
    "    mirror=True,\n",
    ")\n",
    "fig.update_yaxes(zeroline=True, zerolinewidth=1, zerolinecolor=\"LightSteelBlue\")\n",
    "# Set y-axes titles\n",
    "fig.update_yaxes(\n",
    "    title_text=\"Average Correlation Coefficient\",\n",
    "    secondary_y=False,\n",
    "    range=[-0.1, 0.1],\n",
    ")\n",
    "fig.update_yaxes(title_text=\"Sharpe Ratio\", secondary_y=True, range=[-0.5, 2.5])\n",
    "\n",
    "fig.show()\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig = make_subplots(rows=2, cols=3)\n",
    "\n",
    "trace0 = go.Histogram(\n",
    "    x=performance_score[performance_score[\"algo\"] == \"A2C\"][\"score\"].values,\n",
    "    nbinsx=25,\n",
    "    name=\"A2C\",\n",
    "    histnorm=\"probability\",\n",
    ")\n",
    "trace1 = go.Histogram(\n",
    "    x=performance_score[performance_score[\"algo\"] == \"PPO\"][\"score\"].values,\n",
    "    nbinsx=25,\n",
    "    name=\"PPO\",\n",
    "    histnorm=\"probability\",\n",
    ")\n",
    "trace2 = go.Histogram(\n",
    "    x=performance_score[performance_score[\"algo\"] == \"DT\"][\"score\"].values,\n",
    "    nbinsx=25,\n",
    "    name=\"DT\",\n",
    "    histnorm=\"probability\",\n",
    ")\n",
    "trace3 = go.Histogram(\n",
    "    x=performance_score[performance_score[\"algo\"] == \"LR\"][\"score\"].values,\n",
    "    nbinsx=25,\n",
    "    name=\"LR\",\n",
    "    histnorm=\"probability\",\n",
    ")\n",
    "trace4 = go.Histogram(\n",
    "    x=performance_score[performance_score[\"algo\"] == \"SVM\"][\"score\"].values,\n",
    "    nbinsx=25,\n",
    "    name=\"SVM\",\n",
    "    histnorm=\"probability\",\n",
    ")\n",
    "trace5 = go.Histogram(\n",
    "    x=performance_score[performance_score[\"algo\"] == \"RF\"][\"score\"].values,\n",
    "    nbinsx=25,\n",
    "    name=\"RF\",\n",
    "    histnorm=\"probability\",\n",
    ")\n",
    "\n",
    "fig.append_trace(trace0, 1, 1)\n",
    "fig.append_trace(trace1, 1, 2)\n",
    "fig.append_trace(trace2, 1, 3)\n",
    "fig.append_trace(trace3, 2, 1)\n",
    "fig.append_trace(trace4, 2, 2)\n",
    "fig.append_trace(trace5, 2, 3)\n",
    "# Update xaxis properties\n",
    "fig.update_xaxes(title_text=\"Correlation coefficient\", row=2, col=2)\n",
    "fig.update_yaxes(title_text=\"Frequency\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Frequency\", row=2, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    paper_bgcolor=\"rgba(1,1,0,0)\",\n",
    "    plot_bgcolor=\"rgba(1, 1, 0, 0)\",\n",
    "    font=dict(\n",
    "        size=18,\n",
    "    ),\n",
    ")\n",
    "fig.update_layout(legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=1))\n",
    "\n",
    "fig.update_xaxes(\n",
    "    showline=True,\n",
    "    linecolor=\"black\",\n",
    "    showgrid=True,\n",
    "    gridwidth=1,\n",
    "    gridcolor=\"LightSteelBlue\",\n",
    "    mirror=True,\n",
    ")\n",
    "fig.update_yaxes(\n",
    "    showline=True,\n",
    "    linecolor=\"black\",\n",
    "    showgrid=True,\n",
    "    gridwidth=1,\n",
    "    gridcolor=\"LightSteelBlue\",\n",
    "    mirror=True,\n",
    ")\n",
    "fig.update_yaxes(zeroline=True, zerolinewidth=1, zerolinecolor=\"LightSteelBlue\")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig = make_subplots(rows=2, cols=3)\n",
    "\n",
    "trace0 = go.Histogram(\n",
    "    x=multi_performance_score[multi_performance_score[\"algo\"] == \"A2C\"][\"score\"].values,\n",
    "    nbinsx=25,\n",
    "    name=\"A2C\",\n",
    "    histnorm=\"probability\",\n",
    ")\n",
    "trace1 = go.Histogram(\n",
    "    x=multi_performance_score[multi_performance_score[\"algo\"] == \"PPO\"][\"score\"].values,\n",
    "    nbinsx=25,\n",
    "    name=\"PPO\",\n",
    "    histnorm=\"probability\",\n",
    ")\n",
    "trace2 = go.Histogram(\n",
    "    x=multi_performance_score[multi_performance_score[\"algo\"] == \"DT\"][\"score\"].values,\n",
    "    nbinsx=25,\n",
    "    name=\"DT\",\n",
    "    histnorm=\"probability\",\n",
    ")\n",
    "trace3 = go.Histogram(\n",
    "    x=multi_performance_score[multi_performance_score[\"algo\"] == \"LR\"][\"score\"].values,\n",
    "    nbinsx=25,\n",
    "    name=\"LR\",\n",
    "    histnorm=\"probability\",\n",
    ")\n",
    "trace4 = go.Histogram(\n",
    "    x=multi_performance_score[multi_performance_score[\"algo\"] == \"SVM\"][\"score\"].values,\n",
    "    nbinsx=25,\n",
    "    name=\"SVM\",\n",
    "    histnorm=\"probability\",\n",
    ")\n",
    "trace5 = go.Histogram(\n",
    "    x=multi_performance_score[multi_performance_score[\"algo\"] == \"RF\"][\"score\"].values,\n",
    "    nbinsx=25,\n",
    "    name=\"RF\",\n",
    "    histnorm=\"probability\",\n",
    ")\n",
    "\n",
    "fig.update_layout(yaxis1=dict(range=[0, 0.2]))\n",
    "fig.update_layout(yaxis2=dict(range=[0, 0.2]))\n",
    "fig.update_layout(yaxis3=dict(range=[0, 0.4]))\n",
    "fig.update_layout(yaxis4=dict(range=[0, 0.4]))\n",
    "fig.update_layout(yaxis5=dict(range=[0, 0.4]))\n",
    "fig.update_layout(yaxis6=dict(range=[0, 0.4]))\n",
    "\n",
    "fig.append_trace(trace0, 1, 1)\n",
    "fig.append_trace(trace1, 1, 2)\n",
    "fig.append_trace(trace2, 1, 3)\n",
    "fig.append_trace(trace3, 2, 1)\n",
    "fig.append_trace(trace4, 2, 2)\n",
    "fig.append_trace(trace5, 2, 3)\n",
    "# Update xaxis properties\n",
    "fig.update_xaxes(title_text=\"Correlation coefficient\", row=2, col=2)\n",
    "fig.update_yaxes(title_text=\"Frequency\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Frequency\", row=2, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    paper_bgcolor=\"rgba(1,1,0,0)\",\n",
    "    plot_bgcolor=\"rgba(1, 1, 0, 0)\",\n",
    "    font=dict(\n",
    "        size=18,\n",
    "    ),\n",
    ")\n",
    "fig.update_layout(legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=1))\n",
    "fig.update_xaxes(\n",
    "    showline=True,\n",
    "    linecolor=\"black\",\n",
    "    showgrid=True,\n",
    "    gridwidth=1,\n",
    "    gridcolor=\"LightSteelBlue\",\n",
    "    mirror=True,\n",
    ")\n",
    "fig.update_yaxes(\n",
    "    showline=True,\n",
    "    linecolor=\"black\",\n",
    "    showgrid=True,\n",
    "    gridwidth=1,\n",
    "    gridcolor=\"LightSteelBlue\",\n",
    "    mirror=True,\n",
    ")\n",
    "fig.update_yaxes(zeroline=True, zerolinewidth=1, zerolinecolor=\"LightSteelBlue\")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
